{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#gen3-documentation","title":"Gen3 Documentation","text":"<p>This is your home for all technical documentation related to the design, deployment, use, and maintenance of a Gen3 data commons or mesh.</p> <p>Please visit Gen3.org if you would like a high-level overview of Gen3 as well as details about the Gen3 philosophy, community events, and governance.</p> <p>Gen3 documentation is organized by the category of person interacting with Gen3:</p> <ul> <li>Gen3 User - This is a data scientist, researcher, or analyst who needs to explore, download, or analyze data found within an existing instance of Gen3.</li> <li>Gen3 Developer - This is a software engineer who wants to extend Gen3 either by contributing to the source code or by integrating Gen3 services into a larger system.  This section will cover the Gen3 architecture including the individual microservices and how they interact with each other.</li> <li>Gen3 Operator - This is for those organizations who operate their own Gen3 instances.  It will include content on how to Deploy or Spin up a Gen3 instance, configure a data dictionary and upload data, and customize the frontend.</li> </ul> <p></p>"},{"location":"contact/","title":"Contact Information","text":""},{"location":"contact/#contact-information","title":"Contact Information","text":""},{"location":"contact/#gen3-slack","title":"Gen3 Slack","text":"<p>Gen3 Slack is the preferred medium of correspondence if you have any question about the set up or use of a Gen3 system.  Slack offers the benefit of everyone learning from others' questions.  It also allows community members to contribute to a response.  You can sign up for the Gen3 slack workspace here.</p>"},{"location":"contact/#gen3-discussion-board","title":"Gen3 Discussion Board","text":"<p>The Gen3 Discussion board allows you to post questions on a variety of topics to get feedback from the community.</p>"},{"location":"contact/#ctds-help-desk","title":"CTDS Help Desk","text":"<p>To reach a member of the CTDS User Services Team you may contact us via email at support@gen3.org.  For general questions about CTDS, please use ctds@uchicago.edu.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/09/13/Gen3%20on%20laptop/","title":"Running K8s on Your Laptop: Exploring the Options","text":"","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/09/13/Gen3%20on%20laptop/#running-k8s-on-your-laptop-exploring-the-options","title":"Running K8s on Your Laptop: Exploring the Options","text":"<p>Kubernetes (often abbreviated as \"K8s\") is an open-source platform designed to automate deploying, scaling, and managing containerized applications. Initially, Kubernetes might seem more fitting for large scale, cloud environments. However, for learning, development, and testing purposes, running Kubernetes locally on your laptop is incredibly beneficial. Let's dive into the various ways you can achieve this.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/09/13/Gen3%20on%20laptop/#1-minikube","title":"1. Minikube","text":"<p>Pros:</p> <ul> <li>Officially supported by Kubernetes.</li> <li>Provides a full-fledged K8s cluster with just one node.</li> <li>Supports many Kubernetes features out-of-the-box.</li> <li>Easy to install and use.</li> </ul> <p>Cons:</p> <ul> <li>Can be resource-intensive.</li> <li>Requires a virtual machine (VM) or a local container runtime.</li> </ul> <p>Overview:</p> <p>Minikube is essentially a tool that runs a single-node Kubernetes cluster locally inside a VM (by default). This makes it perfect for users looking to get a taste of Kubernetes without the complications of setting up a multi-node cluster.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/09/13/Gen3%20on%20laptop/#2-docker-desktop","title":"2. Docker Desktop","text":"<p>Pros:</p> <ul> <li>Comes integrated with Docker, a popular containerization tool.</li> <li>Provides Kubernetes out-of-the-box, no additional installation required.</li> <li>Does not require a VM for macOS and Windows.</li> </ul> <p>Cons:</p> <ul> <li>Limited to a single node.</li> <li>Might not support all K8s features.</li> </ul> <p>Overview:</p> <p>Docker Desktop, available for both Windows and macOS, offers a simple way to start a Kubernetes cluster. By simply checking a box in the settings, you get a single-node K8s cluster running alongside your Docker containers.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/09/13/Gen3%20on%20laptop/#3-kind-kubernetes-in-docker","title":"3. Kind (Kubernetes IN Docker)","text":"<p>Pros:</p> <ul> <li>Runs K8s clusters using Docker containers as nodes.</li> <li>Lightweight and fast.</li> <li>Can simulate multi-node clusters.</li> </ul> <p>Cons:</p> <ul> <li>Might be slightly more complex for beginners.</li> <li>Intended primarily for testing Kubernetes itself.</li> </ul> <p>Overview:</p> <p>Kind is an innovative solution that allows you to run Kubernetes clusters where each node is a Docker container. It\u2019s especially useful for CI/CD pipelines and testing Kubernetes itself.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/09/13/Gen3%20on%20laptop/#4-microk8s","title":"4. MicroK8s","text":"<p>Pros:</p> <ul> <li>Lightweight and fast.</li> <li>Single command installation.</li> <li>Offers various add-ons for enhanced functionality.</li> </ul> <p>Cons:</p> <ul> <li>Limited to Linux.</li> <li>Not as widely adopted as other solutions.</li> </ul> <p>Overview:</p> <p>MicroK8s is a minimal Kubernetes distribution aimed at developers and edge computing. It's a snap package, which makes it extremely simple to install on any Linux distribution.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/09/13/Gen3%20on%20laptop/#5-k3s","title":"5. K3s","text":"<p>Pros:</p> <ul> <li>Extremely lightweight.</li> <li>Simple to install and run.</li> <li>Suitable for edge, IoT, and CI.</li> </ul> <p>Cons:</p> <ul> <li>Strips out certain default K8s functionalities to remain light.</li> </ul> <p>Overview:</p> <p>K3s is a lightweight version of Kubernetes. It's designed for use cases where resources are a constraint or where you don't need the full feature set of standard Kubernetes.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/09/13/Gen3%20on%20laptop/#6-rancher-desktop","title":"6. Rancher Desktop","text":"<p>Pros:</p> <ul> <li>Provides a user-friendly GUI for managing Kubernetes clusters.</li> <li>Supports multi-node clusters.</li> <li>Offers integration with Rancher for enhanced Kubernetes management.</li> <li>Works on Windows, macOS, and Linux.</li> </ul> <p>Cons:</p> <ul> <li>Requires additional setup compared to some other options.</li> <li>May consume more resources for multi-node clusters.</li> </ul> <p>Overview:</p> <p>Rancher Desktop is a versatile tool that simplifies the management of Kubernetes clusters on your local machine. It offers a user-friendly graphical interface, making it an excellent choice for users who prefer a visual approach to Kubernetes cluster management. Rancher Desktop can set up and manage multi-node clusters, which can be valuable for testing and development scenarios. Additionally, it integrates seamlessly with Rancher, providing even more advanced Kubernetes management capabilities.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/09/13/Gen3%20on%20laptop/#conclusion","title":"Conclusion","text":"<p>Running Kubernetes on your laptop is feasible and offers a variety of methods, each catering to different use cases. Whether you\u2019re a developer wanting to test out your applications, an enthusiast keen on learning Kubernetes, or even someone looking to set up CI/CD pipelines, there's an option for you.</p> <p>It's essential to weigh the pros and cons of each method, consider your resource limitations, and the scope of your projects. Regardless of the option you choose, diving into the world of Kubernetes is an enriching experience, offering a deep dive into modern cloud-native development and operations.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2024/10/15/observability/","title":"Observability","text":"","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2024/10/15/observability/#deploying-a-comprehensive-observability-stack-with-helm","title":"Deploying a Comprehensive Observability Stack with Helm","text":"<p>Monitoring and observability are essential for maintaining modern infrastructure and applications. With the new Observability Helm Chart, setting up a robust monitoring system is easier than ever. This chart provides an integrated stack featuring Grafana for visualizations, Loki for log aggregation, and Mimir for metrics storage and querying. Alloy can then be deployed in any cluster to collect logs and metrics to foward to Loki and Mimir. Additionally, you can optionally deploy the Faro Collector Helm Chart to further enhance observability by supporting Real User Monitoring (RUM) via the Fence Service.</p>","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2024/10/15/observability/#overview-of-the-observability-helm-chart","title":"Overview of the Observability Helm Chart","text":"<p>The Observability Helm Chart deploys a complete observability solution to your Kubernetes cluster. It bundles three core components:</p>","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2024/10/15/observability/#grafana","title":"Grafana:","text":"<p>An industry-leading visualization platform that allows users to create dashboards, track metrics, and set alerts.</p>","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2024/10/15/observability/#mimir","title":"Mimir:","text":"<p>A scalable time-series database optimized for efficiently storing and querying metrics across applications and infrastructure.</p>","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2024/10/15/observability/#loki","title":"Loki:","text":"<p>A log aggregation system designed to index and query logs with minimal resource usage, seamlessly integrating with Grafana.</p>","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2024/10/15/observability/#general-architecture","title":"General Architecture","text":"<p>In this setup, Loki and Mimir are configured with internal ingress resources, enabling Alloy to send metrics and logs securely via VPC peering connections. Both Loki and Mimir write the ingested data to Amazon S3 for scalable and durable storage. This data can be queried and visualized through Grafana, which is hosted behind an internet-facing ingress. Access to Grafana can be restricted using CIDR ranges defined through the ALB ingress annotation: alb.ingress.kubernetes.io/inbound-cidrs: \"cidrs\". Additionally, the chart supports SAML authentication for Grafana, configured through the grafana.ini field, ensuring secure user access.</p> <p></p>","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2024/10/15/observability/#fips-compliant-images","title":"Fips compliant images","text":"<p>Gen3 provides FIPS-compliant images, which are set as the default in the values file for Grafana, Mimir, and Loki. These images are self-hosted and maintained by the Gen3 Platform Team, ensuring secure and compliant operations. The Platform Team is responsible for managing image upgrades, and service versions will be updated as deemed necessary by the team.</p>","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2024/10/15/observability/#built-in-gen3-alerts","title":"Built-in Gen3 Alerts","text":"<p>This Helm chart comes equipped with built-in Gen3 alerts, defined in the 'alerting' section of the values.yaml. These alerts enable you to immediately leverage your logs and metrics as soon as Grafana is up and running.</p>","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2024/10/15/observability/#built-in-gen3-dashboards","title":"Built-in Gen3 Dashboards","text":"<p>You can utilize Gen3-specific visualizations by visiting our grafana-dashboards repo</p>","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2024/10/15/observability/#alloy-and-faro-enhancing-observability","title":"Alloy and Faro: Enhancing Observability","text":"","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2024/10/15/observability/#alloy","title":"Alloy:","text":"<p>Collects logs and metrics from your services and sends them to Loki and Mimir for storage and analysis. Alloy acts as a bridge between your services and the observability stack, ensuring data flows smoothly to the right destinations.</p>","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2024/10/15/observability/#faro-collector","title":"Faro Collector:","text":"<p>A specialized configuration of Alloy designed to collect Real User Monitoring (RUM) data from Grafana Faro. This setup captures frontend metrics.</p>","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2024/10/15/observability/#helm-charts-overview","title":"Helm Charts Overview","text":"<p>Observability Helm Chart: Deploys Grafana, Loki, and Mimir as the foundation of your observability platform.</p> <p>Alloy Helm Chart: Configures Alloy to collect logs and metrics and forward them to Loki and Mimir. Alloy can be deployed in a separate cluster or VPC or it can be deployed in multiple clusters/vpcs.</p> <p>Faro Collector Helm Chart: Adds RUM data collection to the stack by configuring Alloy to receive frontend metrics from Grafana Faro.</p>","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2024/10/15/observability/#conclusion","title":"Conclusion","text":"<p>This new suite of Helm charts provides everything you need to monitor your Gen3 instance.</p> <p>To see detailed instructions on how to set up these charts, please refer to the following links:</p> <ul> <li>Observability</li> <li>Alloy</li> <li>Faro</li> </ul>","tags":["Observability","Helm Chart","Grafana","Loki","Mimir","Alloy","Faro Collector","Real User Monitoring (RUM)","Metrics","Log Aggregation","Kubernetes","Dashboards","Kubernetes Monitoring","Time-Series Database","Alerting","Frontend Monitoring","Grafana Dashboards","Open Source Monitoring"]},{"location":"blog/2023/08/13/k8s%20for%20development/","title":"Choosing the Right Lightweight Kubernetes Tool for Local Development","text":"","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/08/13/k8s%20for%20development/#choosing-the-right-lightweight-kubernetes-tool-for-local-development","title":"Choosing the Right Lightweight Kubernetes Tool for Local Development","text":"<p>Kubernetes, the popular container orchestration platform, is a cornerstone of modern development and deployment. However, running Kubernetes locally for development and testing purposes requires efficient tools that don't consume excessive resources. In this article, we'll explore several lightweight Kubernetes tools for local development and discuss their pros and cons.</p> <p>Of course, getting every bell and whistle working (like that handy ingress feature that routes external traffic around the cluster) might need some extra tweaking on a basic laptop setup. But hey, half the fun is figuring out how to configure your local environment to really sing, right? As we look at tools for local dev, we'll hit on ways to tune things up for peak Gen3 performance.</p> <p>When it comes to local Kubernetes development, several solid options exist for standing up a dev cluster directly on your laptop. In this blog post, we will explore popular choices!</p> <p>Now, here's the cool part - Gen3 works on any Kubernetes cluster, whether you've just spun one up on your laptop or have a full-blown production cluster. That means you can kick the tires locally before taking it out for a spin in the real world.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/08/13/k8s%20for%20development/#kind-kubernetes-in-docker","title":"Kind (Kubernetes IN Docker)","text":"<p>Overview: Kind runs Kubernetes inside a Docker container, making it an excellent choice for local development and testing. It is also used by the Kubernetes team to test Kubernetes itself.</p> <p>Pros: - Fast cluster creation (around 20 seconds). - Robust and reliable, thanks to containerd usage. - Suitable for CI environments (e.g., TravisCI, CircleCI).</p> <p>Cons: - Ingress controllers needs to be deployed manually</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/08/13/k8s%20for%20development/#preferred-for-gen3","title":"Preferred for gen3","text":"<p>In my experience, this is the most preferred method for running Gen3 on a laptop especially when paired up with OrbStack instead of Docker/Rancher desktop. I use this as my preffered K8s on my M1 Macbook.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/08/13/k8s%20for%20development/#docker-for-desktop","title":"Docker for Desktop","text":"<p>Overview: Docker for Desktop is an accessible option for MacOS users. Enabling Kubernetes in the Docker For Mac preferences allows you to run Kubernetes locally.</p> <p>Pros: - Widely used and well-supported. - No additional installations required. - Built images are immediately available in-cluster.</p> <p>Cons: - Resource-intensive due to docker-shim usage. - Difficult to customize and troubleshoot.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/08/13/k8s%20for%20development/#microk8s","title":"MicroK8s","text":"<p>Overview: MicroK8s is recommended for Ubuntu users. It is installed using Snap and includes useful plugins for easy configuration.</p> <p>Pros: - Minimal overhead on Linux (no VM). - Simplified configuration with plugins. - Supports a local image registry for fast image management.</p> <p>Cons: - Resetting the cluster is slow and can be error-prone. - Best optimized for Ubuntu, may be less stable on other platforms.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/08/13/k8s%20for%20development/#rancher-desktop","title":"Rancher Desktop","text":"<p>Overview: Rancher Desktop is an open-source alternative to Docker Desktop. It uses containerd by default and offers flexibility in choosing a container runtime.</p> <p>Pros: - Cross-platform (MacOS/Linux/Windows). - Utilizes k3s, known for its speed and resource efficiency. - Ingress with Traefik works out of the box</p> <p>Cons: - Rapidly evolving, not fully supported by all tools.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/08/13/k8s%20for%20development/#minikube","title":"Minikube","text":"<p>Overview: Minikube is a versatile option offering high fidelity and customization. It supports various Kubernetes versions, container runtimes, and more.</p> <p>Pros: - Feature-rich local Kubernetes solution. - Customizable with multiple options. - Supports a local image registry for efficient image handling.</p> <p>Cons: - Initial setup complexity, especially with VM drivers. - Some advanced options may require manual configuration. - Resource-intensive if using a VM.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/08/13/k8s%20for%20development/#k3d","title":"k3d","text":"<p>Overview: k3d runs k3s, a lightweight Kubernetes distribution, inside a Docker container. k3s removes optional and legacy features while maintaining compatibility with full Kubernetes.</p> <p>Pros: - Extremely fast startup (less than 5 seconds on most machines). - Built-in local registry optimized for Tilt.</p> <p>Cons: - Less widely used, leading to limited documentation. - Some tools may have slower adoption.</p> <p>In conclusion, choosing the right lightweight Kubernetes tool for your local development depends on your specific needs and preferences. Each tool offers a unique set of advantages and drawbacks, so consider your project requirements and platform compatibility when making your decision.</p> <p>Feel free to experiment with these tools and share your experiences in the Kubernetes development journey!</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/10/13/k8s%20tools/","title":"Boost Your K8s Productivity with These Handy Tools","text":"","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/10/13/k8s%20tools/#boost-your-k8s-productivity-with-these-handy-tools","title":"Boost Your K8s Productivity with These Handy Tools","text":"<p>Managing Kubernetes clusters and resources can get complicated quickly. Thankfully, there are some great open source tools that make working with k8s much easier. In this post, I'll highlight some of my favorite k8s productivity boosters.</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/10/13/k8s%20tools/#kubectl-aliases","title":"kubectl Aliases","text":"<p>One of the first things I do when setting up my workstation to work with Kubernetes environments is create a set of aliases for common kubectl commands. This saves a ton of typing! Some useful aliases include:</p> Text Only<pre><code>alias k=kubectl\nalias kg=kubectl get\nalias kgp=kubectl get pod\nalias kd=kubectl describe\nalias ke=kubectl edit\n</code></pre> Full list of aliases! Text Only<pre><code>if (( $+commands[kubectl] )); then\n    __KUBECTL_COMPLETION_FILE=\"${ZSH_CACHE_DIR}/kubectl_completion\"\n\n    if [[ ! -f $__KUBECTL_COMPLETION_FILE ]]; then\n        kubectl completion zsh &gt;! $__KUBECTL_COMPLETION_FILE\n    fi\n\n    [[ -f $__KUBECTL_COMPLETION_FILE ]] &amp;&amp; source $__KUBECTL_COMPLETION_FILE\n\n    unset __KUBECTL_COMPLETION_FILE\nfi\n\n# This command is used a LOT both below and in daily life\nalias k=kubectl\n\n# Execute a kubectl command against all namespaces\nalias kca='f(){ kubectl \"$@\" --all-namespaces;  unset -f f; }; f'\n\n# Apply a YML file\nalias kaf='kubectl apply -f'\n\n# Drop into an interactive terminal on a container\nalias keti='kubectl exec -ti'\n\n# Manage configuration quickly to switch contexts between local, dev ad staging.\nalias kcuc='kubectl config use-context'\nalias kcsc='kubectl config set-context'\nalias kcdc='kubectl config delete-context'\nalias kccc='kubectl config current-context'\n\n# List all contexts\nalias kcgc='kubectl config get-contexts'\n\n# General aliases\nalias kdel='kubectl delete'\nalias kdelf='kubectl delete -f'\n\n# Pod management.\nalias kgp='kubectl get pods'\nalias kgpw='kgp --watch'\nalias kgpwide='kgp -o wide'\nalias kep='kubectl edit pods'\nalias kdp='kubectl describe pods'\nalias kdelp='kubectl delete pods'\n\n# get pod by label: kgpl \"app=myapp\" -n myns\nalias kgpl='kgp -l'\n\n# Service management.\nalias kgs='kubectl get svc'\nalias kgsw='kgs --watch'\nalias kgswide='kgs -o wide'\nalias kes='kubectl edit svc'\nalias kds='kubectl describe svc'\nalias kdels='kubectl delete svc'\n\n# Ingress management\nalias kgi='kubectl get ingress'\nalias kei='kubectl edit ingress'\nalias kdi='kubectl describe ingress'\nalias kdeli='kubectl delete ingress'\n\n# Namespace management\nalias kgns='kubectl get namespaces'\nalias kens='kubectl edit namespace'\nalias kdns='kubectl describe namespace'\nalias kdelns='kubectl delete namespace'\nalias kcn='kubectl config set-context $(kubectl config current-context) --namespace'\n\n# ConfigMap management\nalias kgcm='kubectl get configmaps'\nalias kecm='kubectl edit configmap'\nalias kdcm='kubectl describe configmap'\nalias kdelcm='kubectl delete configmap'\n\n# Secret management\nalias kgsec='kubectl get secret'\nalias kdsec='kubectl describe secret'\nalias kdelsec='kubectl delete secret'\n\n# Deployment management.\nalias kgd='kubectl get deployment'\nalias kgdw='kgd --watch'\nalias kgdwide='kgd -o wide'\nalias ked='kubectl edit deployment'\nalias kdd='kubectl describe deployment'\nalias kdeld='kubectl delete deployment'\nalias ksd='kubectl scale deployment'\nalias krsd='kubectl rollout status deployment'\nkres(){\n    kubectl set env $@ REFRESHED_AT=$(date +%Y%m%d%H%M%S)\n}\n\n# Rollout management.\nalias kgrs='kubectl get rs'\nalias krh='kubectl rollout history'\nalias kru='kubectl rollout undo'\n\n# Port forwarding\nalias kpf=\"kubectl port-forward\"\n\n# Tools for accessing all information\nalias kga='kubectl get all'\nalias kgaa='kubectl get all --all-namespaces'\n\n# Logs\nalias kl='kubectl logs'\nalias klf='kubectl logs -f'\n\n# File copy\nalias kcp='kubectl cp'\n\n# Node Management\nalias kgno='kubectl get nodes'\nalias keno='kubectl edit node'\nalias kdno='kubectl describe node'\nalias kdelno='kubectl delete node'\n</code></pre> <p>I stole my k8s aliases from a Github Gist. Huge shoutout to Github User doevelopper</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/10/13/k8s%20tools/#k9s","title":"k9s","text":"<p>https://k9scli.io</p> <p>k9s provides a terminal UI for interacting with your Kubernetes clusters. It's great for get a quick overview of pods, nodes, services etc. Some of the handy features include:</p> <ul> <li>Live filtering of resources</li> <li>Easy log viewing</li> <li>Executing containers</li> <li>Resource editing</li> </ul> <p>k9s makes it super easy to manage Kubernetes in a terminal-centric workflow.</p> <p></p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"blog/2023/10/13/k8s%20tools/#kubectx-and-kubens","title":"kubectx and kubens","text":"<p>kubectx and kubens allow you to quickly switch between Kubernetes contexts and namespaces. This comes in handy when you're working with multiple clusters or namespaces.</p> <p>Some examples:</p> <p>kubens staging - switch to staging namespace kubectx minikube - change context to minikube cluster</p> <p>No more typing out full context and namespace names!</p> <p>Here's a kubectx demo: </p> <p>...and here's a kubens demo: </p> <p>Credit: Created and released by Ahmet Alp Balkan</p>","tags":["Kubernetes","Minikube","Kind","K3s","Local Development","Lightweight Kubernetes","Container Orchestration","CI/CD","Development Tools","Docker","Rancher Desktop","Kubernetes Clusters","DevOps","Kubernetes Management","Containerization","Microservices","Kubernetes Alternatives","Kubernetes Learning","Kubernetes Testing","Resource Optimization"]},{"location":"gen3-resources/about/","title":"About Gen3","text":""},{"location":"gen3-resources/about/#what-is-gen3","title":"What is Gen3?","text":"<p>Gen3 is a data platform for building data commons and data meshes. The Gen3 platform consists of open-source software services that support the emergence of healthy data ecosystems by enabling the creation of cloud-based data resources, including data commons and analysis workspaces. Gen3 aims to accelerate and democratize the process of scientific discovery by making it easy to manage, analyze, harmonize, and share large and complex datasets in the cloud.</p> <p>You can find more information about Gen3 on gen3.org.</p>"},{"location":"gen3-resources/about/#powered-by-gen3","title":"Powered by Gen3","text":"<p>Gen3 is used by dozen of projects around the world to manage and analyze biomedical data.  You can find an incomplete list of current Gen3 projects here.</p>"},{"location":"gen3-resources/about/#license","title":"License","text":"<p>Gen3 is licensed under an Apache 2.0 license, which is a permissive license whose main conditions require preservation of copyright and license notices. Contributors provide an express grant of patent rights. Licensed works, modifications, and larger works may be distributed under different terms and without source code.</p> <p>The full text of the license can be found in each Gen3 repository and here.</p>"},{"location":"gen3-resources/about/#contact-information","title":"Contact Information","text":""},{"location":"gen3-resources/about/#gen3-slack","title":"Gen3 Slack","text":"<p>Gen3 Slack is the preferred medium of correspondence if you have any question about the set up or use of a Gen3 system.  Slack offers the benefit of everyone learning from others' questions.  It also allows community members to contribute to a response.  You can sign up for the Gen3 slack workspace here.</p>"},{"location":"gen3-resources/about/#gen3-discussion-board","title":"Gen3 Discussion Board","text":"<p>The Gen3 Discussion board allows you to post questions on a variety of topics to get feedback from the community.</p>"},{"location":"gen3-resources/about/#ctds-help-desk","title":"CTDS Help Desk","text":"<p>To reach a member of the CTDS User Services Team you may contact us via email at support@gen3.org.  For general questions about CTDS, please use ctds@uchicago.edu.</p>"},{"location":"gen3-resources/coming-soon/","title":"Coming Soon!","text":""},{"location":"gen3-resources/coming-soon/#coming-soon","title":"Coming Soon!","text":""},{"location":"gen3-resources/coming-soon/#from-a-user-services-person-near-you","title":"from a User Services person near you!","text":""},{"location":"gen3-resources/community/","title":"Gen3 Community","text":""},{"location":"gen3-resources/community/#gen3-community","title":"Gen3 Community","text":"<p>Welcome to the Gen3 Community! Learn from our community members and engage in technical discussions. Gen3 is an open-source product and we rely on and encourage our community of users, developers, and operators to actively participate in community activities.</p>"},{"location":"gen3-resources/community/#code-of-conduct","title":"Code of Conduct","text":"<p>The Gen3 Community values respect and professionalism. Our purpose is to share knowledge and make everyone feel safe and included.  Please review our code of conduct here.</p>"},{"location":"gen3-resources/community/#community-events","title":"Community Events","text":"<p>About every other month, there is a virtual Gen3 Forum for the community of Gen3 developers, operators, sponsors and users of Gen3 data platforms. These events aim to share information about how to set up new Gen3 instances, build a community that can help each other, and create clear paths for support from the Gen3 core development team.</p> <p>Check out the community events page for upcoming and past events.</p>"},{"location":"gen3-resources/community/#source-code","title":"Source Code","text":"<p>View repositories for all Gen3 services on GitHub.</p>"},{"location":"gen3-resources/community/#release-notes","title":"Release Notes","text":"<p>Gen3 releases typically happen every month. The latest release notes can be found here.</p>"},{"location":"gen3-resources/community/#contribute-to-gen3","title":"Contribute to Gen3","text":"<p>Learn how to contribute code and documentation improvements to Gen3 here.</p>"},{"location":"gen3-resources/community/#contact","title":"Contact","text":"<p>Check out the contact page for more information on how to engage with the Gen3 community and the code maintainers at CTDS.</p>"},{"location":"gen3-resources/community/#gen3-videos","title":"Gen3 Videos","text":"<p>Explore our videos and webinars on YouTube to learn more about Gen3 and our community.</p>"},{"location":"gen3-resources/community/#gen3-publications","title":"Gen3 Publications","text":"<p>See a sample of the publications made possible by Gen3 systems here.</p>"},{"location":"gen3-resources/faq/","title":"Frequently Asked Questions","text":""},{"location":"gen3-resources/faq/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"gen3-resources/faq/#data-dictionary","title":"Data Dictionary","text":""},{"location":"gen3-resources/faq/#im-customizing-the-data-schema-where-i-can-find-information-about-the-data-dictionary-customization-variables-definition-and-files-formatting","title":"I'm customizing the data schema, where I can find information about the data dictionary customization, variables' definition, and files formatting?","text":"<p>Please refer documentation about Creating New Dictionary and Data Types.</p>"},{"location":"gen3-resources/faq/#i-have-a-dictionary-in-my-local-schemas-folder-how-can-i-generate-the-schemajson","title":"I have a dictionary in my local schemas folder, how can I generate the <code>schema.json</code>?","text":"<p>You can use our dictionaryutils tool.  You can find an example on how to generate JSON in the README</p>"},{"location":"gen3-resources/faq/#cloud-automation-legacy","title":"Cloud Automation (Legacy)","text":""},{"location":"gen3-resources/faq/#im-setting-up-a-csoc-free-instance-what-should-i-set-for-the-config_folder","title":"I'm setting up a CSOC-free instance, what should I set for the <code>config_folder</code>?","text":"<p>The <code>config_folder</code> variable sets the location of your <code>user.yaml</code> file. If you are using a local file, you can put any value - it won't be used. If you are using any of our <code>user.yaml</code> files, then you may need to know which one. If you are neither using ours nor local, you can also put a random value, and then later when deploying kubernetes, you will be able to point the location of your permissions file.</p>"},{"location":"gen3-resources/faq/#i-modified-the-useryaml-file-how-do-i-update-arborist","title":"I modified the <code>user.yaml</code> file. How do I update Arborist?","text":"<p>If you run <code>gen3 job run usersync</code> changes should be picked up automatically. If you don't want to run that command every time you make changes to your <code>user.yaml</code>, you can deploy the cronjob <code>gen3 job run usersync-cronjob</code> and kubernetes will check for changes every hour and update services accordingly.</p>"},{"location":"gen3-resources/faq/#how-do-i-change-the-title-of-my-webpage-from-the-default-generic-data-commons-automation","title":"How do I change the title of my webpage from the default \"Generic Data Commons\"? Automation?","text":"<p>You can override the default configuration in your commons by using <code>gitops.json</code> file. Set your <code>portal_app</code> to <code>gitops</code> in your file <code>cdis-manifest/your-commons-name/manifest.json</code>. Then create <code>cdis-manifest/your-commons-name/portal/gitops.json</code> file. You can use this file as a template for your own <code>gitops.json</code> file. Set navigation title and login title at your choice. Then reroll portal with <code>gen3 kube-setup-portal</code>.</p>"},{"location":"gen3-resources/faq/#i-updated-the-fence-configuration-why-havent-the-changes-taken-effect","title":"I updated the Fence configuration. Why haven't the changes taken effect?","text":"<p>After updating the Fence configuration, run <code>kubectl delete secret fence-config</code>, <code>gen3 kube-setup-fence</code> and roll Fence for configuration changes to take effect with <code>gen3 roll fence</code>.</p>"},{"location":"gen3-resources/faq/#i-receive-errors-when-running-queries-on-the-graphiql-query-page-eg-cannot-query-field-field-on-type-root","title":"I receive errors when running queries on the GraphiQL \"Query\" page, e.g., Cannot query field \"field\" on type \"Root\".","text":"<p>You may need to hit the <code>Switch to Graph Model</code> button in the upper right corner.</p>"},{"location":"gen3-resources/faq/#how-do-i-run-sns-services","title":"How do I run SNS services?","text":"<p>Please refer to Cloud Automation documentation.</p>"},{"location":"gen3-resources/faq/#arborist-and-sheepdog-dont-work-as-expected-what-should-i-check","title":"Arborist and Sheepdog don't work as expected, what should I check?","text":"<p><code>kubectl logs &lt;name of pod&gt;</code> can show you corresponding errors in the logs.</p>"},{"location":"gen3-resources/faq/#ive-updated-the-data-dictionary-what-services-should-i-restart-for-the-change-to-take-effect","title":"I've updated the data dictionary, what services should I restart for the change to take effect?","text":"<p>First, restart Peregrine <code>gen3 roll peregrine</code> and Sheepdog <code>gen3 roll sheepdog</code>.  After these two services restarted, please portal with <code>gen3 roll portal</code>.  Please note, if you are making significant changes to the dictionary, you will also need to update <code>portal_app</code> in the manifest.</p>"},{"location":"gen3-resources/faq/#i-branched-out-the-windmill-portal-should-i-update-manifest-with-a-new-version-of-portal","title":"I branched out the Windmill portal, should I update manifest with a new version of portal?","text":"<p>Yes, you can update the manifest to use your version of the portal and use <code>gen3\\ kube-setup-portal</code> or <code>gen3 roll portal</code> to changes to take effect.</p>"},{"location":"gen3-resources/faq/#how-can-i-update-the-app-for-the-portal","title":"How can I update the APP for the portal?","text":"<p>You can set the <code>global.portal_app</code> setting in your manifest to the name of your windmill configuration file and run <code>gen3 kube-setup-portal</code>.</p>"},{"location":"gen3-resources/faq/#what-do-i-need-to-restart-if-i-updated-fence-configuration-with-oauth-credentials","title":"What do I need to restart if I updated Fence configuration with OAuth credentials?","text":"<p>Please run <code>kubectl delete secret fence-config</code> and <code>gen3 kube-setup-fence</code>.  If you also updated the <code>user.yaml</code> in the folder <code>apis_configs</code>, you need to run <code>gen3 job run useryaml-job</code>.</p>"},{"location":"gen3-resources/faq/#im-storing-useryaml-in-s3-not-locally-and-updated-it-how-do-i-ensure-changes-take-effect","title":"I'm storing <code>user.yaml</code> in S3 (not locally) and updated it, how do I ensure changes take effect?","text":"<p>After you change <code>user.yaml</code>, run <code>gen3 kube-setup-secrets</code> and <code>gen3 job run usersync</code>.</p>"},{"location":"gen3-resources/faq/#i-see-my-data-on-a-query-page-but-not-in-the-exploration-page-what-other-services-do-i-need-to-deployconfigure-to-have-it-running","title":"I see my data on a Query page, but not in the Exploration page. What other services do I need to deploy/configure to have it running?","text":"<p>The Exploration page needs to be set up with Elastic Search using Guppy. Please refer to the relevant Guppy and Cloud Automation documentation, and see an Sample Configuration for the Guppy.</p>"},{"location":"gen3-resources/faq/#compose-services-deprecated","title":"Compose Services (Deprecated)","text":""},{"location":"gen3-resources/faq/#i-installed-compose-services-how-do-i-create-a-program-and-a-project","title":"I installed Compose Services, how do I create a program and a project?","text":"<p>Please refer to corresponding Gen3 Documentation to create your first program and project.</p>"},{"location":"gen3-resources/faq/#i-created-a-program-but-i-see-a-notfound-error-when-i-try-to-create-a-project-within-my-program","title":"I created a program, but I see a <code>NotFound</code> error when I try to create a project within my program.","text":"<p>Please make sure you inserted program <code>name</code> in the URL, not <code>dbgap_accession_number</code>.</p>"},{"location":"gen3-resources/faq/#i-can-not-submit-data-to-the-project-i-just-created","title":"I can not submit data to the project I just created.","text":"<p>Please ensure you granted yourself permissions in the <code>user.yaml</code> file. You can find this file in the <code>Secrets</code> folder within your Compose Services folder. Follow provided in the file example to grant yourself relevant permissions. You can find more information in the Setting Up Users documentation.</p>"},{"location":"gen3-resources/faq/#useryaml-contains-information-about-programproject-but-i-still-cannot-access-it","title":"User.yaml contains information about program/project, but I still cannot access it.","text":"<p>Please ensure you are using <code>dbgap_accession_number</code> value as an identifier for the <code>auth_id</code> field, not program's <code>name</code> or project's <code>code</code>.</p>"},{"location":"gen3-resources/faq/#i-see-a-warning-about-my-connection-not-being-private","title":"I see a warning about my connection not being private.","text":"<p>Don't worry. The warning about connection not being private appears because <code>creds_setup.sh</code> script generated self-signed SSL certificate to allow microservices to communicate with each other without bypassing SSL verification. Your browser can not verify this certificate. Once you install a trusted SSL certificate instead of generated by our script, this warning will go away.</p>"},{"location":"gen3-resources/faq/#i-want-to-run-compose-services-on-the-server-how-do-i-configure-it","title":"I want to run Compose Services on the server, how do I configure it?","text":"<p>To configure Compose Services with your domain, please change the <code>BASE_URL</code> field in the <code>fence_config.yaml</code> file, and the <code>hostname</code> field in the <code>peregrine.json</code> and <code>sheepdog.json</code> files.</p>"},{"location":"gen3-resources/faq/#im-trying-to-upload-files-with-gen3-client-but-received-an-error-x509-certificate-signed-by-unknown-authority","title":"I'm trying to upload files with gen3-client, but received an error <code>x509: certificate signed by unknown authority</code>.","text":"<p>You can add self-signed certificate to your trusted certificates. Steps may vary depending on Operating System you use.</p>"},{"location":"gen3-resources/glossary/","title":"Glossary","text":""},{"location":"gen3-resources/glossary/#glossary","title":"Glossary","text":""},{"location":"gen3-resources/glossary/#allow-lists","title":"Allow Lists","text":"<p>An allow list is simply a list of users (identified based on your method of authentication) that controls which users have access to which data.  It is in the form of a user.yaml file that is maintained by the operator of your Gen3 system. Gaining access may require you to sign a Data Use Agreement.  Data access is granted at the program or project level.  </p> <p>Alternatively, an allow list may also refer to a list of authorized tools within a Gen3 workspace.</p>"},{"location":"gen3-resources/glossary/#api","title":"API","text":"<p>Gen3 services expose APIs (or Application Programming Interface), which allows users to interact directly with the system or data without using the Data Portal or GUI.  An Open API refers to an API that does not require any authentication.  </p>"},{"location":"gen3-resources/glossary/#commons-services-operations-center-csoc","title":"Commons Services Operations Center (CSOC)","text":"<p>A Common Services Operations Center is an operations center operated by a commons services provider for setting up, configuring, operating, and monitoring data commons, data meshes, data hubs, and other data platforms for managing, analyzing, and sharing data.</p>"},{"location":"gen3-resources/glossary/#crosswalk","title":"Crosswalk","text":"<p>Typically, used for linking patients from across data commons where some patient data exists in commons A and additional data exists in commons B. This linkage enables metadata associations across commons and the promise of richer datasets.  Crosswalks can be made for several types of metadata and are recorded in the metadata service. An example of how to set this up is found here.</p>"},{"location":"gen3-resources/glossary/#data-commons","title":"Data Commons","text":"<p>A data commons co-locates data with cloud computing infrastructure and commonly used software services, tools, and applications for managing, integrating, analyzing and sharing data that are exposed through web portals and APIs to create an interoperable resource for a research community. A data commons provides services so that the data is findable, accessible, interoperable and reusable (FAIR)</p>"},{"location":"gen3-resources/glossary/#data-dictionary","title":"Data Dictionary","text":"<p>Every Gen3 data commons employs a data model, which serves to describe, organize, and harmonize data sets submitted by different users. Data harmonization facilitates cross-project analyses and is thus one of the pillars of the data commons paradigm. The data model organizes experimental metadata variables, \u201cproperties\u201d, into linked categories, \u201cnodes\u201d, through the use of a data dictionary. The data dictionary lists and describes all nodes in the data model, as well as defines and describes the properties in each node. A Gen3 Data Dictionary is specified by a YAML file per node. Additional details on Gen3 data dictionaries can be found here.</p>"},{"location":"gen3-resources/glossary/#data-hub","title":"Data Hub","text":"<p>A data hub is a data platform in a data mesh that supports search and discovery of data in one, two, or more data platforms in a data mesh. Typically, the data itself remains in the data repository, data commons, or other data platform in the mesh, but the metadata for the data is typically replicated in the data hub. Data hubs often are connected to workspaces or analysis environments operated by the data hub so the data can be analyzed.</p>"},{"location":"gen3-resources/glossary/#data-mesh-aka-data-fabric-or-data-ecosystem","title":"Data Mesh (aka Data Fabric or Data Ecosystem)","text":"<p>Data meshes contain multiple data commons, data repositories, or data resources that can interoperate using a common set of services. Each data commons or repository may have distinct data types, data models, and data governance.  A mesh provides for discovery of data from connected repositories and allows users to bring this data together in one place for joint analysis. To see how to set up the necessary services for running a data mesh view instructions here.</p>"},{"location":"gen3-resources/glossary/#data-types","title":"Data Types","text":""},{"location":"gen3-resources/glossary/#structured-data","title":"Structured Data","text":"<p>Data is structured if it is organized into records and fields, with each record consisting of one or more data elements (data fields). In biomedical data, data fields are often restricted to controlled vocabularies to make querying them easier. In Gen3 this would include clinical or experimental data submitted to the graph model, which is queriable via a GraphQL API.  It can be flattened (via ETL) and the result viewable on the Data Portal Exploration Page.</p>"},{"location":"gen3-resources/glossary/#semi-structured-data","title":"Semi-structured Data","text":"<p>Semi-structured data is organized as unique identifiers with flexible key/value pairs (including nesting). The key/value pairs may be consistent between records, but are not required to be. This is typically used for storing publicly available metadata about available datasets or additional public metadata about samples.  The MDS and AggMDS both include semi-structured data and power the Discovery Page.</p>"},{"location":"gen3-resources/glossary/#unstructured-data","title":"Unstructured Data","text":"<p>Unstructured data represents files on a hard drive or cloud storage with no consistent schema. These data tend to represent either bulk clinical and phenotypic data in spreadsheet format or patient level data such as images or genomic sequencing files.</p>"},{"location":"gen3-resources/glossary/#data-portal-pages","title":"Data Portal Pages","text":"<p>See an in-depth description of each page here.</p>"},{"location":"gen3-resources/glossary/#data-dictionary-page","title":"Data Dictionary Page","text":"<p>Interactive page that shows the data dictionary in both a graph and table format.  It is powered by the data dictionary YAML files and allows users to see the controlled vocabulary across all data model nodes.</p>"},{"location":"gen3-resources/glossary/#discovery-page","title":"Discovery Page","text":"<p>Powered by the metadata service.  Typically includes public metadata at the project level including search fields, tags, and study page fields.</p>"},{"location":"gen3-resources/glossary/#explorer-page","title":"Explorer Page","text":"<p>Primary page for exploring data within a Gen3 data commons.  It is powered by flattened data within the Guppy microservice.  It is highly configurable based on operator requirements for building cohorts.</p>"},{"location":"gen3-resources/glossary/#landing-page","title":"Landing Page","text":"<p>Contains some basic summary text and optionally a histogram of data pulled from Guppy (flattened data)</p>"},{"location":"gen3-resources/glossary/#dois","title":"DOIs","text":"<p>A Digital Object Identifier is an identifier used to permanently and stably identify (usually digital) objects. DOIs provide a standard mechanism for retrieval of metadata about the object, and generally a means to access the data object itself. A DOI is a specific ISO standard for a particular class of persistent identifiers and is supported by the DOI Foundation.</p>"},{"location":"gen3-resources/glossary/#edge-node","title":"Edge Node","text":"<p>Edge nodes may be created at clinics, labs, hospitals, or academic institutions to share data with data platforms.  The data platform nodes connect with a data hub (forming a data fabric/mesh) to provide a federated platform for data discovery.  Edge nodes share only a subset of the data they store with data platforms</p>"},{"location":"gen3-resources/glossary/#etl","title":"ETL","text":"<p>Structured data submitted to commons are stored in PostgreSQL. Querying data from PostgreSQL with multiple join is painful and inefficient. So, we use ElasticSearch as a place to store materialized dataset. Extract-transform-load (ETL) is a process that creates the materialized data from PostgreSQL and store them in ElasticSearch.  This is accomplished via the Tube microservice. More details of running an ETL can be found here.</p>"},{"location":"gen3-resources/glossary/#fair-data","title":"FAIR Data","text":"<p>FAIR data are data which are findable, accessible, interoperable, and reusable[12]. There is now an extensive literature on FAIR data.</p>"},{"location":"gen3-resources/glossary/#framework-services","title":"Framework Services","text":"<p>Framework Services or Data Commons Framework (DCF) Services is the term used by Gen3 to refer to data mesh services in the narrow middle architecture, for data meshes, such as the NCI Cancer Research Data Commons. These are set of standards-based services with open APIs for authentication, authorization, creating and accessing FAIR data objects, and for working with bulk structured data in machine-readable, self-contained format.</p>"},{"location":"gen3-resources/glossary/#flattened-data","title":"Flattened Data","text":"<p>Structured data that has been processed via Tube and stored in elasticsearch to accelerate searchability.</p>"},{"location":"gen3-resources/glossary/#gen3-client","title":"Gen3 Client","text":"<p>The Gen3 Client is a command-line tool for downloading, uploading, and submitting data files to and from a Gen3 data commons.  Some of the same functionality can be found in the Gen3 SDK. You can find installation and use instructions here.</p>"},{"location":"gen3-resources/glossary/#gen3-sdk","title":"Gen3 SDK","text":"<p>The Gen3 Software Development Kit (SDK) for Python provides classes and functions for handling common tasks when interacting with a Gen3 commons. It also exposes a Command Line Interface (CLI). The API for a commons can be overwhelming, so this SDK/CLI aims to simplify communication with various microservices.  It can also download and upload files like the Gen3 Client.  You can find installation and use instructions here.</p>"},{"location":"gen3-resources/glossary/#gen3-microservices","title":"Gen3 Microservices","text":"<p>A simple list of most relevant microservices are included below.  For a description of each service and links to their respective repositories please visit the Developer's Guide.</p> <ul> <li>Data Portal</li> <li>Indexd</li> <li>Fence</li> <li>Guppy</li> <li>Hatchery</li> <li>Metadata Service (MDS)</li> <li>Pelican</li> <li>Peregrine</li> <li>Requestor</li> <li>Sheepdog</li> <li>Sower</li> <li>Tube</li> <li>Workspace Token Service</li> </ul>"},{"location":"gen3-resources/glossary/#globally-unique-identifier-guid","title":"Globally Unique Identifier (GUID)","text":"<p>A GUID is an essentially unique identifier that is generated by an algorithm so that no central authority is needed, but rather different programs running in different locations can generate GUID with a low probability that they will collide. A common format for a GUID is the hexadecimal representation of a 128-bit binary number. Some external systems may use the term Universallt Unique Identifier (UUID), which is essentially the same thing.</p>"},{"location":"gen3-resources/glossary/#graph-model","title":"Graph model","text":"<p>The graph model refers to the structured data within a Gen3 data commons.  The \"graph\" is defined by the relationship between nodes that is specified in the data dictionary.  This can be flattened via Tube and stored in elasticsearch to accelerate searchability.</p>"},{"location":"gen3-resources/glossary/#kubernetes","title":"Kubernetes","text":"<p>An open-source system for automating deployment, scaling, and management of containerized applications, which Gen3 is built from.</p>"},{"location":"gen3-resources/glossary/#manifest","title":"Manifest","text":"<p>Usually refers to a file manifest.  This is a json formatted file that includes GUIDs, file names, md5 checksums, and file sizes for files of interest.  It can be used by the gen3 client or SDK to download files provided a user has the appropriate credentials.</p>"},{"location":"gen3-resources/glossary/#microservice","title":"Microservice","text":"<p>Microservices are a software architecture that organizes software into small, independent services that communicate over well-defined APIs. These services can be developed, set up, and scaled independently. A more traditional architecture is to put all the APIs and other required functionality into a single application. This is sometimes called a monolithic architecture. Microservices provide important advantages for large-scale systems that require scalability and must continue to evolve even as their code base grows very large, but increases the complexity of operating small-scale systems.</p>"},{"location":"gen3-resources/glossary/#persistent-drive","title":"Persistent Drive","text":"<p>This is a directory in a Gen3 workspace that allows a user to store files that will remain available after termination of the workspace session.  It will be represented as <code>pd</code>.</p>"},{"location":"gen3-resources/glossary/#portable-format-for-biomedical-data-pfb","title":"Portable Format for Biomedical data (PFB)","text":"<p>PFB is a serialization file format designed to store bio-medical data and metadata. The format is built on top Avro to make it fast, extensible and interoperable between different systems. You can find the GitHub repo here and the publication here.</p>"},{"location":"gen3-resources/glossary/#workspace","title":"Workspace","text":"<p>Gen3 workspaces are secure data analysis environments in the cloud that can access data from one or more data resources, including Gen3 Data Commons. Gen3 Workspaces use the Gen3 Framework Services for user authentication and authorization, data object indexing, and metadata services. Gen3 Workspaces support Jupyter notebooks, RStudio notebooks, and other custom applications that can access data through Gen3 open APIs.  For instructions on the use of a workspace see here.</p>"},{"location":"gen3-resources/developer-guide/","title":"Gen3 Developer Guide - Extend Gen3","text":""},{"location":"gen3-resources/developer-guide/#gen3-developer-guide-extend-gen3","title":"Gen3 Developer Guide - Extend Gen3","text":"<p>Welcome to the Gen3 Developer Guide.  This guide is primarily for software engineers that are looking to attain a deeper understanding of Gen3 services in order to interact with or modify them.</p> <p>If you are a researcher or data scientist looking to access a Gen3 Data Commons or mesh to locate, access, or analyze data please read the Gen3 User Guide.  If you are looking to deploy, maintain, configure, or submit data please take a look at the Gen3 Operator Guide.</p>"},{"location":"gen3-resources/developer-guide/#resources","title":"Resources","text":"<ul> <li>Gen3 Source Code</li> <li>The Gen3 Data Hub is an open-access data commons that can be used as an example system.  It is highlighted throughout the documentation.</li> <li>The Biomedical Research Hub is an example data mesh that is also used to highlight mesh-specific features.</li> </ul>"},{"location":"gen3-resources/developer-guide/architecture/","title":"Gen3 Architecture","text":"<p>This documentation is intended for developers who want to understand the design and architecture of Gen3.</p>"},{"location":"gen3-resources/developer-guide/architecture/#overview","title":"Overview","text":"<p>Gen3 is a modular, open-source software platform that exposes a standard set of application programming interfaces (APIs) and user interfaces (UIs). It is capable of managing various types of cloud-based data and enabling cloud-based compute over those data.</p> <p>The underlying software and APIs are designed from the ground up to be interoperable, standards-based, and configurable.  The Gen3 software is intended to be cloud-agnostic and its deployment is facilitated by containerization and orchestration frameworks such as Kubernetes.</p> <p>NOTE: While cloud-agnostic, we (the Center for Translational Data Science, maintainers of Gen3) use AWS internally for most instances of Gen3 we manage, and therefore some documentation may include AWS specific instructions.</p> <p>Gen3 can handle various categories of data based on their structure.</p> <p>Unstructured data represents files on a hard drive or cloud storage with no consistent schema. These data tend to represent either bulk clinical and phenotypic data in spreadsheet format or patient level data such as images or genomic sequencing files.</p> <p>Structured data are data that adhere to a specific and strict schema with a graphical representation of nodes with properties and relationships to other nodes. This schema can be referred to as a data model or data dictionary and tends to represent clinical and phenotypic data, linking subjects and their clinical and phenotypic data to studies and their samples. This is typically used to represent a harmonized version of the data.</p> <p>Semi-structured data is organized as unique identifiers with flexible key/value pairs (including nesting). The key/value pairs may be consistent between records, but are not required to be. This is typically used for storing publicly available metadata about available datasets or additional public metadata about samples.</p> <p>The overall architecture is designed to support FAIR data access[1], which includes features such as permanent digital IDs, open APIs, rich clinical and experimental data, and services modeled from external standards such as from GA4GH.  Gen3 data meshes go one step further and support Secure and Authorized FAIR Environment (SAFE) environments[2].</p>"},{"location":"gen3-resources/developer-guide/architecture/#products","title":"Products","text":"<p>The distinct Gen3 products include Gen3 Data Commons, Gen3 Data Meshes, Gen3 Framework Services, and Gen3 Analytic Workspaces. Each product represents common applications and use cases with a subset of Gen3 UIs and APIs made available.</p>"},{"location":"gen3-resources/developer-guide/architecture/#gen3-data-commons","title":"Gen3 Data Commons","text":"<p>Gen3 Data Commons co-locate exploration and visualization tools with data management services for import and export of structured information like clinical, phenotypic, or biospecimen data, along with data objects, like genomics data files or medical images. Gen3 Data Commons are capable of interoperation with other resources in a data mesh (AKA fabric or ecosystem) by utilizing the Gen3 Framework Services.</p>"},{"location":"gen3-resources/developer-guide/architecture/#gen3-data-meshes","title":"Gen3 Data Meshes","text":"<p>A Gen3 Data Mesh connects independent data resources into a single interoperable data ecosystem. The APIs provided as part of this product include support for indexing data objects, associating metadata with the data objects, associating metadata with each data resource, controlling user access to data via a flexible access control policy engine, a graphical user interface, and workspaces that run over the open APIs.</p> <p>The level of integration in a data mesh can vary greatly. In a very integrated scenario, all data has a single data model and governance structure and is in fact just a data commons.</p> <p>In a less integrated scenario, data may live in separate data commons with separate data models and access control policies, but where Gen3 indexes and associates metadata with files, provides GUIDs (Globally Unique Identifiers) for all files, provides access control, and exposes open APIs.</p> <p>On the least integrated side of the spectrum, Gen3 may provide only dataset-level metadata to make datasets discoverable and enable workspaces.</p>"},{"location":"gen3-resources/developer-guide/architecture/#gen3-framework-services","title":"Gen3 Framework Services","text":"<p>Gen3 Framework Services are a minimal set of software that provide open APIs and UIs that form the foundation, or \u201cframework\u201d, for building systems. Such foundational support includes indexing data objects, associating metadata with the data objects, controlling user access to data via a policy engine, and providing a data discovery UI.</p> <p>Gen3 Framework Services can be deployed alone and other desired functionality can be built off the extensive open APIs. It is more common that they are deployed as part of a broader product like Gen3 Data Commons or a Gen3 Data Mesh.</p> <p>Gen3 Framework Services aim to provide mechanisms to enable data to be fully Findable, Accessible, Interoperable, and Reusable (FAIR). One of the guiding principles of Gen3 as a whole, but most importantly the Gen3 Framework, is to use existing standards and solutions to common problems. This is why standards like OpenID Connect, OAuth 2.0, and GA4GH DRS have been adopted. We aim to adopt community standards and solutions when they provide an interoperable solution in an efficient way.</p>"},{"location":"gen3-resources/developer-guide/architecture/#gen3-analytic-workspaces","title":"Gen3 Analytic Workspaces","text":"<p>Gen3 Analytic Workspaces support secure data analysis environments in the cloud that can access data from one or more data resources, including Gen3 Data Commons.</p> <p>Gen3 workspaces are often fully integrated with a specific data commons, and coming soon are workspaces as stand-alone analysis environments with a user pay model. Workspaces use the Gen3 Framework Services for user authentication and authorization and for retrieving data objects and metadata from data resources, like Gen3 Data Commons.</p> <p>By default, Gen3 workspaces include Jupyter notebooks and RStudio but can be configured to host virtually any application, including analysis workflows such as Nextflow, data processing pipelines, or data visualization apps. In the future, the Gen3 Workflow Execution API will enable asynchronous and long-running workflows and pipelines to be executed.</p>"},{"location":"gen3-resources/developer-guide/architecture/#security","title":"Security","text":"<p>Gen3 is designed to enable organizations to support secure and compliant data sharing and analysis solutions, but security and compliance regulations and concerns may vary from project to project. To discuss Gen3 security and compliance it is useful to first identify the players or stakeholders involved in a Gen3 production system.</p> <ul> <li>Gen3 development team at the University of Chicago</li> <li>An infrastructure or platform provider like AWS, Azure, or a private cloud</li> <li>The Gen3 operator or technical team maintaining the Gen3 deployment</li> <li>The project sponsors who commission the Gen3 instance around a scientific research question</li> </ul>"},{"location":"gen3-resources/developer-guide/architecture/#gen3-development-team-at-the-university-of-chicago","title":"Gen3 Development Team at the University of Chicago","text":"<ul> <li>Provides stable, secure Gen3 releases and appropriate communications with the open-source community.</li> </ul> <p>Note: The Gen3 team is responsible for providing a solid foundation for the Gen3 platform, but external factors like infrastructure providers and operators will also impact the overall security and compliance posture.</p>"},{"location":"gen3-resources/developer-guide/architecture/#infrastructure-or-platform-provider-eg-aws-azure-private-cloud","title":"Infrastructure or Platform Provider (e.g. AWS, Azure, Private Cloud)","text":"<ul> <li>Responsible for physical security controls and measures.</li> <li>May be the same as the sponsor or technical operator if deployed on institutional on-premises infrastructure.</li> </ul> <p>Interaction Note: The Gen3 team relies on the infrastructure provider to ensure the underlying security measures are in place. However, the Gen3 operator is still responsible for understanding applicable laws and regulations.</p>"},{"location":"gen3-resources/developer-guide/architecture/#gen3-operator-or-technical-team","title":"Gen3 Operator or Technical Team","text":"<ul> <li>Responsible for:<ul> <li>Understanding all applicable laws and regulations for compliance</li> <li>Gen3 configurations</li> <li>CI/CD flow</li> <li>Platform operation and monitoring architecture</li> <li>Continuous Monitoring (ConMon)</li> <li>Policies and procedures related to the specific deployment</li> <li>Tools deployed outside of Gen3</li> </ul> </li> <li>Needs a good understanding of security best practices and laws governing their specific deployment, use case, and data.</li> </ul> <p>NOTE: It is highly recommended that Gen3 operators keep their deployment on the latest monthly release of Gen3</p> <p>Interaction Note: The Gen3 operator is responsible for ensuring compliance with applicable laws and regulations, but relies on the infrastructure provider to ensure the underlying security measures are in place. The project sponsor may also define specific requirements that the technical operator will configure in Gen3.</p>"},{"location":"gen3-resources/developer-guide/architecture/#project-sponsors","title":"Project Sponsors","text":"<ul> <li>May define specific use cases or requirements that the technical operator will configure in Gen3.</li> <li>Responsibilities include commissioning the Gen3 instance around a scientific research question.</li> </ul> <p>Interaction Note: The sponsor commissions the Gen3 instance, but does not directly manage its operation. The technical operator and Gen3 team must work together to ensure compliance with applicable laws and regulations while meeting the sponsor's requirements.</p> <p>Many of the CTDS-run commons operate at a FedRAMP (Federal Risk and Authorization Management Program) Moderate level as Gen3 Data Ecosystems Platform and, in collaboration with the Open Commons Consortium, LI-SaaS (Low Impact Software as a Service) as Gen3 Data Commons Service. However, other operators can choose to run a Gen3 system at a higher or lower security standard.</p>"},{"location":"gen3-resources/developer-guide/architecture/#key-features","title":"Key Features","text":""},{"location":"gen3-resources/developer-guide/architecture/#scalability","title":"Scalability","text":"<p>Gen3 can be used as a tool to manage small to very large projects.  This inherent flexibility is embedded in several Gen3 features:</p> <ul> <li>Gen3 is cloud-agnostic and can be run on any commercial cloud as well as on-prem infrastructure.  This will allow a range of groups to take advantage of the software given the requirements of different organizations.</li> <li>It takes advantage of kubernetes, which is an open-source system for automating deployment, scaling, and management of containerized applications.</li> <li>Our installation uses Karpenter, which is an open-source Kubernetes cluster autoscaler.  This allows a project to easily transition as it gains more or fewer users over a long or short period of time.</li> <li>Helm charts, which is a kubernetes package manager, allows operators to quickly deploy a new system.</li> <li>The use of terraform scripts allow operators to efficiently manage both Gen3 and infrastructure resources.</li> </ul>"},{"location":"gen3-resources/developer-guide/architecture/#data-model","title":"Data Model","text":"<p>The Gen3 data model is a graph-like relational model with nodes and edges; it specifies how different files, patients, experiments, and clinical visits are all related to one another.  It is central to a Gen3 Data Commons, which is autogenerated after the data dictionary and data model are specified.</p> <p>To be more specific, each node represents an entity, which consists of a related group of attributes or data elements. Clinical variables like a cancer diagnosis or a subject\u2019s gender might go into the diagnosis or demographic nodes, respectively. Variables related to how a biological sample was collected or processed may be found in a biospecimen node. Data files, such as medical images or genomic files, can also be nodes and have associated metadata variables like file size, format, and file name.</p> <p>Edges or links between nodes indicate relationships between them. For example, a Sample node may derive or connect to a specific patient or a genomic data file may also be associated with a specific sample.</p> <p>Properties are assigned different types including: string, boolean, floating point number, integer, or enumeration. Properties can also be defined as arrays of any of those types. The acceptable values for properties can be further restrained by defining regex patterns that strings must match or minimum or maximum values for numeric data. Nodes, properties, and permissible values are specified in a series of YAML files.</p>"},{"location":"gen3-resources/developer-guide/architecture/#data-portal","title":"Data Portal","text":"<p>Since the launch of Gen3, the user interface has been powered by the monolithic Gen3 Data Portal. Its design presents challenges in data movement between pages and has become increasingly difficult to extend.</p> <p>We are developing a new service, the Gen3 Frontend Framework (Gen3.2), to overcome these limitations. It offers enhanced custom content development, application performance, deployment, and maintenance. As we transition to the new framework, we will focus on the Gen3 Frontend Framework rather than the soon to be replaced Gen3 Data Portal.</p> <p>The Gen3 Frontend Framework consists of two primary modules\u2014core and frontend\u2014combined with a NextJS web application to create the Gen3 data commons UI. This architecture reduces code complexity, abstracts UI interactions with Gen3 services, supports customization, and simplifies deployment and cost.</p> <p>The core module interfaces with Gen3 services and analysis tools, managing user context. This includes the current cohort, selected studies, files, and active analysis tools. The module provides Gen3 services and API access via \u201chooks,\u201d allowing seamless data reading and updates. This design isolates pages and components from Gen3 API changes and standardizes service access. Additionally, the core module features the Analysis Tool Framework (ATF), which supports the development of applications using Gen3 services and third-party APIs. The ATF maintains a registry of tools, along with metadata describing their applicable contexts.</p> <p>The frontend module delivers standard Gen3 pages (Exploration, Discovery, Query, Data Dictionary, Workspace, and Profile), new Gen3 components, theming, and configuration support. Components leverage hooks from the core module to access Gen3 services. The module supports authenticated and access-controlled pages and applications through a Secured Content Provider wrapper. It also handles the registration of analysis tools with the ATF.</p> <p>Analysis tools are developed using React, Gen3 core/frontend packages, and custom code. These tools are published as NPM modules and registered as plugins with the ATF during the server build, making them accessible via the data commons\u2019 root URL.</p>"},{"location":"gen3-resources/developer-guide/architecture/#architectural-diagrams","title":"Architectural Diagrams","text":""},{"location":"gen3-resources/developer-guide/architecture/#architecture-overview","title":"Architecture Overview","text":""},{"location":"gen3-resources/developer-guide/architecture/#gen3-graph-data-flow","title":"Gen3 Graph Data Flow","text":""},{"location":"gen3-resources/developer-guide/architecture/#references","title":"References","text":"<ol> <li>Wilkinson, M., Dumontier, M., Aalbersberg, I. et al. The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018 (2016). https://doi.org/10.1038/sdata.2016.18</li> <li>Grossman, R.L., Boyles, R.R., Davis-Dusenbery, B.N. et al. A Framework for the Interoperability of Cloud Platforms: Towards FAIR Data in SAFE Environments. Sci Data 11, 241 (2024). https://doi.org/10.1038/s41597-024-03041-5</li> </ol>"},{"location":"gen3-resources/developer-guide/contribute/","title":"Contribute Code to Gen3","text":""},{"location":"gen3-resources/developer-guide/contribute/#contributing-to-gen3","title":"Contributing to Gen3","text":""},{"location":"gen3-resources/developer-guide/contribute/#how-to-contribute","title":"How to Contribute","text":""},{"location":"gen3-resources/developer-guide/contribute/#ask-questions","title":"Ask questions","text":"<p>If you have a question, check out #gen3_community on Slack.  Here you can ask both the maintainers at the Center for Translational Data Science, University of Chicago as well as the larger Gen3 community.  Request an invite here.</p> <p>You can also post to the Gen3 Forum here.</p> <p>Finally, you can send an email request to the maintainers here (support@gen3.org).</p> <p>You may also create an issue in GitHub.</p>"},{"location":"gen3-resources/developer-guide/contribute/#provide-updates-to-documentation","title":"Provide updates to documentation","text":""},{"location":"gen3-resources/developer-guide/contribute/#technical-documentation","title":"Technical documentation","text":"<p>Gen3 documentation is constantly evolving. All content is stored in our documentation repo.  There you can find instructions about how to report issues or gaps in the documentation.  You are also encouraged to modify the documentation files directly and submit a pull request for us to review.</p>"},{"location":"gen3-resources/developer-guide/contribute/#tutorials","title":"Tutorials","text":"<p>Beyond the basic documentation, having an end to end tutorial on a particular topic can extraordinarily helpful.  Please feel free to suggest a tutorial, which could be in the form a blog post.</p>"},{"location":"gen3-resources/developer-guide/contribute/#participate-in-community-events-or-on-slack","title":"Participate in community events or on Slack","text":"<p>Gen3 has a community forum every other month where CTDS or other Gen3 users/operators present on topics of mutual interest to the community.  We are always looking for contributions so please recommend topics or volunteer to present if a call is made to the community.  Your experiences are valuable for the rest of the community to learn from!</p> <p>Your participation in the community Slack channel is also very much appreciated.  If you know the answer to a question or have something to contribute please speak up!</p>"},{"location":"gen3-resources/developer-guide/contribute/#create-an-issue","title":"Create an Issue","text":"<p>If you have an idea for a new feature or a bugfix, it is best to communicate with the University of Chicago Center for Translational Data Science (CTDS) developers early. Slack is a great forum for getting early feedback on an idea. You may also create an issue in GitHub. Be sure to browse through existing GitHub issues and if one seems related, comment on it.</p> <p>Once you are ready, create a GitHub issue in whichever Gen3 repository is appropriate.  If you are not sure which repo is the correct location please utilize the Gen3 Community Repo.</p>"},{"location":"gen3-resources/developer-guide/contribute/#issue-lifecycle","title":"Issue Lifecycle","text":"<p>When an issue is first created, it is flagged <code>waiting-for-triage</code> waiting for a team member to triage it. Once the issue has been reviewed, the team may ask for further information if needed, and based on the findings, the issue is either assigned for further review internally or is closed with a specific status.  When a fix is ready, the issue is closed and may still be open until the fix is released.</p> <p>Internal to CTDS, JIRA issues are created to track issues.</p>"},{"location":"gen3-resources/developer-guide/contribute/#submit-a-pull-request","title":"Submit a Pull Request","text":"<p>You may submit updates to code or documentation by creating a pull request.</p> <ol> <li>Note that all code contributions are subject to our Apache 2.0 license attached to each of our repositories (https://www.apache.org/licenses/LICENSE-2.0).</li> <li> <p>Background information</p> <ul> <li>Should you create an issue first? No, just use the description of the pull request to provide context and motivation, as you would for an issue.</li> <li>If your code update is at all complicated you will likely be asked to fill out a Community Feature Document to help the  Gen3 team review your PR.  You can find the template here.  Please make a copy, and provide with your PR either as an attachment or provide a shareable link to a completed version.  Sharing as a Google doc may be helpful to allow for comments and discussion between you and the Gen3 team.</li> </ul> </li> <li> <p>Always check out the <code>main</code> or <code>master</code> branch and submit pull requests against it.</p> <ul> <li>Your pull request should include any new or modified tests, as needed. Consider both unit and integration tests. Please refer to the documentation for Gen3 integration tests.</li> <li>Follow naming conventions described below.</li> </ul> </li> <li> <p>Run all relevant tests against your modified code before you submit a pull request.</p> </li> <li>If there is a prior issue, reference the GitHub issue number in the description of the pull request.</li> <li> <p>Ensure your PR description is populated and following the required template</p> <ul> <li>Our Gen3 Release Notes are parsed from PR descriptions. Each bullet/line under the PR template headings get into our release notes as individual bullets, so the language should be succinct and high-level</li> <li>Context, motivation, overview, and all other info about the change should go above the required headings (which won\u2019t get parsed and pulled into the Gen3 Release Notes)</li> </ul> </li> <li> <p>Once your PR is made, a CTDS staff member may reach out with additional questions and/or comment directly on your PR in GitHub.</p> </li> </ol> <p>If your PR is not approved, we\u2019ll provide feedback about the rationale and possible steps to get it approved. Reasons for not approving PRs include: does not follow Gen3 coding conventions, inconsistent with Gen3 product vision, includes new features or packages that would be hard to maintain, or inconsistent with security requirements.</p> <p>If accepted, your contribution may be heavily modified as needed prior to merging. You may also be asked to rework the submission.</p> <p>If asked to make corrections, simply push the changes against the same branch, and your pull request will be updated. In other words, you should not create a new pull request when asked to make changes.</p>"},{"location":"gen3-resources/developer-guide/contribute/#naming-conventions","title":"Naming Conventions","text":"<p>Branches are named as <code>type/scope</code>, and commit messages are written as <code>type(scope): explanation</code>, where</p> <ul> <li><code>scope</code> identifies the thing that was added or modified,</li> <li><code>explanation</code> is a brief description of the changes in imperative present   tense (such as \"add function to _\", not \"added function\"),</li> <li>and <code>type</code> is defined as:</li> </ul> Text Only<pre><code>type = \"chore\" | \"docs\" | \"feat\" | \"fix\" | \"refactor\" | \"style\" | \"test\"\n</code></pre> <p>Some example branch names:</p> <ul> <li><code>refactor/db-calls</code></li> <li><code>test/user</code></li> <li><code>docs/deployment</code></li> </ul> <p>Some example commit messages:</p> <ul> <li><code>fix(scope): remove admin scope from client</code></li> <li><code>feat(project_members): list all members given project</code></li> <li><code>docs(generation): fix generation script and update docs</code></li> </ul>"},{"location":"gen3-resources/developer-guide/contribute/#run-tests-on-your-own-gen3-instance","title":"Run Tests on your own Gen3 Instance","text":"<p>In order to appropriately test your pull requests you must have a Gen3 deployment running.  You can find instructions for how to run Gen3 using our preferred method of helm charts here.</p>"},{"location":"gen3-resources/developer-guide/contribute/#reference-docs","title":"Reference Docs","text":""},{"location":"gen3-resources/developer-guide/contribute/#git-and-github-resources","title":"Git and GitHub resources","text":"<p>Before starting a new contribution, you need to be familiar with Git and GitHub concepts like: commit, branch, push, pull, remote, fork, repository, etc. There are plenty of resources online to learn Git and GitHub, for example:</p> <ul> <li>Git Guide</li> <li>GitHub Quick start</li> <li>GitHub on YouTube</li> <li>Git and GitHub learning resources</li> <li>Collaborating with Pull Requests</li> <li>GitHub Documentation, guides and help topics</li> <li>And many more...</li> </ul>"},{"location":"gen3-resources/developer-guide/key_repos/","title":"Key Repositories","text":""},{"location":"gen3-resources/developer-guide/key_repos/#key-repositories","title":"Key Repositories","text":"<p>This page contains a list of key GitHub repositories with which a Gen3 operator or developer may want to interact or contribute to.  </p>"},{"location":"gen3-resources/developer-guide/key_repos/#gen3-microservices","title":"Gen3 Microservices","text":"<p>Gen3 features and functionality are enabled by independent and modular microservices.  These services can be developed, set up, and scaled independently. Microservices provide important advantages for large-scale systems that require scalability and must continue to evolve even as their code base grows very large, but increases the complexity of operating small-scale systems.</p> <p>While the average user does not need to know the details and names of each microservice, if you are interested in adding new features or modifying your Gen3 system in some way it may be helpful to have a deeper understanding of a specific microservice.  We have included brief descriptions below along with a link to their documentation in GitHub.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#arborist","title":"Arborist","text":"<p>Arborist acts as the Gen3 Policy Engine. It is an attribute-based access control (ABAC) policy engine, designed for use with the Gen3 stack. Arborist tracks resources requiring access control, along with actions which users may perform to operate on these resources, and roles, which aggregate permissions to perform one or more actions.</p> <p>It is utilized by any service to make authorization decisions, whether that be API-level access, or the permission to read or delete a specific indexed record.</p> <p>Services should offload all authorization related logic to Arborist as much as possible.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#data-portal","title":"Data Portal","text":"<p>The data portal service is an interactive website that allows users to explore, submit, and download data. The Windmill service utilizes the APIs offered by the data commons just as any other externally built app could.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#fence","title":"Fence","text":"<p>Fence is an authentication (AuthN) and authorization (AuthZ) service which utilizes OpenID Connect flow (an extension of OAuth2) to generate tokens for clients. It can also provide tokens directly to a user. Clients and users may then use those tokens (JWT) with other Gen3 Data Commons services to access protected endpoints that require specific permissions. Fence can be configured to support different Identity Providers (IDPs) for AuthN. At the moment, supported IDPs include Google, and Shibboleth supporting providers such as NIH iTrust.</p> <p>Fence also handles the authorization syncing (though the results end up in Arborist), the management of necessary AWS/Google IAM credentials for administering signed URLs to end-users, and the exposure of the APIs for signed URL generation (both Gen3's and GA4GH DRS).</p> <p>NOTE: The Fence images serving signed URLs are typically deployed as a separate kubernetes service <code>presigned-url-fence</code> with much greater scaling capabilities (to separate traffic related to authN/Z from data access traffic). The Indexd service still handles the management of the indexed records themselves (with their GUIDs and DRS URIs).</p> <p>This separation of concerns (records vs data access) is intentional, but the fact that the Fence codebase holds both authN/Z and data access logic (requiring 2 kubernetes services of the same image) is known technical debt. It is likely that in the future a new service or mechanism for retrieving file-based data will replace <code>presigned-url-fence</code>.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#guppy","title":"Guppy","text":"<p>Server that supports GraphQL queries on data from elasticsearch. Guppy integrates with the Gen3 Policy Engine, Arborist, to filter out results that a user should not see. This is done by comparing a list of resources the user has access to (from Arborist), with the specified authorization resources of the records they are querying.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#hatchery","title":"Hatchery","text":"<p>Hatchery creates Kubernetes Pods for workspace services. Workspace services must expose HTTP servers. Ambassador is used to proxy user traffic through to their container workspace once it is launched by Hatchery.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#indexd","title":"Indexd","text":"<p>The Indexd service provides permanent, digital, globally unique IDs (GUIDs) for data objects. These IDs can be used to retrieve the data, or query the metadata associated with the object. The Indexd service tracks the locations and hash of every asset (file) in the data commons object store. It exports RESTful APIs for registering a new asset, and retrieving data for an existing asset.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#manifest-service","title":"Manifest Service","text":"<p>This service handles reading from and writing to a user's S3 folder containing their manifests. A manifest is a JSON file that lists records a researcher may be interested in analyzing. This service stores a manifest to a user folder in an s3 bucket and delivers it for later use, such as when the researcher wants to mount the manifest in their workspace. If the \"prefix\" config variable is set, user folders will be stored in a directory of that name within the s3 bucket.</p> <p>NOTE: We are developing a comprehensive replacement of this service, which will include real cohort management and better integration of selected data across UI pages.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#metadata-service-mds","title":"Metadata Service (MDS)","text":"<p>The Metadata Service provides an API for retrieving JSON metadata of GUIDs. It is a flexible option for \"semi-structured\" data (key:value mappings).  The content of the MDS powers the Data Portal Discovery Page for a Data Commons. The Gen3 SDK can be used to upload and edit the metadata. This service includes a feature known as the aggregated metadata service (AggMDS), which caches metadata from the metadata services of multiple data commons. The AggMDS holds the content viewable in a Data Portal Discovery page for a Data Mesh.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#peregrine","title":"Peregrine","text":"<p>Peregrine is the metadata seeking service which responds to GraphQL search queries and translates them to queries over the graph-like postgres database for structured data. The service translates the GraphQL search into the appropriate statements which are run against the PostgreSQL backend before being returned as friendly JSON.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#requestor","title":"Requestor","text":"<p>Requestor exposes an API to manage access requests.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#sheepdog","title":"Sheepdog","text":"<p>The Sheepdog service is responsible for herding user submissions of metadata into the graph database. The submissions are quality controlled against the data dictionary to ensure all required fields are present and have appropriate data values. The Sheepdog service is also responsible for supporting bulk export of the metadata into TSV or JSON formats.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#sower","title":"Sower","text":"<p>Sower dispatches Kubernetes jobs.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#tube","title":"Tube","text":"<p>Microservice that controls the ETL process of structured data.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#workspace-token-service","title":"Workspace Token Service","text":"<p>The Gen3 workspace token service acts as an OIDC client which acts on behalf of users to request refresh tokens from Fence. This happens when a user logs into a workspace from the browser. WTS then stores the refresh token for that user, and manages access tokens and refresh tokens for workers that belong to specific users in the workspace.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#deployment-and-support","title":"Deployment and Support","text":"<p>The repositories below include ones that are important for deployment or interacting with a Gen3 system.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#gen3-data-client","title":"Gen3 Data Client","text":"<p>The Gen3 Client (or Data Client) is a command-line tool for downloading, uploading, and submitting data files to and from a Gen3 data commons.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#helm","title":"Helm","text":"<p>Gen3 relies upon Helm to manage installation and management of Kubernetes applications. Helm is used to build \"charts\", which are packages of Kubernetes resources that are used to deploy apps to a cluster. Helm is the recommended way to deploy Gen3.</p>"},{"location":"gen3-resources/developer-guide/key_repos/#sdk","title":"SDK","text":"<p>The Gen3 Software Development Kit (SDK) for Python provides classes and functions for handling common tasks when interacting with a Gen3 commons. It also exposes a Command Line Interface (CLI).</p>"},{"location":"gen3-resources/developer-guide/key_repos/#microservice-nginx-route-table","title":"Microservice NGINX Route Table","text":"<p>This table is helpful for debugging errors in front-end applications like Windmill: data portal or other Gen3 clients. You can easily identify the running service that is returning an error, based on its absolute HTTP request path. Source.</p> <p>NOTE: We intend to eventually have a more centralized API definition and potentially an API Gateway, while shifting towards an API First development strategy.</p> Microservice URL Path Prefix (NGINX Location) GitHub Repository ambassador-service /lw-workspace/proxy/ https://github.com/uc-cdis/cloud-automation/tree/master/kube/services/ambassador arborist-service /gen3-authz https://github.com/uc-cdis/arborist arborist-service ~ /authz/? https://github.com/uc-cdis/arborist arborist-service /authz/resources https://github.com/uc-cdis/arborist arborist-service /gen3-authz-test https://github.com/uc-cdis/arborist fence-service /authn-proxy https://github.com/uc-cdis/fence fence-service /user/ https://github.com/uc-cdis/fence fenceshib-service / https://github.com/uc-cdis/cloud-automation/tree/master/kube/services/fenceshib google-sa-validation-service /google-sa-validation-status/ https://github.com/uc-cdis/cloud-automation/tree/master/kube/services/google-sa-validation grafana /grafana/ guppy-service /guppy/ https://github.com/uc-cdis/guppy hatchery-service /lw-workspace/ https://github.com/uc-cdis/hatchery indexd-service /ga4gh/ https://github.com/uc-cdis/indexd indexd-service /index/ https://github.com/uc-cdis/indexd jupyterhub-service /lw-workspace/ https://github.com/uc-cdis/cloud-automation/tree/master/kube/services/jupyterhub jupyterhub-service /lw-workspace/hub/logout https://github.com/uc-cdis/cloud-automation/tree/master/kube/services/jupyterhub manifestservice-service /manifests/ https://github.com/uc-cdis/manifestservice metadata-service /mds/ https://github.com/uc-cdis/metadata-service peregrine-service /peregrine/_status https://github.com/uc-cdis/peregrine peregrine-service /peregrine/_version https://github.com/uc-cdis/peregrine peregrine-service /api/search https://github.com/uc-cdis/peregrine peregrine-service /api/v0/submission/graphql https://github.com/uc-cdis/peregrine peregrine-service /api/v0/submission/getschema https://github.com/uc-cdis/peregrine pidgin-service /coremetadata/ https://github.com/uc-cdis/pidgin prometheus-server /prometheus/ https://github.com/uc-cdis/cloud-automation/tree/master/kube/services/monitoring sheepdog-service /api/ https://github.com/uc-cdis/sheepdog sower-service /job/ https://github.com/uc-cdis/sower workspace-token-service /wts/ https://github.com/uc-cdis/workspace-token-service"},{"location":"gen3-resources/operator-guide/","title":"Gen3 Operator Guide - Configure and Deploy Gen3","text":""},{"location":"gen3-resources/operator-guide/#gen3-operator-guide-configure-and-deploy-gen3","title":"Gen3 Operator Guide - Configure and Deploy Gen3","text":"<p>This is the guide for users who want to stand up a new Gen3 instance. It includes the following content:</p> <ul> <li>An overview of Gen3 deployment and considerations</li> <li>A list of prerequisite software and resources for production deployment of Gen3</li> <li>A description of predeployment tasks, including our approach to Infrastructure as Code, preparing an SSL certificate, and setting up a Secrets manager.</li> <li>A description of the different authentication methods that can be used in Gen3</li> <li>A guide to Helm deployment and configuration</li> <li>Postdeployment tasks, including creating a Data Dictionary and Data Model, submitting data, customizing search for Query, Exploration, and Discovery pages in Gen3, and customizing the front end</li> </ul>"},{"location":"gen3-resources/operator-guide/#gen3-deployment-options","title":"Gen3 Deployment Options","text":""},{"location":"gen3-resources/operator-guide/#helm-preferred","title":"Helm (preferred)","text":"<p>Helm is a Kubernetes package manager that allows you to easily define, install, and upgrade even complex applications. It is a tool that streamlines the installation and management of applications on Kubernetes platforms. We recommend that Gen3 users deploy with Helm.</p> <p>Jump to the Helm configuration and deployment guide</p> <p>In a Gen3 deployment, Helm serves as the primary tool for:</p> <ul> <li>Defining Deployments: Helm uses configuration files called charts to define how Gen3 components should be deployed. These charts encapsulate the necessary configuration, dependencies, and deployment logic.</li> <li>Installation: Helm streamlines the process of installing Gen3 components into your Kubernetes cluster. With Helm, you can easily deploy Gen3 services, databases, and other essential components.</li> <li>Configuration Management: Helm simplifies the management of configuration settings for Gen3 services. You can customize settings, such as database connection details, service replicas, and more, through Helm values.</li> <li>Upgrades and Rollbacks: As Gen3 evolves, Helm enables you to effortlessly upgrade your deployment to the latest versions. In case of issues, it also provides the ability to roll back to previous configurations.</li> </ul>"},{"location":"gen3-resources/operator-guide/#other-deployment-options","title":"Other Deployment Options","text":"<p>Although we strongly recommend deploying with Helm, there may be circumstances where Helm deployment is not possible. For these cases, there are older approaches that can be used to deploy Gen3:</p> <p>Cloud-automation (legacy)</p> <p>Cloud-automation is preserved for legacy reasons. In the future, it may no longer be supported, so we encourage all Gen3 operators to update their Gen3 installation to use Gen3 Helm.</p> <p>Gen3 cloud-automation  was used in the past to deploy Gen3 data commons in production environments on Amazon Web Services, Google Cloud Platform, Microsoft Azure, and OpenStack environments. Cloud-automation is fully-featured, supporting integrated logging, security, and compliance steps. With cloud-automation, we utilize Kubernetes to orchestrate our services into a scalable environment that can be run in a cost-efficient manner for many tens to thousands of users.</p> <p>Even so, cloud-automation is a \"bespoke solution\" for CTDS, biased to how CTDS deploys Gen3 with limited configuration overrides. It also has a steep learning curve. Because of these disadvantages, we have developed the Helm approach to configure and deploy Gen3; cloud-automation is no longer recommended for deployment.</p> <p>Although we do not recommend using this, you can explore Gen3 cloud-automation here</p> <p>Compose-services (deprecated)</p> <p>Compose-services is deprecated, and is no longer maintained. We are linking to this for legacy purposes.</p> <p>Compose-services was used to deploy Gen3 at a small scale, for experimental commons, small commons, or local development of the Gen3 stack. Deployment used Docker containers for the Gen3 microservices and nginx. The microservices and nginx images were pulled from quay.io (master), while Postgres (9.5) images were pulled from Docker Hub. Nginx was used as a reverse proxy to each of the services.</p> <p>Although we do not recommend using this, you can explore Gen3 compose-services here</p>"},{"location":"gen3-resources/operator-guide/authorization/","title":"Controlling data authorization","text":""},{"location":"gen3-resources/operator-guide/authorization/#controlling-authorization-of-data-access","title":"Controlling authorization of data access","text":""},{"location":"gen3-resources/operator-guide/authorization/#unstructured-data","title":"Unstructured data","text":"<p>Files can be either open access or controlled access within a Gen3 Data Commons.  Access to controlled files is managed either through dbGaP or via an allow list.</p>"},{"location":"gen3-resources/operator-guide/authorization/#authentication","title":"Authentication","text":"<p>Authentication is a way of telling a Gen3 system who you are.  This requires you configure an Identity Provider (IdP), which is configured through the Fence service. At the moment, the supported IDPs include:</p> <ul> <li>Google</li> <li>Shibboleth</li> <li>NIH iTrust</li> <li>InCommon</li> <li>eduGAIN</li> <li>CILogon</li> <li>Cognito</li> <li>Synapse</li> <li>Microsoft</li> <li>ORCID</li> <li>RAS</li> </ul>"},{"location":"gen3-resources/operator-guide/authorization/#authorization","title":"Authorization","text":"<p>Authorization establishes to which files a particular user or user group has access.</p> <p>The <code>user.yaml</code> file is one way to get authorization information into Gen3. It is ingested via Fence's usersync script. The format of this file is tightly coupled with the notions of resource, role and policy as defined by Gen3's policy engine, Arborist.</p> <p>Detailed instructions on the format of the user.yaml can be found here.</p> <p>Authorization can also be managed via dbGaP, which is documented here.</p>"},{"location":"gen3-resources/operator-guide/authorization/#authorization-more-granular-than-the-project-level","title":"Authorization more granular than the project level","text":"<p>By default, authorization can be made at the level of the project, but indexd can support more granular access.  To modify authorization within indexd, you can do the following:</p> <ol> <li>Obtain GUIDs for files for which you wish to update the authZ field.</li> <li>Download the Gen3 Python SDK here, as this will allow you to make changes to the indexd records. Run <code>pip install gen3</code>.</li> <li> <p>Programatically change the authz of the indexd record:</p> <p>With the list of GUIDs for a specific institution and your credentials that you downloaded from the profile page on the commons, you will run the following Python script that will make edits to the indexd database. In this example Python script, the changes to the authz field are being made to the program and project my_program-TEST1. In this instance, the new authz field is going to have a sources resource called DEMO. The endpoint is the common\u2019s url and the auth function will call your credentials files.</p> Text Only<pre><code>import gen3\nfrom gen3.auth import Gen3Auth\nfrom gen3.index import Gen3Index\n\nguids=[\"guid1\",\u201dguid2\u201d,\u201dguid3\u201d...\u201dguidN\u201d]\n\nnew_authz=\"/programs/my_program/projects/TEST1/sources/DEMO\"\n\n\nendpoint=\"https://url.commons.org\" #commons URL\nauth=Gen3Auth(endpoint, refresh_file=\"credentials.json\") #your creds\n\nindex = Gen3Index(endpoint,auth)\n\nfor guid in guids:\n    index.update_record(guid=guid, authz=[new_authz])\n    print(guid + \" has been updated to the following authz: \" + new_authz)\n</code></pre> </li> <li> <p>Edit the user.yaml</p> </li> </ol> <p>The user.yaml will require changes to three sections to make these files with new authz fields accessible:</p> <p>Policies. This notes the resource path and the permissions (role_id) you will give to the id.</p> Text Only<pre><code>- id: 'my_program-TEST_DEMO_downloader'\nrole_ids:\n- 'reader'\n- 'storage_reader'\nresource_paths:\n- '/programs/my_program/projects/TEST1/sources/DEMO\n</code></pre> <p>Resources. The list structure of the resources as seen in your resource path, for example <code>/programs/my_program/projects/TEST1/sources/DEMO:</code></p> Text Only<pre><code>resources:\n- name: my_program\n  subresources:\n     - name: projects\n        subresources:\n        - name: TEST1\n          subresources:\n          - name: sources\n           subresources:\n           - name: name1\n           - name: DEMO\n           - name: name2\n           - name: name3\n           - name: name4\n</code></pre> <p>Users. The user profile and the id that is assigned to them, which allows for the permissions set in the policies:</p> Text Only<pre><code>  user@gmail.edu:\n    policies:\n    - my_program-TEST1_reader\n    - my_program-TEST1_name1_downloader\n    - my_program-TEST1_DEMO_downloader\n    - my_program-TEST1_name2_downloader\n    - my_program-TEST1_name3_downloader\n    - my_program-TEST1_name4_downloader\n</code></pre>"},{"location":"gen3-resources/operator-guide/authorization/#structured-data","title":"Structured data","text":"<p>Structured data can be masked or hidden from unauthorized users within the Guppy Service.  The tiered-access setting is configured through either the <code>TIER_ACCESS_LEVEL</code> environment variable or the <code>tier_access_level</code> properties on individual indices in the esConfig. Guppy supports three different levels of tiered access:</p> <ol> <li>Private by default: only allows access to authorized resources</li> <li>Regular: allows all kind of aggregation (with limitation for unauthorized resources), but forbid access to raw data without authorization</li> <li>Libre: access to all data</li> </ol> <p>Read more details about configuration within the Guppy documentation.</p>"},{"location":"gen3-resources/operator-guide/authorization/#semi-structured-data","title":"Semi-structured data","text":"<p>Access to semi-structured data (i.e. MDS or AggMDS) is completely open-access and cannot be made to be controlled access at this time.</p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/","title":"Create Data Dictionary","text":""},{"location":"gen3-resources/operator-guide/create-data-dictionary/#creating-a-new-data-dictionary","title":"Creating a New Data Dictionary","text":"<p>Every Gen3 data commons employs a data model, which serves to describe, organize, and harmonize structured data. Data harmonization facilitates cross-project analyses and is thus one of the pillars of the data commons paradigm. The data model organizes experimental or clinical variables, \u201cproperties\u201d, into linked categories, \u201cnodes\u201d, through the use of a data dictionary. The data dictionary lists and describes all nodes in the data model, as well as defines and describes the properties in each node. A Gen3 Data Dictionary is specified by a YAML file per node.  A dictionary needs to be defined to allow submission of structured metadata and use of the Exploration Page in the Data Portal.</p> <p>A few example nodes are listed below:</p> <ul> <li>Clinical variables like a \"primary cancer diagnosis\" or a subject's \"gender\" might go into the \"diagnosis\" or \"demographic\" nodes, respectively.</li> <li>Sample-related variables like \"how a tumor sample was collected\" and \"what analyte was extracted from it\" might go into the \"biospecimen\" or \"analyte\" nodes, respectively.</li> <li>Data files also have associated metadata variables like file size, format, and filename. These properties are grouped into nodes that describe various types of data files, like \"mri_image\", for an MRI image data file.</li> </ul> <p>Each node in the data dictionary is linked in a logical manner to other nodes, which facilitates generating a visual overview, or graphical model, of a project.</p> <p>The following image displays the data dictionary in Table View, the 'Medical History' node entry in the dictionary with the list of properties, and an example graphical model of a project:</p> <p></p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#core-dictionary","title":"Core Dictionary","text":"<p>If you have followed our helm deployment instructions we have created a basic data dictionary to get you started.  You can use the default Data Dictionary as a starting point for creating your own data dictionary in your own commons. It is a consensus of previously used data dictionaries and will make creating your own data dictionary more efficient. It is easy to replace the default dictionary at deployment as long as you have 1) not submitted any data to the default dictionary and 2) ETL mapping has not occurred with the default dictionary.</p> <p>A list of some example data dictionaries are included below, which you can review for potential ideas or examples for how to capture specific data types:</p> Data Commons Dictionary Viewer Dictionary Repo Data Commons Framework Services DCFS Dictionary GitHub Medical Imaging and Data Resource Center (MIDRC) MIDRC Dictionary GitHub Justice Community Opiod Innovation Network (JCOIN) JCOIN Dictionary GitHub <p>The basic structure of this and all dictionaries includes the following node categories:</p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#node-categories","title":"Node Categories","text":""},{"location":"gen3-resources/operator-guide/create-data-dictionary/#administrative","title":"Administrative","text":"<p>The Project, Study, and Subject nodes are administrative nodes that are required for any Gen3 data commons. Administrative nodes store basic project and study information for their associated cases or subjects.  Also, the Subject node level is where the nodes start differentiating between commons.  It can also be represented as Case depending on the use case.</p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#clinical","title":"Clinical","text":"<p>The Subject node links to the Demographic and Diagnosis clinical nodes. These nodes are Clinical nodes that are used to store clinical and medical history related data. The Demographic node stores properties that represent the statistical characterization of human populations or segments of human populations (for example, characterization by year of birth, sex, and race). This node is typically used to store properties that do not change over time. The Diagnosis clinical node represents the investigation, analysis, and recognition of the presence and nature of disease, condition, or injury from expressed signs and symptoms. It also pertains to the scientific determination of any kind and the concise results of such an investigation.</p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#biospecimen","title":"Biospecimen","text":"<p>The Biospecimen node category is associated with data related to biological specimens as it relates to testing, diagnostic, propagation, treatment or research purposes.  This node can contain one or more components including but not limited to cellular molecules, cells, tissues, organs, body fluids, embryos, and body excretory products.  It is composed of nodes such as Sample, Aliquot, and Read Group.  For example, in some commons Sample can represent an actual piece of tissue removed from a biological entity for testing, diagnostic, propagation, treatment or research purposes and an aliquot represents a nucleic acid extraction from this tissue.  The number of biospecimen nodes can be tuned for a particular commons depending on the desired level of granularity required.  Read group refers to sequencing read group and can be important for commons that require bioinformatics processing of sequencing data.</p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#analysis","title":"Analysis","text":"<p>The Analysis node category stores data that is associated with genomic pipeline analysis that is typical of next generation sequencing (NGS).  Example Analysis nodes include Genotyping Array Workflow which stores metadata for genotyping array workflow, the Proteomic Workflow node  which stores metadata for the protein mass spectrometry workflow, and the Aligned Workflow node which stores metadata for alignment pipeline used to align reads.</p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#data-file","title":"Data File","text":"<p>The Data File node category is used to store metadata related to data files that are stored in the cloud. It contains nodes such as Submitted Aligned Reads, Submitted Unaligned Reads, and Aligned Reads.  These nodes contains properties such as file_name, file_size, and data_format which describe information stored in the file.</p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#index-file","title":"Index File","text":"<p>The Index File node category stores the metadata that is associated with different file formats (for example, BAI and BAM).  For example, the Aligned Reads Index node contains the index for a set of aligned reads.</p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#notation","title":"Notation","text":"<p>The Notation node category is used to store data that does not fit into other categories (for example, it doesn't store index files, data files, or analysis data).  The ability to update/modify a dictionary is an important functionality that may arise based on project and clinical data needs.</p> <p>The following image depicts the graph view of the core data dictionary (The key in the top right corner of the image indicates the node categories):</p> <p></p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#modifying-a-data-dictionary","title":"Modifying a Data Dictionary","text":"<p>Once you have obtained the baseline dictionary, you can make updates to it in order to tailor it to your particular project. Some groups have created open-source tools for editing data dictionaries using other methods, such as gen3schemadev, which interconverts YAML schemas with TSV format spreadsheets. This tool and others like it found on the Gen3 Tools Page may be useful.</p> <p>Note: The Gen3 dictionary is stored in JSON format following the jsonschema. The backend (Sheepdog) stores properties in the database as jsonb.</p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#referencing-external-data-standards","title":"Referencing external data standards","text":"<p>It is possible to include references to controlled vocabularies such as the National Cancer Institute Thesaurus (NCIt).  This will help with the comparison of studies and projects across data commons and provide researchers with proper references.  The NCIt is being used for many of the schemas as it's inclusive of several different domains (for example, clinical, drug, etc.).  It also has an abundance of non-domain related terms such as nominal (for example, gender and race) and ordinal (for example, left, right, first, and last) along with other useful categories of terms.  The benefit of this effort is that it will facilitate cross data commons comparison.  For instance, if tuberculosis is a term associated with multiple studies, a search of that term will provide insight into each of the studies.  It will also help with the prevention of adding multiple terms for properties that mean the same thing.  The example below demonstrates a cross-study comparison using YAML files (Gen3 uses YAML files to help organize data dictionaries).  The two files both relate to blood pressure finding, but each has a different term name.  The external reference helps with harmonization efforts by helping identify terms that have the same meaning.</p> Text Only<pre><code>Dictionary 1:\nBlood Pressure Measurement:\n    description: Measurement of blood pressure\n    enum:\n      - 90 over 60 (90/60) or less\n      - More than 90 over 60 (90/60) and less than 120 over 80 (120/80)\n      - More than 120 over 80 and less than 140 over 90 (120/80-140/90)\n      - 140 over 90 (140/90) or higher (over a number of weeks\n    termDef:\n       - term: Blood Pressure Finding\n         source: NCI Thesaurus\n         term_id: C54707\n         term_version: 18.10e (Release date:2018-10-29)\n         term_url: \"https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&amp;ns=ncit&amp;code= C54707\"\nDictionary 2:\nBlood Pressure Reading:\n    description: An indication of blood pressure level\n    enum:\n      - low blood pressure\n      - normal\n      - pre hypertension\n      - hypertension\n    termDef:\n       - term: Blood Pressure Finding\n         source: NCI Thesaurus\n         term_id: C54707\n         term_version: 18.10e (Release date:2018-10-29)\n         term_url: \"https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&amp;ns=ncit&amp;code= C54707\"\n</code></pre>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#best-practices","title":"Best Practices","text":""},{"location":"gen3-resources/operator-guide/create-data-dictionary/#data-harmonization","title":"Data Harmonization","text":"<p>When adding a new project or study into a new or an already existing data dictionary, it's important to follow the process of harmonization of data.  The harmonization process centers around updates or additions to the data dictionary, along with the possible need to harmonize the data. This process helps with the prevention of redundant properties, nodes, and allowable values that may already exist in a data dictionary.  It also involves the possibility of a data migration if the data dictionary changes affect the preexisting data (for example, the height property unit of measure change from meters to centimeters).  Before submitting new data to a commons or submitting updates to a data dictionary, check the current dictionary for properties, nodes, or enumerated values that already exist.  If there is a similar property, node, or enumerated value that exists, it's best practice to use the existing node, property, or enumerated value.  For example, if a candidate property named \u201cinfection agent\u201d and a property named \u201cinfectious agent\u201d already exist, then use \u201cinfectious agent.\u201d</p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#specificity-vs-generality","title":"Specificity vs. Generality","text":"<p>One of the goals when providing an external reference is to figure out the level of specificity when breaking down a property name that contains multiple concepts.  The question is whether the new references should be created with very specific designations. This is known as pre-coordination.   This option would likely create the need for the request of new terms in the external standard if the term is not in existence. The other question is, should the use of multiple terms that already exist in an external standard be used? This is known as post-coordination?  The best practice adopted by Gen3 is to use specificity whenever corresponding terms are available in the external standard.  However, If specific terms are not available, lean towards generality by creating multiple terms that already exist in an external standard.  For instance, if grapefruit juice is a property of interest and it's not found in the external reference, but grapefruit and juice are found individually, then using the individual properties is the preferred method.</p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#creating-valuable-data-descriptions","title":"Creating Valuable Data Descriptions","text":"<p>It's important to create clear and concise descriptions for each property in a dictionary. The descriptions should be understandable by someone who is not familiar with a particular domain. When available, including the unit of measure in parenthesis at the end of the description would be helpful in cases where the unit of measure is not included elsewhere in the description. When a clear description is not readily available, it's recommended that an external vocabulary such as NCIt be used as they offer definitions for terms from a plethora of domains.</p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#representing-longitudinal-data","title":"Representing Longitudinal Data","text":"<p>Gen3 does not recommend the reporting of actual dates as they are considered PHI. Instead, the concept of date intervals can be used in its place. Properties such as <code>days_to_birth</code>, <code>days_to_death</code>, <code>days_to_last_follow_up</code>, and <code>days_to_treatment</code> provide a means to keep track of timing between visits while protecting study participant identities. These properties begin with the same date, which is called the index date. The index date is day 0 and any event that occurs before that date is a negative number and any event that occurs after that date is a positive number. For example, if <code>days_to_birth</code> is -12784 and the index date is diagnosis, this means that the participant was born 12,784 days prior to the diagnosis given the negative number. It also infers that the patient was 35 at diagnosis.</p> <p>Gen3 provides the ability to store longitudinal data. A clinical node that is not included in the core dictionary is the Visit or Follow-Up node. The Visit node is a clinical node that is used to store longitudinal data that is collected over time and usually has a many to one relationship with its parent node, meaning that an observation/response was observed for a subject/unit repeatedly over time. Clinical properties that are common for this node include height, weight, and BMI (body mass index). If the need arises, the node can be added to a data dictionary.</p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#selecting-the-property-type","title":"Selecting the Property Type","text":"<p>The Gen3 model allows for a selection of property types that can be used to specify or limit values submitted for each property. The available types are jsonschema types such as:</p> <ul> <li>string example</li> <li>boolean example</li> <li>number example; can be float, long, or double as shown here.</li> <li>integer example</li> <li>enum example</li> <li>array example</li> <li>regex patterns (including strings, integers, and numbers to modify the format) example</li> <li>null example</li> <li>object example</li> </ul>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#avoiding-data-loops","title":"Avoiding Data Loops","text":"<p>When creating a data model it's important to avoid loops or cycles between nodes.  A cycle is created when a relationship between nodes is created on top of an already existing chain of relationships.  In other words, loops occur when one relationship is completely derivable from combined relationships that already exist.  Loops have a negative effects on down stream functions and tools such as ETL mappings and elastic search.</p> <p>The following diagram is an example of a loop as R3 is redundant as it brings no new associations that are not already presented by the combination of R1 and R2:</p> <p></p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#dictionary-update-documentation","title":"Dictionary Update Documentation","text":"<p>When making updates to data dictionaries, it's important to document these changes for good record keeping purposes.  The documentation should be implemented in the release notes of the respective GitHub site.  All changes should be denoted from minor to major changes.  Common updates include enumerated value modifications, adding or removing properties or nodes, and updates to links that describe relationships and dependencies between nodes.  Every release is associated with a unique tag which differentiates dictionary versions.  The tags are incrementally changed based on how extensive an update (for example, for a minor update such as changing a property name version 3.3.2 would become 3.3.3).</p>"},{"location":"gen3-resources/operator-guide/create-data-dictionary/#example-release-notes","title":"Example Release Notes","text":"Text Only<pre><code>  Gen3 Product: Sample Data Hub\n  Release Date: May 30, 2019\n  New Features and Changes\n\n  Create the following node:\n  sample\n\n  Add the following properties to the sample node:\n  sample_name\n  sample_time\n\n  Remove the following properties from the demographic node:\n  sex\n  height\n\n  Add new link:\n  sample to visit node\n</code></pre> <p>Proper documentation of dictionary updates fosters accountability and creates a historical representation of all dictionary changes that will allow future operators of the dictionary to understand how the dictionary has evolved over time.</p> <p>When generating the release notes there are conventions that have been established that help with transparency and readability of release notes.  The conventions include:</p> <ol> <li>Start the subject line with a verb (for example, Update to enumerated value)</li> <li>Use the imperative mood in the subject line (for example, Add, not Added or Adds header styles)</li> <li>Limit the subject line to about 50 characters</li> <li>Do not end the subject line with a period</li> <li>Separate subject from body with a blank line</li> <li>Wrap the body at 72 characters</li> <li>Use the body to explain what and why, not how</li> </ol>"},{"location":"gen3-resources/operator-guide/customize-frontend/","title":"Customize the Front End","text":""},{"location":"gen3-resources/operator-guide/customize-frontend/#customize-appearance-of-the-front-end","title":"Customize appearance of the front end","text":"<p>Below we show a few examples of how to customize the Gen3 Data Portal.</p> <p>For a more technical and complete background, see portal configurations on GitHub.</p>"},{"location":"gen3-resources/operator-guide/customize-frontend/#login-page","title":"Login Page","text":""},{"location":"gen3-resources/operator-guide/customize-frontend/#login-image","title":"Login Image","text":"<p>Customize the image that appears on the Login Page with a vector graphic (eg. *.svg) of your choice.</p> <p></p> <ul> <li>Review code to include the path-to-image in gitops.json.</li> </ul>"},{"location":"gen3-resources/operator-guide/customize-frontend/#information-on-login-and-commons","title":"Information on Login and Commons","text":"<p>Customize the text that appears on the Login Page by specifying title, description, subtitle, contact, or email.</p> <ul> <li>Review the code to edit title, subtitle, text, contact, and email.</li> </ul>"},{"location":"gen3-resources/operator-guide/customize-frontend/#landing-page","title":"Landing Page","text":""},{"location":"gen3-resources/operator-guide/customize-frontend/#information-on-commons","title":"Information on Commons","text":"<p>Customize the name of the Data Commons, the info text, and the button below that  appear on the top left side of the Landing Page after logging in.</p> <p></p> <ul> <li>Review the code to edit heading, text, and link.</li> </ul>"},{"location":"gen3-resources/operator-guide/customize-frontend/#summary-statistics","title":"Summary Statistics","text":"<p>Customize the summary statistics that appear on the top right side of the Landing Page after logging in. The attributes are graphQL fields, which must be in the dictionary, configured in the etlMapping.yaml, and populated with data on the backend.</p> <p></p> <ul> <li>Review the code to edit graphQL queries.</li> <li>Review the code to edit the graphQl queries after being logged in.</li> </ul>"},{"location":"gen3-resources/operator-guide/customize-frontend/#cards","title":"Cards","text":"<p>Customize the cards that appear on the bottom of the Landing Page after logging in.</p> <p></p> <ul> <li>Review the code to edit name, icons, body, link, and label of the cards.</li> <li>Adding a new icon requires saving the icon in this repository and in this file.</li> </ul>"},{"location":"gen3-resources/operator-guide/customize-frontend/#navigation-items","title":"Navigation Items","text":"<p>Customize the icons, link, and names that appear on the Data Commons navigation bar. The \"tooltip\" shows text upon hovering over the icon.</p> <p></p> <ul> <li>[Review the code to edit icon, link, color, tooltip, and name of the navigation items][gitops.json navbar].</li> <li>Adding a new icon requires saving the icon in this repository and [in this file][icons inex].</li> </ul>"},{"location":"gen3-resources/operator-guide/customize-frontend/#data-commons-or-mesh-title","title":"Data Commons or Mesh Title","text":"<p>Customize the title that appears in the top left corner such as for the Biomedical Research Hub.</p> <p></p> <ul> <li>Review the code to edit the title of the Data Commons.</li> </ul>"},{"location":"gen3-resources/operator-guide/customize-frontend/#top-bar","title":"Top Bar","text":"<p>Customize the top bar that appears in the top right corner.</p> <p></p> <ul> <li>Review the code to edit the top bar (link, name, icon, dropdown) of the Data Commons.</li> </ul>"},{"location":"gen3-resources/operator-guide/customize-frontend/#color-theme","title":"Color Theme","text":"<p>Customize the color theme for buttons, top navigation bar, and any types of charts on the Exploration and Landing Page.</p> <ul> <li>Review the code to edit the 9 colors of a Data Commons.</li> </ul>"},{"location":"gen3-resources/operator-guide/customize-frontend/#footer-logo","title":"Footer Logo","text":"<p>Customize the logos in the Footer.</p> <p></p> <ul> <li>Review the code to edit the source, link, and name of logos in the footer of a Data Commons.</li> </ul>"},{"location":"gen3-resources/operator-guide/customize-frontend/#notebook-browser","title":"Notebook Browser","text":"<p>Customize the Notebook Browser page to preview Jupyter Notebooks by adding images, titles, descriptions, and links to the Jupyter Notebook.</p> <p></p> <ul> <li>Review the code to edit the title, description, link, and imageURL.</li> </ul>"},{"location":"gen3-resources/operator-guide/customize-search/","title":"Customize Gen3 Search","text":""},{"location":"gen3-resources/operator-guide/customize-search/#customize-gen3-search","title":"Customize Gen3 Search","text":"<p>The ability to search and parse through data (structured, unstructured, and semi-structured) in a data commons or mesh is a core feature of Gen3.  In the sections below we describe what services must be running and actions taken to enable search via the different front end pages.</p>"},{"location":"gen3-resources/operator-guide/customize-search/#exploration-page","title":"Exploration Page","text":"<p>The primary tool for exploring data within a Gen3 data commons is the Exploration Page, which offers faceted search of data across projects, for example, https://gen3.datacommons.io/explorer. This page can be accessed from the /explorer endpoint or the top navigation bar, by clicking on the \u201cExploration\u201d icon.  See the user guide Exploration Page section for details on how to interact with the Gen3 Exploration Page.</p> <p>In order to have a functioning exploration page you must must have have Tube and Guppy microservices running as well as an ETL mapping.</p> <p></p>"},{"location":"gen3-resources/operator-guide/customize-search/#etl","title":"ETL","text":"<p>Three microservices Peregrine, Guppy, and Tube were developed to support both \u201cgraph\u201d and \u201cflat\u201d abstracted data models. Peregrine transforms GraphQL queries into Structured Query Language (SQL) statements into the data commons backend. Tube flattens the graph data into an ElasticSearch format, which is then able to be queried by Guppy. Execution of GraphQL queries are possible through both Gen3-developed APIs as well as browser-based GUIs.</p> <p>Construction of the \u201cflat\u201d model starts with the Tube microservice, which extracts, transforms, and loads (ETL) the underlying relational database into the flattened ElasticSearch format.  ElasticSearch is used due to its enhanced performance with the type of metadata queries run by users.  The ETL process is run when changes are made to the graph so subsequent queries match between \u201cgraph\u201d and \u201cflat\u201d abstractions. Once transformed, like Peregrine, the Guppy microservice transforms GraphQL statements into Elastic Search Query DSL.</p> <p>Newly ingested data to the Sheepdog Service can be queried immediately via Peregrine, but not on the Data Explorer, which is powered by Guppy on the backend.  During the ETL process, Tube will populate ElasticSearch indices and Guppy makes the ElasticSearch indices available for queries for the Data Explorer.</p> <p>In practice, Guppy/Tube need to be configured with the ElasticSearch indices in the manifest.json (versions in the versions block and indices in the guppy block) and the etlMapping.yaml file has to be configured to list those indices. Additionally, <code>aws-es-proxy</code> needs to be included in the versions block of the manifest.json, unless a customized endpoint to access ElasticSearch can be provided. Note that configuring the etlMapping.yaml is dependent on what users want to display on the Explorer page and needs to match to the Data Dictionary. The etlMapping.yaml can be validated against the Data Dictionary as described here. After configuring etlMapping.yaml, indices need to be created, cleaned, or/and re-populated using the <code>gen3 gitops configmaps</code> command to read the new etlMapping.yaml, and the <code>gen3 job run etl</code> command to run the ETL. Note, that new indices need to be added to both files etlMapping.yaml and manifest.json.</p>"},{"location":"gen3-resources/operator-guide/customize-search/#configure-exploration-page","title":"Configure Exploration Page","text":"<p>In the next step, the gitops.json needs to be configured to display and populate the indices of interest in the Data Explorer. The exploration page has one or several tabs at the top, which each represent a flattened ElasticSearch document of structured metadata records across all the projects in the data commons, which is displayed as a table towards the bottom center of the page.</p> <p>Remember that only the properties occurring in the etlMapping.yaml can be brought into the gitops.json. The gitops.json can be tested locally and validated against the Data Dictionary and etlMapping.yaml file. Finally, after new indices are introduced, Guppy needs to be rolled using the command <code>gen3 roll guppy</code>. A comprehensive list of commands in cloud automation is given here(https://github.com/uc-cdis/cloud-automation/blob/master/doc/README.md).</p>"},{"location":"gen3-resources/operator-guide/customize-search/#query-page","title":"Query Page","text":"<p>The query page in the data portal provides users with an interactive interface for querying the structured data in a Gen3 system via GraphQL API requests.  See the user guide Query Page section for details on how to interact with the Gen3 structured data API.</p> <p>In order for the page to be functional and the data ready for query you must have Sheepdog, Peregrine, and Guppy installed and configured.</p> <p>The graph model depends on Sheepdog and Peregrine. The Flat Model depends on ETL mapping, Tube, Guppy, and Elasticsearch.</p> <p>For details on the ETL mapping please see the ETL section.</p> <p></p>"},{"location":"gen3-resources/operator-guide/customize-search/#discovery-page","title":"Discovery Page","text":"<p>The Gen3 Discovery Page allows the visualization of metadata from within the metadata service (MDS) for data commons or the aggregated metadata services (AggMDS) for data meshes. This typically includes public metadata about a project to make it discoverable. Users should be able to search based on free text or filter based on tags.</p> <p>For more information on using the Discovery Page please see the User Guide Discovery Page section.</p> <p></p>"},{"location":"gen3-resources/operator-guide/customize-search/#metadata-service","title":"Metadata Service","text":"<p>To view data in the discovery page you must have a populated metadata service or alternatively an Aggregated metadata service (aggMDS), which caches the metadata from two or more metadata source to provide a unified view of the commons on the discovery page.</p> <p>Instructions for the creation and modification of an MDS record can be found here as part of the Gen3 SDK.  Every data commons is different as there is no standardization of MDS and therefore any example we provide may not apply to your particular system.</p> <p>To view the MDS for the Gen3 Data Hub you can go here. You can see in the snippet below some summary metadata for the 1000 Genomes project with is part of the Gen3 Data Hub:</p> Text Only<pre><code>{\n  \"1000_Genomes_Project\": {\n    \"_guid_type\": \"discovery_metadata\",\n    \"gen3_discovery\": {\n      \"link\": \"https://www.genome.gov/27528684/1000-genomes-project\",\n      \"tags\": [\n        {\n          \"name\": \"Aligned Reads\",\n          \"category\": \"Condition\"\n        }\n      ],\n      \"authz\": \"/programs/OpenAccess/projects/1000_Genomes_Project\",\n      \"source\": \"1000 Genomes Project\",\n      \"commons\": \"Open Access Data Commons\",\n      \"funding\": \"\",\n      \"summary\": \"The 1000 Genomes Project is a collaboration among research groups in the US, UK, and China and Germany to produce an extensive catalog of human genetic variation that will support future medical research studies. It will extend the data from the International HapMap Project, which created a resource that has been used to find more than 100 regions of the genome that are associated with common human diseases such as coronary artery disease and diabetes. The goal of the 1000 Genomes Project is to provide a resource of almost all variants, including SNPs and structural variants, and their haplotype contexts. This resource will allow genome-wide association studies to focus on almost all variants that exist in regions found to be associated with disease. The genomes of over 1000 unidentified individuals from around the world will be sequenced using next generation sequencing technologies. The results of the study will be publicly accessible to researchers worldwide.\",\n      \"study_id\": \"1000_Genomes_Project\",\n      \"full_name\": \"1000 Genomes Project\",\n      \"study_url\": \"https://www.genome.gov/27528684/1000-genomes-project\",\n      \"__manifest\": [\n        {\n          \"md5sum\": \"e1e56e29efad64c002e5e9749f85350f\",\n          \"file_name\": \"ALL.chrY.phase3_integrated_v2b.20130502.genotypes.vcf.gz\",\n          \"file_size\": 5656911,\n          \"object_id\": \"dg.OADC/60afa140-d2ab-4e32-bf73-40bf48787655\",\n          \"commons_url\": \"gen3.datacommons.io/\"\n        },\n</code></pre> <p>Instructions for working with the API are found here.</p>"},{"location":"gen3-resources/operator-guide/customize-search/#aggregated-metadata-service","title":"Aggregated Metadata Service","text":"<p>An Aggregated metadata service (aggMDS) caches the metadata from two or more metadata sources to provide a unified view of the commons on the discovery page.</p> <p>This is performed via AggMDS sync job, which copies metadata from multiple data commons into a single data store.</p> <p>The AggMDS sync job depends on JSON based configuration file, which defines information about:</p> <ul> <li>Data source and data adaptor information</li> <li>Normalizing data fields</li> <li>adding optional individual overrides</li> </ul> <p>An example json config file from the Biomedical Research Hub can be found here.  A snippet of the adapter section for the Gen3 Data Hub (formerly known as the Open Access Data Commons) is shown below.  For more information on constructing the json config file please review the instructions here.</p> Text Only<pre><code>\"Open Access Data Commons\": {\n            \"mds_url\": \"https://gen3.datacommons.io/\",\n            \"commons_url\" : \"gen3.datacommons.io\",\n            \"adapter\": \"gen3\",\n            \"config\" : {\n                \"guid_type\": \"discovery_metadata\",\n                \"study_field\": \"gen3_discovery\"\n            },\n            \"keep_original_fields\": false,\n            \"field_mappings\" : {\n                \"authz\": \"path:authz\",\n                \"tags\": \"path:tags\",\n                \"_unique_id\": \"path:study_id\",\n                \"study_id\": \"path:study_id\",\n                \"study_description\": \"path:study_description\",\n                \"full_name\": \"path:full_name\",\n                \"short_name\": \"path:short_name\",\n                \"commons\": \"Open Access Data Commons\",\n                \"study_url\": \"path:study_url\",\n                \"_subjects_count\" : {\"path\":\"_subjects_count\", \"default\" : 0 },\n                \"__manifest\": \"path:__manifest\",\n                \"commons_url\" : \"gen3.datacommons.io\"\n            }\n</code></pre> <p>The aggregated metadata service for the Biomedical Research Hub can be seen here.</p>"},{"location":"gen3-resources/operator-guide/customize-search/#configure-discover-page","title":"Configure Discover Page","text":"<p>Customize and enable the Discovery Page by editing the table items, advanced search fields, tags, and study page fields (i.e. page that opens up upon clicking on a study).</p> <p>To enable your Discovery Page you must modify your gitops.json file.</p> <p>Configure your Discovery page by further modifying your gitops.json file.</p>"},{"location":"gen3-resources/operator-guide/customize-search/#workspace-token-service","title":"Workspace token service","text":"<p>Setting up a functioning mesh where you can access files from individual commons also requires the workspace token service.  You can see a demo of what is needed to link a commons to a mesh in the Data Mesh Webinar.</p>"},{"location":"gen3-resources/operator-guide/gen3-authn-methods/","title":"Authentication Methods","text":""},{"location":"gen3-resources/operator-guide/gen3-authn-methods/#authentication-methods-available-in-gen3","title":"Authentication Methods Available in Gen3","text":"<p>Authentication is a way of telling a Gen3 system who you are. This requires you configure an Identity Provider (IdP), which is configured through the Fence service. Your choice of IdP will be determined by the governance and security requirements of the data commons or mesh you are operating.  At the moment, the supported IDPs include the following. Pros and Cons are included for a subset of popular options.  Please refer to the fence documentation for additional details.</p> <ul> <li>Google - Institutional Account<ul> <li>Pros<ul> <li>User\u2019s organization inherently takes responsibility for actions taken by user under the auspice of their organizational identity.</li> <li>User\u2019s organization takes responsibility for activating/deactivating and monitoring identity.</li> </ul> </li> <li>Cons<ul> <li>Can only be used if the user\u2019s organization utilizes Google for identity management.</li> </ul> </li> </ul> </li> <li>Shibboleth<ul> <li>NIH iTrust - NIH Login / iTrust / eRA Commons. eRA Commons is an authentication system developed by NIH for the management of research grants.<ul> <li>Pros<ul> <li>Intended for officials, principal investigators, trainees and post-docs at institutions/organizations to access and share information relating to research grants.</li> <li>Ties in tightly with dbGaP, which controls authorization for many NIH projects.</li> </ul> </li> <li>Cons<ul> <li>Can only be used if the commons is sponsored by an NIH institute.</li> </ul> </li> </ul> </li> <li>InCommon - InCommon, operated by Internet2, provides a secure and privacy-preserving trust fabric for research and higher education, and their partners, in the United States. Individual identity providers, such as NIH iTrust and most academic institutions, are federated by InCommon.<ul> <li>Pros<ul> <li>User\u2019s organization inherently takes responsibility for actions taken by user under the auspice of their organizational identity.</li> <li>User\u2019s organization takes responsibility for activating/deactivating and monitoring identity.</li> </ul> </li> <li>Cons<ul> <li>Can only be used if the user\u2019s organization is part of the InCommon federation.</li> <li>Reference this resource for organizations supported by InCommon.</li> </ul> </li> </ul> </li> <li>eduGAIN - eduGAIN is an international \u201cinterfederation\u201d of identity and service providers around the world. InCommon is a participant in eduGAIN.<ul> <li>Pros<ul> <li>User\u2019s organization inherently takes responsibility for actions taken by user under the auspice of their organizational identity.</li> <li>User\u2019s organization takes responsibility for activating/deactivating and monitoring identity.</li> <li>International presence, and InCommon is a participant</li> </ul> </li> <li>Cons<ul> <li>Can only be used if the user\u2019s organization is part of one of the 60+ federations participating in eduGAIN. Reference this resource for organizations supported by eduGAIN.</li> </ul> </li> </ul> </li> </ul> </li> <li>Microsoft - Microsoft Office 365 secure authentication system as provided by the user\u2019s organization.<ul> <li>Pros<ul> <li>User\u2019s organization inherently takes responsibility for actions taken by user under the auspice of their organizational identity.</li> <li>User\u2019s organization takes responsibility for activating/deactivating and monitoring identity.</li> </ul> </li> <li>Cons<ul> <li>Can only be used if the user\u2019s organization utilizes Microsoft Office 365 for identity management.</li> </ul> </li> </ul> </li> <li>ORCID - Provides an identifier for individuals to use with their name as they engage in research, scholarship, and innovation activities.<ul> <li>Pros<ul> <li>Most (all) researchers either have an ORCID or can easily create an ORCID.</li> </ul> </li> <li>Cons<ul> <li>User\u2019s organization does not inherently take responsibility for actions taken by user under auspice of ORCID identity.</li> <li>Neither user\u2019s organization nor ORCID take responsibility for activating/deactivating users based on affiliation.</li> </ul> </li> </ul> </li> <li>Google or Microsoft - Personal Account Google Sign-In / Microsoft Office 365 secure authentication systems as registered by an individual with no organizational affiliation or management.<ul> <li>Pros<ul> <li>Most users either have a Google/Microsoft identity or can easily create one.</li> </ul> </li> <li>Cons<ul> <li>User\u2019s organization does not inherently take responsibility for actions taken by user under auspice of personal Google/Microsoft identify.</li> <li>Neither user\u2019s organization does not take responsibility for activating/deactivating users based on affiliation.</li> </ul> </li> </ul> </li> <li>RAS</li> <li>CILogon</li> <li>Cognito</li> <li>Synapse</li> </ul>"},{"location":"gen3-resources/operator-guide/iac-atlantis/","title":"Atlantis","text":""},{"location":"gen3-resources/operator-guide/iac-atlantis/#atlantis","title":"Atlantis","text":"<p>Atlantis is a tool for running Terraform Workflows with ease.  It allows for collaborting on Terraform and enables developers and operators to run terraform plan and apply directly from Terraform pull requests. Atlantis then comments back on the pull request with the output.</p>"},{"location":"gen3-resources/operator-guide/iac-atlantis/#prerequisites","title":"Prerequisites","text":""},{"location":"gen3-resources/operator-guide/iac-atlantis/#downloads","title":"Downloads","text":"<p>You will need a working kubernetes cluster and to follow the steps outlined in the Atlantis documentation to cofigure it to work with a github repo and your cloud.</p> <p>After that is done, you may also need to install supplemental tools within the pod to have it work with our terraform modules. The best way to do this is to fork their image and add in a step to install any supplemental tools. The likely ones you will need to install are helm, kubectl, aws and postgres</p>"},{"location":"gen3-resources/operator-guide/iac-atlantis/#usage","title":"Usage","text":""},{"location":"gen3-resources/operator-guide/iac-atlantis/#how-to-use-atlantis","title":"How to use Atlantis","text":"<p>Atlantis is a little different than the other management processes, as you will need to use one of those, but Atlantis can add the support to run terraform in a CI/CD pipeline. This is useful if you want to automate the deployment and maintenance of your gen3 instances. This option is recommended if you want to collaborate with others on the deployment and maintenance of your gen3 instances, as it allows you to easily review and approve changes to your infrastructure through github pr's. There are a lot of possible ways to use Atlantis and the specific implementation you decide on will depend on your needs, but we will go over how to use it once you have it setup.</p>"},{"location":"gen3-resources/operator-guide/iac-atlantis/#making-changes","title":"Making changes","text":"<p>Once you have an Atlantis setup configured to your specific needs, you should be able to start making changes from pull requests. The way Atlantis works is it will watch your configured github repo for pull requests. Once it see's one it will run a specified workflow, which generally means it will plan your change. Depending on the configuration you have setup, it will then either comment on the pull request with the plan, or it will plan and apply if the plan is successful. We recommend setting it up to comment on the pull request with the plan, and then you can review the plan and approve it if it looks good. Once you approve the plan, Atlantis will apply the changes and comment on the pull request with the results. If you have it setup to run a plan and apply, then it will just comment on the pull request with the results of the apply.</p>"},{"location":"gen3-resources/operator-guide/iac-overview/","title":"Infrastructure as Code - Overview","text":""},{"location":"gen3-resources/operator-guide/iac-overview/#infrastructure-as-code-overview","title":"Infrastructure as Code Overview","text":"<p>Similar to how Helm helps with the gen3 software management, Terraform plays a crucial role in simplifying the deployment and management of infrastructure components within our supported cloud platforms, currently AWS. We define templates for the infrastructure components in the form of terraform modules, which are then used to deploy the infrastructure components in the cloud. The terraform modules are defined in the Gen3 Terraform GitHub repository.</p>"},{"location":"gen3-resources/operator-guide/iac-overview/#role-of-terraform-in-gen3-deployment","title":"Role of Terraform in Gen3 Deployment","text":"<p>In a Gen3 deployment, Terraform serves as the primary tool for:</p> <ul> <li> <p>Defining Cloud Infrastructure: Terraform uses directories of configuration files called modules to define what infrastructure should be deployed. These modules encapsulate the necessary configuration, dependencies, and deployment logic.</p> </li> <li> <p>Provisioning: Terraform streamlines the process of provisioning cloud infrastructure to support and run your gen3 instance. With terraform, you can easily deploy and manage the infrastructure components, necessary for your gen3 deployment in the cloud.</p> </li> <li> <p>Configuration Management: Terraform simplifies the management of configuration settings necessary for creating cloud infrastructure. You can customize settings, such as database size and type, EKS(AWS's managed kubernetes service) versions, and more, through terraform configuration files.</p> </li> <li> <p>Upgrades and Maintenance: As Gen3 and the cloud evolves, terraform enables you to effortlessly upgrade your infrastructure to deploy the latest, greatest and most secure hardware and configuration within the cloud. Using terraform, you can easily scale or modify your infrastructure to meet the growing needs of your gen3 deployment.</p> </li> </ul>"},{"location":"gen3-resources/operator-guide/iac-overview/#installation-and-configuration-of-helm","title":"Installation and Configuration of Helm","text":"<p>If you haven't already installed and configured terraform for your Gen3 deployment, follow these steps:</p> <ol> <li>Installation: Install Terraform by following their official guide, official terraform installation guide. We have written and tested our terraform on version 1.2, as well as our deprecated 0.11 versions of modules, so we recommend using either of those versions with their respective modules.</li> </ol> <p>With terraform installed and configured, you are ready to deploy and manage cloud infrastructure necessary to run gen3 within AWS effortlessly, ensuring a production ready gen3 system.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/","title":"Terraform","text":""},{"location":"gen3-resources/operator-guide/iac-terraform/#gen3-terraform","title":"Gen3 Terraform","text":"<p>Gen3-terraform contains our terraform modules for deploying and managing cloud infrastructure necessary for running gen3 within AWS. The modules are organized into directories based on the version of terraform they correspond to, then the type of infrastructure they deploy. The tf_files_deprecated directory contains our deprecated 0.11 versions of modules, while the tf_files directory contains our &gt;1.0 versions of modules, although we have been using version 1.2 of terraform with these modules, so we recommend using that version of terraform with these modules. Under these directories we break down into clouds and then individual portions of clouds. If using the latest terraform modules, the most important module will be the commons one, which will spin up all the necessary infrastructure for a gen3 commons. The other modules are used if you'd like to spin up individual components or supplementary infrastructure within a commons, like service accounts, AWS roles etc. It should go without saying, but you will need admin access to an AWS account to be able to run our terraform modules for AWS.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#usage","title":"Usage","text":"<p>There are are a few ways to utilize our terraform modules, which will largely depend on how many instances you plan to run and how hands off you want the maintenance to be. The following are a few options that we have utilized:</p> <ul> <li>Running Terraform within the cloned repo: This option allows you to run terraform directly within the cloned repo, which is useful for testing and development. This option is not recommended for production use, as it is not as secure as the other options, and it is not as easy to maintain. This option is also not recommended if you plan to run multiple instances of gen3, as you will have to clone the repo for each instance, and it will be difficult to maintain the state files for each instance.</li> <li>Referencing our Terraform modules within your own modules: This option allows you to reference our modules within your own modules, which is useful if you want to customize the modules to your needs. This option is recommended if you plan to run a few instances of gen3, as you can create a module for each instance, and it will be easier to maintain the state files for each instance. This option is also recommended if you want to customize the modules to your needs, as you can easily do so within your own modules.</li> <li>Wrapping terraform with Terragrunt: This option allows you to wrap terraform with terragrunt, which is useful if you want to run many instances of gen3, as it will allow you to run terraform across multiple modules at once. This option also let's you set configuration heirarchally, so you can set configuration at the root level, and it will be inherited by all the modules, allowing you to perform standard maintanance across every instance with ease. This approach adds a new layer of complexity though, so it is only recommended if you plan on managing many instances of gen3.</li> <li>Running terraform or terragrunt with Atlantis: This option can be used with the two above options, and it allows you to run terraform or terragrunt in a CI/CD pipeline, which is useful if you want to automate the deployment and maintenance of your gen3 instances. This option is recommended if you want to collaborate with others on the deployment and maintenance of your gen3 instances, as it allows you to easily review and approve changes to your infrastructure through github pr's.</li> </ul> <p>The terraform modules are the backbone of our infrastructure deployment, and there are many ways to utilize them effectively. We have provided a few examples of how we have utilized them, but we encourage you to explore other options as well, and find the one that works best for you. In the following sections we will go into detail on how to setup the above options, and how to utilize them to deploy and maintain your gen3 instances.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#running-terraform-from-gen3-terraform","title":"Running Terraform from Gen3-Terraform","text":""},{"location":"gen3-resources/operator-guide/iac-terraform/#prerequisites","title":"Prerequisites","text":""},{"location":"gen3-resources/operator-guide/iac-terraform/#downloads","title":"Downloads","text":"<p>You will need to follow the process outlined in the Terraform Getting Started page to download terraform. Once you have that downloaded you will need to clone our gen3-terraform repo. You will also need to install the AWS CLI to let terraform interact with AWS. Depending on the deployments you run, you may also need to install kubectl and postgres.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#cloud-credentials","title":"Cloud Credentials","text":"<p>You will need to configure your AWS CLI to have admin access to your account, that way terraform can deploy the necessary infrastructure within your account. There are a few ways to accomplish this, but the easiest way is to create a new IAM user, attach the \"arn:aws:iam::aws:policy/AdministratorAccess\" to the user and then create a access key/id. Once you have the key and secret you will need to create a file at ~/.aws/credentials and add the following.</p> Bash<pre><code>[default]\naws_access_key_id=&lt;access key&gt;\naws_secret_access_key=&lt;secret key&gt;\n</code></pre> <p>This access can be verified by running the following.</p> Bash<pre><code>aws sts get-caller-identity\n</code></pre> <p>If it returns the username of your user, then your credentials are properly configured.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#deployment","title":"Deployment","text":""},{"location":"gen3-resources/operator-guide/iac-terraform/#prepare-the-deployment","title":"Prepare the deployment","text":"<p>To run terraform directly from the gen3-terraform repo you'll need to start by navigating to the directory of the module you want to use. We recommend using the terraform 1.0 commons module. This will provision all the necessary config for a production gen3 instance. If you just want to create a more basic development cluster you can use this module instead.</p> <p>Once you're in the correct directory, you'll want to make a couple supplemental files to tell terraform how to store the information about the state of your deployment. Create a provider.tf file to tell terraform which cloud we are using.</p> Bash<pre><code>provider \"aws\" {}\n</code></pre> <p>Next create a backend.tf file, to tell terraform where to store the state file in s3.</p> Bash<pre><code>terraform {\n  backend \"s3\" {\n    bucket = \"s3 bucket name\"\n    encrypt = \"true\"\n    key = \"directory in bucket/terraform.tfstate\"\n    region = \"us-east-1\"\n  }\n}\n</code></pre> <p>Next take a look at the variables.tf file to see the possible variables you can change. To set the variables you can edit the default values in this file, or more ideally, you can create a config.tfvars file which will set the values for the variables. The file will look like this.</p> Bash<pre><code>vpc_name=\"your commons name\"\naws_region=\"us-east-1\"\n...\n</code></pre> <p>Once that is all setup you can proceed to running terraform.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#running-terraform","title":"Running terraform","text":"<p>Terraform will run in 2 parts, an initial plan and then an apply. This allows you to see the changes to your infrastructure to prevent unwanted changes that may affect the uptime of your application. Initially it should show only additions, but as you perform maintenance you will likely want to look at the plan more closely to ensure important resources are not deleted or modified.</p> <p>To create a terraform plan, you can run the following.</p> Bash<pre><code>terraform plan -var-file config.tfvars\n</code></pre> <p>Terraform should automatically pull in all the configuration from any files with a .tf extension in the current directory, as well as all references to the modules referenced in those files. The -var-file flag says to use the variables within the config.tfvars as variables within the module. After running this you should see a plan with a few hundred resources being added to your account.</p> <p>The next step is to apply these changes if you're happy with them. To do that you can run a similar command, but change plan to apply.</p> Bash<pre><code>terraform apply -var-file config.tfvars\n</code></pre> <p>It will ask you if you want to commit these changes and if you approve them then terraform will run for a while and create the resources.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#note","title":"Note","text":"<p>Terraform may time out creating resources or run into errors. If it does, it can be normal due to long provisioning processes and race conditions. If you see errors run apply a couple more times and if the errors persist, reach out over our community slack channel ffor assistance.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#maintenance","title":"Maintenance","text":""},{"location":"gen3-resources/operator-guide/iac-terraform/#changing-infrastructure","title":"Changing Infrastructure","text":"<p>The steps for performing maintenance are similar to deployment. You will need to access the cloned repo and go to the directory where you ran terraform. This will contain all the configuration you setup as well as a reference to your state file. If you don't have this, you can always recreate it from scratch and as long as your state file still exists in the S3 bucket you configured, everything will work as normal.</p> <p>Once you are in the properly prepped repo, you can make a change to the config.tfvars file, for example changing eks_version from 1.24 to 1.25 to ugrade EKS. After you make that change run a plan, although this time you will want to check the plan more carefully to make sure only the change you intend to make is happening, and run an apply.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#destroying-your-infrastructure","title":"Destroying Your Infrastructure","text":"<p>To destroy your infrastructure you can run the exact same commands as the plan and apply, only you will add a --destroy flag to the end.</p> Bash<pre><code>terraform plan -var-file config.tfvars --destroy\nterraform apply -var-file config.tfvars --destroy\n</code></pre> <p>This will let you see everything that will be destroyed and once you apply the change the destruction will commence.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#running-terraform-as-a-module","title":"Running Terraform as a Module","text":""},{"location":"gen3-resources/operator-guide/iac-terraform/#prerequisites_1","title":"Prerequisites","text":""},{"location":"gen3-resources/operator-guide/iac-terraform/#downloads_1","title":"Downloads","text":"<p>You will need to follow the process outlined in the Terraform Getting Started page to download terraform. Once you have that downloaded you will need to clone our gen3-terraform repo. You will also need to install the AWS CLI to let terraform interact with AWS. Depending on the deployments you run, you may also need to install kubectl and postgres.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#cloud-credentials_1","title":"Cloud Credentials","text":"<p>You will need to configure your AWS CLI to have admin access to your account, that way terraform can deploy the necessary infrastructure within your account. There are a few ways to accomplish this, but the easiest way is to create a new IAM user, attach the \"arn:aws:iam::aws:policy/AdministratorAccess\" to the user and then create a access key/id. Once you have the key and secret you will need to create a file at ~/.aws/credentials and add the following.</p> Bash<pre><code>[default]\naws_access_key_id=&lt;access key&gt;\naws_secret_access_key=&lt;secret key&gt;\n</code></pre> <p>This access can be verified by running the following.</p> Bash<pre><code>aws sts get-caller-identity\n</code></pre> <p>If it returns the username of your user, then your credentials are properly configured.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#deployment_1","title":"Deployment","text":""},{"location":"gen3-resources/operator-guide/iac-terraform/#prepare-the-deployment_1","title":"Prepare the deployment","text":"<p>To run terraform by referencing our modules from your own you'll need to start by creating a terraform repo, or utilizing one you already have. We recommend using the terraform 1.0 commons module. This will provision all the necessary config for a production gen3 instance. If you just want to create a more basic development cluster you can use this module instead.</p> <p>Once you decide the module you want to use you'll want to create a .tf file that references our module, making sure to include the reference to the github repo as well as the variables you'll be passing into the module. Since you will likely want to use the module as is, you will likely want to pass in every variable. It should look similar to the following.</p> Bash<pre><code>provider \"aws\" {}\n\nterraform {\n  backend \"s3\" {\n    bucket = \"s3 bucket\"\n    encrypt = \"true\"\n    key = \"directory in bucket/terraform.tfstate\"\n    region = \"us-east-1\"\n  }\n}\n\n\nmodule \"commons\" {\n  source = \"git::github.com/uc-cdis/gen3-terraform.git//tf_files/aws/commons?ref=master\"\n\n  vpc_name=var.vpc_name\n  iam_role_name=var.iam_role_name\n  vpc_cidr_block=var.vpc_cidr_block\n  vpc_flow_logs=var.vpc_flow_logs\n  vpc_flow_traffic=var.vpc_flow_traffic\n  aws_region=var.aws_region\n  aws_cert_name=var.aws_cert_name\n  csoc_account_id=var.csoc_account_id\n  peering_cidr=var.peering_cidr\n  fence_db_size=var.fence_db_size\n  sheepdog_db_size=var.sheepdog_db_size\n  indexd_db_size=var.indexd_db_size\n  db_password_fence=var.db_password_fence\n  db_password_peregrine=var.db_password_peregrine\n  db_password_sheepdog=var.db_password_sheepdog\n  db_password_indexd=var.db_password_indexd\n  portal_app=var.portal_app\n  fence_snapshot=var.fence_snapshot\n  peregrine_snapshot=var.peregrine_snapshot\n  sheepdog_snapshot=var.sheepdog_snapshot\n  indexd_snapshot=var.indexd_snapshot\n  fence_db_instance=var.fence_db_instance\n  sheepdog_db_instance=var.sheepdog_db_instance\n  indexd_db_instance=var.indexd_db_instance\n  hostname=var.hostname\n  kube_ssh_key=var.kube_ssh_key\n  kube_additional_keys=var.kube_additional_keys\n  hmac_encryption_key=var.hmac_encryption_key\n  sheepdog_secret_key=var.sheepdog_secret_key\n  sheepdog_indexd_password=var.sheepdog_indexd_password\n  sheepdog_oauth2_client_id=var.sheepdog_oauth2_client_id\n  config_folder=var.config_folder\n  sheepdog_oauth2_client_secret=var.sheepdog_oauth2_client_secret\n  ami_account_id=var.ami_account_id\n  squid_image_search_criteria=var.squid_image_search_criteria\n  peering_vpc_id=var.peering_vpc_id\n  squid-nlb-endpointservice-name=var.squid-nlb-endpointservice-name\n  slack_webhook=var.slack_webhook\n  secondary_slack_webhook=var.secondary_slack_webhook\n  alarm_threshold=var.alarm_threshold\n  csoc_managed=var.csoc_managed\n  csoc_peering=var.csoc_peering\n  mailgun_api_key=var.mailgun_api_key\n  mailgun_smtp_host=var.mailgun_smtp_host\n  mailgun_api_url=var.mailgun_api_url\n  fence_ha=var.fence_ha\n  sheepdog_ha=var.sheepdog_ha\n  indexd_ha=var.indexd_ha\n  fence_maintenance_window=var.fence_maintenance_window\n  sheepdog_maintenance_window=var.sheepdog_maintenance_window\n  indexd_maintenance_window=var.indexd_maintenance_window\n  fence_backup_retention_period=var.fence_backup_retention_period\n  sheepdog_backup_retention_period=var.sheepdog_backup_retention_period\n  indexd_backup_retention_period=var.indexd_backup_retention_period\n  fence_backup_window=var.fence_backup_window\n  sheepdog_backup_window=var.sheepdog_backup_window\n  indexd_backup_window=var.indexd_backup_window\n  engine_version=var.engine_version\n  fence_auto_minor_version_upgrade=var.fence_auto_minor_version_upgrade\n  indexd_auto_minor_version_upgrade=var.indexd_auto_minor_version_upgrade\n  sheepdog_auto_minor_version_upgrade=var.sheepdog_auto_minor_version_upgrade\n  users_bucket_name=var.users_bucket_name\n  fence_database_name=var.fence_database_name\n  sheepdog_database_name=var.sheepdog_database_name\n  indexd_database_name=var.indexd_database_name\n  fence_db_username=var.fence_db_username\n  sheepdog_db_username=var.sheepdog_db_username\n  indexd_db_username=var.indexd_db_username\n  fence_allow_major_version_upgrade=var.fence_allow_major_version_upgrade\n  sheepdog_allow_major_version_upgrade=var.sheepdog_allow_major_version_upgrade\n  indexd_allow_major_version_upgrade=var.indexd_allow_major_version_upgrade\n  ha-squid_instance_type=var.ha-squid_instance_type\n  ha-squid_instance_drive_size=var.ha-squid_instance_drive_size\n  deploy_single_proxy=var.deploy_single_proxy\n  ha-squid_bootstrap_script=var.ha-squid_bootstrap_script\n  ha-squid_extra_vars=var.ha-squid_extra_vars\n  branch=var.branch\n  fence-bot_bucket_access_arns=var.fence-bot_bucket_access_arns\n  deploy_ha_squid=var.deploy_ha_squid\n  ha-squid_cluster_desired_capasity=var.ha-squid_cluster_desired_capasity\n  ha-squid_cluster_min_size=var.ha-squid_cluster_min_size\n  ha-squid_cluster_max_size=var.ha-squid_cluster_max_size\n  deploy_sheepdog_db=var.deploy_sheepdog_db\n  deploy_fence_db=var.deploy_fence_db\n  deploy_indexd_db=var.deploy_indexd_db\n  single_squid_instance_type=var.single_squid_instance_type\n  network_expansion=var.network_expansion\n  rds_instance_storage_encrypted=var.rds_instance_storage_encrypted\n  fence_max_allocated_storage=var.fence_max_allocated_storage\n  sheepdog_max_allocated_storage=var.sheepdog_max_allocated_storage\n  indexd_max_allocated_storage=var.indexd_max_allocated_storage\n  activation_id=var.activation_id\n  customer_id=var.customer_id\n  fips=var.fips\n  ignore_fence_changes=var.ignore_fence_changes\n  ignore_sheepdog_changes=var.ignore_sheepdog_changes\n  ignore_indexd_changes=var.ignore_indexd_changes\n  prevent_fence_destroy=var.prevent_fence_destroy\n  prevent_sheepdog_destroy=var.prevent_sheepdog_destroy\n  prevent_indexd_destroy=var.prevent_indexd_destroy\n  deploy_alarms=var.deploy_alarms\n  ec2_keyname=var.ec2_keyname\n  instance_type=var.instance_type\n  jupyter_instance_type=var.jupyter_instance_type\n  workflow_instance_type=var.workflow_instance_type\n  secondary_cidr_block=var.secondary_cidr_block\n  users_policy=var.users_policy\n  worker_drive_size=var.worker_drive_size\n  eks_version=var.eks_version\n  workers_subnet_size=var.workers_subnet_size\n  bootstrap_script=var.bootstrap_script\n  jupyter_bootstrap_script=var.jupyter_bootstrap_script\n  kernel=var.kernel\n  jupyter_worker_drive_size=var.jupyter_worker_drive_size\n  workflow_bootstrap_script=var.workflow_bootstrap_script\n  workflow_worker_drive_size=var.workflow_worker_drive_size\n  cidrs_to_route_to_gw=var.cidrs_to_route_to_gw\n  organization_name=var.organization_name\n  jupyter_asg_desired_capacity=var.jupyter_asg_desired_capacity\n  jupyter_asg_max_size=var.jupyter_asg_max_size\n  jupyter_asg_min_size=var.jupyter_asg_min_size\n  workflow_asg_desired_capacity=var.workflow_asg_desired_capacity\n  workflow_asg_max_size=var.workflow_asg_max_size\n  workflow_asg_min_size=var.workflow_asg_min_size\n  iam-serviceaccount=var.iam-serviceaccount\n  domain_test=var.domain_test\n  deploy_workflow=var.deploy_workflow\n  secondary_availability_zones=var.secondary_availability_zones\n  deploy_jupyter=var.deploy_jupyter\n  dual_proxy=var.dual_proxy\n  single_az_for_jupyter=var.single_az_for_jupyter\n  oidc_eks_thumbprint=var.oidc_eks_thumbprint\n  sns_topic_arn=var.sns_topic_arn\n  fips_ami_kms=var.fips_ami_kms\n  fips_enabled_ami=var.fips_enabled_ami\n  availability_zones=var.availability_zones\n  deploy_eks=var.deploy_eks\n  deploy_es=var.deploy_es\n  ebs_volume_size_gb=var.ebs_volume_size_gb\n  encryption=var.encryption\n  es_instance_type=var.es_instance_type\n  es_instance_count=var.es_instance_count\n  es_version=var.es_version\n  es_linked_role=var.es_linked_role\n  cluster_identifier=var.cluster_identifier\n  cluster_instance_identifier=var.cluster_instance_identifier\n  cluster_instance_class=var.cluster_instance_class\n  cluster_engine=var.cluster_engine\n  cluster_engine_version=var.cluster_engine_version\n  master_username=var.master_username\n  storage_encrypted=var.storage_encrypted\n  apply_immediate=var.apply_immediate\n  engine_mode=var.engine_mode\n  serverlessv2_scaling_min_capacity=var.serverlessv2_scaling_min_capacity\n  serverlessv2_scaling_max_capacity=var.serverlessv2_scaling_max_capacity\n  skip_final_snapshot=var.skip_final_snapshot\n  final_snapshot_identifier=var.final_snapshot_identifier\n  backup_retention_period=var.backup_retention_period\n  preferred_backup_window=var.preferred_backup_window\n  password_length=var.password_length\n  deploy_aurora=var.deploy_aurora\n  deploy_rds=var.deploy_rds\n  use_asg=var.use_asg\n  use_karpenter=var.use_karpenter\n  karpenter_version=var.karpenter_version\n  deploy_cloud_trail=var.deploy_cloud_trail\n  send_logs_to_csoc=var.send_logs_to_csoc\n  route_table_name=var.route_table_name\n  eks_public_access=var.eks_public_access\n  deploy_gen3=var.deploy_gen3\n  ambassador_enabled=var.ambassador_enabled\n  arborist_enabled=var.arborist_enabled\n  argo_enabled=var.argo_enabled\n  audit_enabled=var.audit_enabled\n  aws-es-proxy_enabled=var.aws-es-proxy_enabled\n  dbgap_enabled=var.dbgap_enabled\n  dd_enabled=var.dd_enabled\n  dictionary_url=var.dictionary_url\n  dispatcher_job_number=var.dispatcher_job_number\n  fence_enabled=var.fence_enabled\n  guppy_enabled=var.guppy_enabled\n  hatchery_enabled=var.hatchery_enabled\n  indexd_enabled=var.indexd_enabled\n  indexd_prefix=var.indexd_prefix\n  ingress_enabled=var.ingress_enabled\n  manifestservice_enabled=var.manifestservice_enabled\n  metadata_enabled=var.metadata_enabled\n  netpolicy_enabled=var.netpolicy_enabled\n  peregrine_enabled=var.peregrine_enabled\n  pidgin_enabled=var.pidgin_enabled\n  portal_enabled=var.portal_enabled\n  public_datasets=var.public_datasets\n  requestor_enabled=var.requestor_enabled\n  revproxy_arn=var.revproxy_arn\n  revproxy_enabled=var.revproxy_enabled\n  sheepdog_enabled=var.sheepdog_enabled\n  slack_send_dbgap=var.slack_send_dbgap\n  ssjdispatcher_enabled=var.ssjdispatcher_enabled\n  tier_access_level=var.tier_access_level\n  tier_access_limit=var.tier_access_limit\n  usersync_enabled=var.usersync_enabled\n  usersync_schedule=var.usersync_schedule\n  useryaml_s3_path=var.useryaml_s3_path\n  wts_enabled=var.wts_enabled\n  fence_config_path=var.fence_config_path\n  useryaml_path=var.useryaml_path\n  gitops_path=var.gitops_path\n  google_client_id=var.google_client_id\n  google_client_secret=var.google_client_secret\n  fence_access_key=var.fence_access_key\n  fence_secret_key=var.fence_secret_key\n  upload_bucket=var.upload_bucket\n  namespace=var.namespace\n  secrets_manager_enabled=var.secrets_manager_enabled\n  ci_run=var.ci_run\n}\n</code></pre> <p>Next you'll need to define the variables you just passed into our module, since this is technically a new terraform module. So you'll need to make a variables.tf file that should be an exact clone of the variables.tf file of our variables.tf file. If you are adding this to an existing module then you can just append our variables to your current file.</p> <p>Once that is all setup you can proceed to running terraform.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#running-terraform_1","title":"Running terraform","text":"<p>Terraform will run in 2 parts, an initial plan and then an apply. This allows you to see the changes to your infrastructure to prevent unwanted changes that may affect the uptime of your application. Initially it should show only additions, but as you perform maintenance you will likely want to look at the plan more closely to ensure important resources are not deleted or modified.</p> <p>To create a terraform plan, you can run the following.</p> Bash<pre><code>terraform plan -var-file config.tfvars\n</code></pre> <p>Terraform should automatically pull in all the configuration from any files with a .tf extension in the current directory, as well as all references to the modules referenced in those files. The -var-file flag says to use the variables within the config.tfvars as variables within the module. After running this you should see a plan with a few hundred resources being added to your account.</p> <p>The next step is to apply these changes if you're happy with them. To do that you can run a similar command, but change plan to apply.</p> Bash<pre><code>terraform apply -var-file config.tfvars\n</code></pre> <p>It will ask you if you want to commit these changes and if you approve them then terraform will run for a while and create the resources.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#note_1","title":"Note","text":"<p>Terraform may time out creating resources or run into errors. If it does, it can be normal due to long provisioning processes and race conditions. If you see errors run apply a couple more times and if the errors persist, reach out over our community slack channel for assistance.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#maintenance_1","title":"Maintenance","text":""},{"location":"gen3-resources/operator-guide/iac-terraform/#changing-infrastructure_1","title":"Changing Infrastructure","text":"<p>The steps for performing maintenance are similar to deployment. You will need to access the cloned repo and go to the directory where you ran terraform. This will contain all the configuration you setup as well as a reference to your state file. If you don't have this, you can always recreate it from scratch and as long as your state file still exists in the S3 bucket you configured, everything will work as normal.</p> <p>Once you are in the properly prepped repo, you can make a change to the config.tfvars file, for example changing eks_version from 1.24 to 1.25 to ugrade EKS. After you make that change run a plan, although this time you will want to check the plan more carefully to make sure only the change you intend to make is happening, and run an apply.</p>"},{"location":"gen3-resources/operator-guide/iac-terraform/#destroying-your-infrastructure_1","title":"Destroying Your Infrastructure","text":"<p>To destroy your infrastructure you can run the exact same commands as the plan and apply, only you will add a --destroy flag to the end.</p> Bash<pre><code>terraform plan -var-file config.tfvars --destroy\nterraform apply -var-file config.tfvars --destroy\n</code></pre> <p>This will let you see everything that will be destroyed and once you apply the change the destruction will commence.</p>"},{"location":"gen3-resources/operator-guide/iac-terragrunt/","title":"Terragrunt","text":""},{"location":"gen3-resources/operator-guide/iac-terragrunt/#terragrunt","title":"Terragrunt","text":"<p>Terragrunt is a way to deploy and manage Gen3 systems. It is a wrapper around the gen3-terraform repo, which contains the terraform modules used to deploy and manage gen3 commons.</p> <p>Using a terragrunt repo allows you to deploy and manage multiple Gen3 commons at once, and it also allows you to set configuration in a heirarchical manner, having it inherited by all the commons, allowing you to perform standard maintenance across every instance with ease.</p>"},{"location":"gen3-resources/operator-guide/iac-terragrunt/#prerequisites","title":"Prerequisites","text":""},{"location":"gen3-resources/operator-guide/iac-terragrunt/#downloads","title":"Downloads","text":"<p>You will need to follow the process outlined in the Terraform Getting Started page to download terraform. Once you have that downloaded you will need to clone our gen3-terraform repo. You will also need to install the AWS CLI to let terraform interact with AWS. Depending on the deployments you run, you may also need to install kubectl and postgres. Last you will need to install terragrunt.</p>"},{"location":"gen3-resources/operator-guide/iac-terragrunt/#cloud-credentials","title":"Cloud Credentials","text":"<p>You will need to configure your AWS CLI to have admin access to your account, that way terraform can deploy the necessary infrastructure within your account. There are a few ways to accomplish this, but the easiest way is to create a new IAM user, attach the \"arn:aws:iam::aws:policy/AdministratorAccess\" to the user and then create a access key/id. Once you have the key and secret you will need to create a file at ~/.aws/credentials and add the following.</p> Bash<pre><code>[default]\naws_access_key_id=&lt;access key&gt;\naws_secret_access_key=&lt;secret key&gt;\n</code></pre> <p>This access can be verified by running the following.</p> Bash<pre><code>aws sts get-caller-identity\n</code></pre> <p>If it returns the username of your user, then your credentials are properly configured.</p>"},{"location":"gen3-resources/operator-guide/iac-terragrunt/#deployment","title":"Deployment","text":""},{"location":"gen3-resources/operator-guide/iac-terragrunt/#prepare-the-deployment","title":"Prepare the deployment","text":"<p>To run terragrunt you'll need to start by creating a terragrunt repo, or utilizing one you already have. We recommend using the terraform 1.0 commons module module within your terragrunt repo. This will provision all the necessary config for a production gen3 instance. If you just want to create a more basic development cluster you can use this module instead.</p> <p>If you decided to take the terragrunt approach then it's likely you have multiple instances of gen3 you plan on managing, otherwise we do not recommend using terragrunt as it will be overkill. With that in mind, the next step is to logically break down how you want your directory structure to look. We deploy commons within different AWS account, and can have multiple commons per account, depending on need, so our directory structure looks as follows</p> Text Only<pre><code>Cloud Provider -&gt; Account Number -&gt; Commons Instance(ie prod/staging) -&gt; terraform module folder\n</code></pre> <p>However, if you have one account you run things out of then it could also look like this.</p> Text Only<pre><code>Commons Instance(Prod/Staging/Dev) -&gt; terraform module folder\n</code></pre> <p>The actual breakdown doesn't matter as much, but it will help you with maintenance tasks due to how terragrunt runs. Terragrunt allows variables to be set at higher levels and cascade down. So if you want to deploy an EKS version change across each commons, you can set the variable at the root level and have it cascade down. Furthermore, if you want to split these up by Prod/Staging you can just as easily run an upgrade in staging first, then production by setting the variable in a config file within those directories respectively. The lower level variable declarations take precedence though, so if you hard code one of your commons to have a certain EKS version, then it will stay there even if you include the change in a higher level. This makes maintenance of many many commons easy, although if you don't have many, then it doesn't make much sense to use terragrunt, as terraform will suffice.</p> <p>Once you decide on your directory structure you'll want to create terragrunt config files in each level of your hierarchy. These files should have a .hcl extension and look like the following.</p> Bash<pre><code>inputs = merge(local.account_vars.inputs, {\n  vpc_name         = local.vpc_name\n  users_policy     = local.vpc_name\n  es_instance_type = \"t3.small.elasticsearch\"\n})\n\nlocals {\n  account_vars = read_terragrunt_config(find_in_parent_folders(\"account.hcl\"))\n\n  vpc_name       = \"testvpc\"\n}\n</code></pre> <p>The important parts are as follows. First, the inputs section is what gets cascaded down into the final variables file. The merge section contains a reference to the local.account_vars.inputs, which if you look at the locals section references \"read_terragrunt_config(find_in_parent_folders(\"account.hcl\"))\", which basically means it is pulling the config from a file in higher level called account.hcl. That file will look similar to this one, but have variables being set in the inputs section that you want to cascade down into the next file etc. You need to make sure the hcl file names are unique to avoid any weirdness, but with that set, you should be able to set most of your variables to cascase down into your actual terraform directory.</p> <p>Once you are in the lowest level directory, the directory where you will add your terraform, you will make another .hcl file, but it will look like the following.</p> Bash<pre><code>locals {\n  # Load environment-wide variables\n  environment_vars = read_terragrunt_config(find_in_parent_folders(\"env.hcl\"))\n  vpc_name = local.environment_vars.inputs.vpc_name\n\n  account_number = local.environment_vars.locals.account_vars.locals.account_number\n}\n\nterraform {\n  source = \"git::github.com/uc-cdis/gen3-terraform.git//tf_files/aws/commons?ref=master\"\n}\n\n# Include all settings from the root terragrunt.hcl file\ninclude {\n  path = find_in_parent_folders()\n}\n\ngenerate \"provider\" {\n  path      = \"provider.tf\"\n  if_exists = \"overwrite\"\n  contents = &lt;&lt;EOF\nprovider \"aws\" {}\nEOF\n}\n\n# These are the variables we have to pass in to use the module specified in the terragrunt configuration above\ninputs = merge(local.environment_vars.inputs, {\n  hostname=\"test.planx-pla.net\"\n  deploy_rds=true\n  eks_version=\"1.24\"\n})\n\n\nremote_state {\n  backend = \"s3\"\n  config = {\n    bucket = \"bucket name\"\n    encrypt = true\n    key = \"directory in bucket/terraform.tfstate\"\n    region = \"your aws region\"\n  }\n}\n</code></pre> <p>This should look somewhat similar to the setup within the running terraform as a module, but there are a few small changes. For one, we are setting the inputs section to merge in the variables from higher up in the hierarchy, and we have changed the syntax around setting a few other terraform blocks.</p> <p>Once this is setup, you can do the same with any other commons deployments you have by setting up other directories with files that look like this. After all of your commons are defined in the repo, you can move on to running terragrunt.</p>"},{"location":"gen3-resources/operator-guide/iac-terragrunt/#running-terragrunt","title":"Running terragrunt","text":"<p>Terragrunt will run similar to terraform, in that it has a plan and apply phase. In fact it supports the exact same arguments as terraform, with a major exception, the run-all command. Terragrunt allows you to set run-all as the first argument, and any supported terrafrom command after that will run against every defined terrafrom module under the current directory in your repo. To put this more plainly, if you run the following</p> Bash<pre><code>terragrunt run-all plan\n</code></pre> <p>from the root level of your repo, it will plan every terraform module defined. Whereas, if you run it from a specific instance of a commons it will only plan that one commons module. This should show why logically defining your directory structure at the beginning is so powerful. If you know the difference places you want to make changes in order, you can very easily move from directory to directory and make changes incrementally, against subsets of your different gen3 instances.</p> <p>With that in mind, the commands to run changes are as follows</p> <p>This will run a single plan </p>Bash<pre><code>terragrunt plan\n</code></pre> <p>This will run a plan against all resources under the current directory </p>Bash<pre><code>terragrunt run-all plan\n</code></pre> <p>This will run a single apply </p>Bash<pre><code>terragrunt apply\n</code></pre> <p>This will run an apply against all resources under the current directory </p>Bash<pre><code>terragrunt run-all apply\n</code></pre>"},{"location":"gen3-resources/operator-guide/prerequisites/","title":"Prerequisites for Deploying Gen3","text":""},{"location":"gen3-resources/operator-guide/prerequisites/#prerequisites-for-production-deployment-of-gen3","title":"Prerequisites for Production Deployment of Gen3","text":""},{"location":"gen3-resources/operator-guide/prerequisites/#overview","title":"Overview","text":"<p>Before initiating a Gen3 deployment, it's essential to ensure that you have the following prerequisites in place:</p>"},{"location":"gen3-resources/operator-guide/prerequisites/#system-requirements","title":"System Requirements","text":""},{"location":"gen3-resources/operator-guide/prerequisites/#hardware","title":"Hardware:","text":"<ul> <li>Adequate hardware resources, including CPU, RAM, and storage, to support the expected workload and data volume of your Gen3 deployment.</li> <li>Specifications may vary based on the scale and requirements of your Gen3 environment.</li> </ul>"},{"location":"gen3-resources/operator-guide/prerequisites/#software-requirements","title":"Software Requirements","text":""},{"location":"gen3-resources/operator-guide/prerequisites/#kubernetes","title":"Kubernetes:","text":"<ul> <li>Kubernetes is a fundamental requirement for Gen3 deployments. Ensure that you have a Kubernetes cluster set up, configured, and accessible.</li> <li>Gen3 is designed to work with Kubernetes for container orchestration.</li> <li>Utilizing a managed Kubernetes service for production deployments enhances reliability, scalability, and security in your Gen3 environment</li> </ul>"},{"location":"gen3-resources/operator-guide/prerequisites/#helm","title":"Helm:","text":"<ul> <li>Helm, the Kubernetes package manager, is used to deploy and manage Gen3 components.</li> </ul>"},{"location":"gen3-resources/operator-guide/prerequisites/#access-to-necessary-resources","title":"Access to Necessary Resources","text":""},{"location":"gen3-resources/operator-guide/prerequisites/#cloud-provider-account-if-applicable","title":"Cloud Provider Account (if applicable):","text":"<ul> <li>If your Gen3 deployment is in a cloud environment, you'll need access to a cloud provider account (e.g., AWS, GCP, Azure) with the necessary permissions to create and manage resources.</li> </ul>"},{"location":"gen3-resources/operator-guide/prerequisites/#storage","title":"Storage:","text":"<ul> <li>Ensure you have ample and scalable storage resources to accommodate data storage and backups, with careful planning for future data growth.</li> <li>Gen3 supports S3-compatible object storage systems, making it a versatile and efficient storage solution.</li> </ul>"},{"location":"gen3-resources/operator-guide/prerequisites/#required-knowledge-and-permissions","title":"Required Knowledge and Permissions","text":""},{"location":"gen3-resources/operator-guide/prerequisites/#technical-expertise","title":"Technical Expertise:","text":"<ul> <li>Ensure that your team possesses the technical expertise required to set up and maintain a Gen3 deployment. This includes cloud-native technologies like Kubernetes, Helm, Docker, as well as knowledge related to cloud computing environments.</li> </ul>"},{"location":"gen3-resources/operator-guide/prerequisites/#specific-services","title":"Specific Services","text":""},{"location":"gen3-resources/operator-guide/prerequisites/#postgresql","title":"PostgreSQL","text":"<p>Most Gen3 services rely on PostgreSQL for relational data storage. This includes services like Fence, Peregrine, and more.</p> <p>PostgreSQL is required to provide:</p> <ul> <li>Relational data storage</li> <li>Transaction support</li> <li>Advanced data types</li> <li>Performance at scale</li> <li>Reliability and backups</li> </ul> <p>For production deployments, a managed PostgreSQL service is recommended:</p> <ul> <li>On AWS: RDS or Aurora</li> <li>On Google Cloud: Cloud SQL</li> <li>On Azure: Azure Database for PostgreSQL</li> </ul> <p>Using a fully managed PostgreSQL service avoids the overhead of maintaining PostgreSQL and simplifies operations.</p>"},{"location":"gen3-resources/operator-guide/prerequisites/#elasticsearch","title":"Elasticsearch","text":"<p>Elasticsearch is used in Gen3 for indexing and searching data. It provides:</p> <ul> <li>Full text search</li> <li>Flexible querying</li> <li>Scalable analytics</li> <li>Fast aggregations</li> </ul> <p>For production, a managed Elasticsearch service is recommended:</p> <p>On AWS: Amazon Elasticsearch Service</p> <p>Using a fully managed service simplifies operations.</p> <p>Currently Gen3 supports up to Elasticsearch 7. Support for newer versions is in progress. When deploying, use Elasticsearch 7 or earlier.</p>"},{"location":"gen3-resources/operator-guide/prerequisites/#etl-process-for-guppy","title":"ETL process for Guppy","text":"<p>Gen3 uses an ETL (Extract, Transform, Load) process to optimize data for easier exploration.</p> <p>This ETL pipeline:</p> <ul> <li>Extracts data from the Sheepdog database</li> <li>Transforms the data into an optimized format</li> <li>Loads the transformed data into Elasticsearch</li> </ul> <p>The Elasticsearch index powers the guppy service used in data explorer UI:</p> <ul> <li>Fast filtering and aggregation</li> <li>Quick exploration of complex datasets</li> <li>Summary stats and visualizations</li> </ul> <p>The ETL process runs on new data added to Sheepdog, keeping the Guppy cache synchronized.</p> <p>The ETL job handles ingesting large amounts of data, so Guppy can quickly serve queries and visualizations.</p>"},{"location":"gen3-resources/operator-guide/prerequisites/#argo","title":"Argo","text":"<p>Argo Workflows is an optional component used in certain Gen3 commons to define and execute automated workflows. Please be aware that Argo-Workflows support is currently in progress and will soon become a mandatory requirement.</p> <p>It plays a role in simplifying and automating complex tasks within the Gen3 ecosystem.</p>"},{"location":"gen3-resources/operator-guide/prerequisites/#key-features","title":"Key Features","text":"<p>Argo Workflows provides the following key features:</p> <ul> <li> <p>Workflow Definitions in YAML: Argo allows you to define workflows using YAML, providing a human-readable and easily maintainable way to describe your processes.</p> </li> <li> <p>DAG-based Orchestration: The orchestration of tasks in Argo Workflows is based on directed acyclic graphs (DAGs), making it suitable for managing complex dependencies between tasks.</p> </li> <li> <p>User Interface (UI) and API: Argo offers a user-friendly web-based UI and a programmable API to facilitate the execution of workflows on demand. This enables users to interact with and manage their workflows efficiently.</p> </li> <li> <p>Metrics and Logs: Argo Workflows provides comprehensive metrics and logs, offering insight into the status and performance of running workflows. This observability feature is crucial for monitoring and troubleshooting workflows.</p> </li> </ul>"},{"location":"gen3-resources/operator-guide/prerequisites/#use-cases-in-gen3","title":"Use Cases in Gen3","text":"<p>Argo Workflows is integral to Gen3, serving various purposes, including:</p> <ul> <li> <p>ETL Pipelines: Gen3 utilizes Argo Workflows to automate Extract, Transform, Load (ETL) processes, enabling efficient data ingestion and transformation.</p> </li> <li> <p>Usersync: The Usersync process, responsible for synchronizing user data, is orchestrated using Argo Workflows, ensuring user information remains up-to-date.</p> </li> <li> <p>Administrative and Housekeeping Tasks: Argo Workflows is employed for automating administrative and housekeeping tasks within the Gen3 environment, such as data archiving, backup management, and resource scaling.</p> </li> </ul>"},{"location":"gen3-resources/operator-guide/prerequisites/#prerequisites-for-workflow-automation","title":"Prerequisites for Workflow Automation","text":"<p>Argo Workflows is a critical prerequisite for any Gen3 commons aiming to leverage workflow automation. The integration of Argo Workflows with Gen3's authentication and authorization systems is facilitated through the argo-wrapper service. This integration ensures secure and controlled access to workflow execution capabilities within the Gen3 platform.</p> <p>By configuring Argo Workflows and the associated <code>argo-wrapper</code> service, Gen3 commons can harness the power of workflow automation to streamline data processing, user management, and administrative tasks.</p> <p>For detailed information on setting up Argo Workflows and configuring the <code>argo-wrapper</code> service within your Gen3 environment, please refer to the official Gen3 documentation and relevant installation guides.</p>"},{"location":"gen3-resources/operator-guide/ssl-cert/","title":"Prepare SSL Certificate","text":""},{"location":"gen3-resources/operator-guide/ssl-cert/#ssl-certificate","title":"SSL Certificate","text":"<ul> <li>Obtain an SSL/TLS certificate for securing your Gen3 deployment. You can use a certificate authority (CA) or use Let's Encrypt with Certbot for free certificates.</li> <li>Ensure you have a valid domain name for your Gen3 deployment.</li> </ul> <p>A certificate can be created using Certbot. It will ask you to create a DNS TXT record to verify domain ownership.</p> Text Only<pre><code>sudo certbot certonly --manual --preferred-challenges=dns -d fairtox.com\n</code></pre> <p>Complete the DNS challenge, wait for DNS (1-5 min) and then click continue.</p> <p>Once you have the certificate create a kubernetes secret with it.</p> Text Only<pre><code>kubectl create secret tls &lt;secret-name&gt; --cert=&lt;path-to-certificate.pem&gt; --key=&lt;path-to-key.pem&gt;\n</code></pre> <p>We will use this secret later on in our deployment.</p>"},{"location":"gen3-resources/operator-guide/submit-semi-structured-data/","title":"Semi-structured data","text":"","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-semi-structured-data/#semi-structured-data","title":"Semi-structured Data","text":"<p>Semi-structured data is organized as unique identifiers with flexible key/value pairs (including nesting). The key/value pairs may be consistent between records, but are not required to be. This is typically used for storing publicly available metadata about available datasets or additional public metadata about samples. In Gen3, semi-structured data can be stored in the Metadata Service (MDS) or the Aggregated Metadata Service (AggMDS).  Both the MDS or AggMDS can power the Data Portal Discovery Page.</p> <p>Because the structure of a commons' MDS and the Discovery Page configuration are very closely coupled, all content related to creating MDS records are included in the Customize Gen3 Search Interface Section.</p> <p>Instructions for the creation and modification of an MDS record can be found here as part of the Gen3 SDK.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-semi-structured-data/#discovery-page","title":"Discovery Page","text":"","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-structured-data/","title":"Structured Data","text":"","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-structured-data/#structured-data-clinical-or-experimental-data","title":"Structured Data (Clinical or experimental data)","text":"<p>Data is structured if it is organized into records and fields, with each record consisting of one or more data elements (data fields). In biomedical data, data fields are often restricted to controlled vocabularies to make querying them easier. In Gen3 this would include clinical or experimental data submitted to the graph model, which is queriable via a GraphQL API. It can be flattened (via ETL) and the result viewable on the Data Portal Exploration Page.</p> <p>After creating a data dictionary you are ready to submit structured data. These data are submitted in tab-separated value (TSV) files for each node in the project, which can be downloaded from the \"Dictionary\" page of the data commons website. (e.g. Gen3 Data Hub).</p> <p>It may be helpful to think of each TSV as a node in the graph of the data model. Column headers in the TSV are the properties stored in that node, and each row represents a \"record\" or \"entity\" in that node. When a TSV is successfully submitted, each row in that TSV becomes a single record in the node.</p> <p>Properties in a node are either required or not, and this can be determined by referencing the data dictionary's viewer's \"Required\" column for a specific node.</p> <p>There are a number of properties that deserve special mention:</p> <ul> <li> <p><code>submitter_id</code>: Each record in every node will have a <code>submitter_id</code>, which is a unique alphanumeric identifier (any combination of ASCII characters) for that record across the whole project and is specified by the data submitter. It is entirely up to the data contributor what the submitter_id will be for each record in a project, but the string chosen must be unique within that project.</p> </li> <li> <p><code>type</code>: Every node has a <code>type</code> property, which is simply the name of the node. By providing the node name in the \"type\" property, the submission portal knows which node to put the data in.</p> </li> <li> <p><code>id</code>: Every record in every node in a data commons has the unique property <code>id</code>, which is not submitted by the data contributor but rather generated on the backend. The value of the property <code>id</code> is a 128-bit GUID (a unique 32 character identifier).</p> </li> <li> <p><code>project_id</code> and <code>code</code>: Every project record in a data commons is linked to a parent <code>program</code> node and has the properties <code>project_id</code> and a <code>code</code>. The property <code>project_id</code> is the dash-separated combination of <code>program</code> and the project's <code>code</code>. For example, if your project was named 'Experiment1', and this project was part of the 'Pilot' program, the project's <code>project_id</code> would be 'Pilot-Experiment1', and the project's <code>code</code> would be 'Experiment1'. Finally, just like every record in the data commons, the project has the unique property <code>id</code>, which is not to be confused with the project's <code>project_id</code>.</p> </li> </ul> <p>Template TSVs are provided in each node's page in the data dictionary.</p> <p></p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-structured-data/#order-of-node-submission","title":"Order of Node Submission","text":"<p>The prepared TSV files must be submitted in a specific order due to node links. Referring back to the graphical data model, a record cannot be submitted without first submitting the record(s) to which it is linked upstream (its \"parent\"). If nodes are submitted out of order, such as submitting a TSV with links to parent records that don't yet exist, the validator will reject the submission on the basis that the dependency is not present with the error message, \"INVALID_LINK\".</p> <p>In a Gen3 Data Commons, <code>programs</code> and <code>projects</code> are two administrative nodes in the graph database that serve as the most upstream nodes. A <code>program</code> must be created first, followed by a <code>project</code>. Any subsequent data submission and data access, along with control of access to data, is done through the project scope.  In some projects only a subset of submitters may have access to create a program or project.</p> <p>Before you create a program and a project or submit any data, you need to grant yourself permissions. First, you will need to grant yourself access to create a program and second, you need to grant yourself access to see the program. You can create the program before or after having access to see it.</p> <p>For this, you will need to edit the <code>Secrets/user.yaml</code> file following the docs shown here.</p> <p>Make sure to update user privileges: </p>Text Only<pre><code>docker exec -it fence-service fence-create sync --arborist http://arborist-service --yaml user.yaml\n</code></pre> <p>To create a program, visit the URL where your Gen3 Commons is hosted and append <code>/_root</code>.  If you are running the Docker Compose setup locally, then this will be <code>localhost/_root</code>. Otherwise, this will be whatever you set the <code>hostname</code> field to in the creds files for the services with <code>/_root</code> added to the end. Here, you can choose to either use form submission or upload a file.  We will go through the process of using form submission here, as it will show you what your file would need to look like if you were using file upload. Choose form submission, search for \"program\" in the drop-down list and then fill in the \"dbgap_accession_number\" and \"name\" fields. As an example, you can use \"123\" as \"dbgap accession number\" and \"Program1\" as \"name\". Click <code>Upload submission json from form</code> and then <code>Submit</code>. If the message is green (\"succeeded:200\"), that indicates success, while a grey message indicates failure. More details can be viewed by clicking on the \"DETAILS\" button. If you don't see the green message, you can review the sheepdog logs for possible errors and check the Sheepdog database (<code>/datadictionary</code>), where programs and projects are stored. If you see your program in the data dictionary, neglect the fact that at this time the green message does not appear and continue to create a project.</p> <p>To create a project, visit the URL where your Gen3 Commons is hosted and append the name of the program you want to create the project under. For example, if you are running the Docker Compose setup locally and would like to create a project under the program \"Program1\", the URL you will visit will be <code>localhost/Program1</code>. You will see the same options to use form submission or upload a file. This time, search for \"project\" in the drop-down list and then fill in the fields. As an example, you can use \"P1\" as \"code\", \"phs1\" as \"dbgap_accession_number\", and \"project1\" as \"name\". If you use different entries, make a note of the dbgap_accession_number for later. Click <code>Upload submission json from form</code> and then <code>Submit</code>. Again, a green message indicates success while a grey message indicates failure, and more details can be viewed by clicking on the \"DETAILS\" button. You can review in the <code>/datadictionary</code> whether the program and project have been correctly stored.</p> <p>After that, you're ready to start submitting data for that project keeping in mind that you must submit from \"top to bottom\" in the data model to make sure each new node points to an existing node in the database. If nodes are submitted out of order, such as submitting a TSV with links to parent records that don't yet exist, the validator will reject the submission on the basis that the dependency is not present with the error message, \"INVALID_LINK\".</p> <p>As an alternative to creating the program and project nodes in the Data Portal, you can instead use the Gen3 Submission SDK, which has a comprehensive set of tools to enable users to script submission of programs and projects.</p> <p>Sample Code for submission of a Program and Project to a data commons: </p>Text Only<pre><code>import gen3\nfrom gen3.submission import Gen3Submission\n\nGen3Submission.create_program(program_json)\nGen3Submission.create_project('test_program', project_json)\n</code></pre>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-structured-data/#specifying-required-links","title":"Specifying Required Links","text":"<p>At least one link is required for every record in a TSV, and sometimes multiple links could be specified. The links are specified in a TSV with the variable header <code>&lt;nodes&gt;.submitter_id</code>, where &lt;nodes&gt; is the back-reference of the upstream node the record is linking to. The value of this link variable is the specific <code>submitter_id</code> of the parent record. TSV or JSON templates that list all the possible link headers can be downloaded from the Data Dictionary Viewer on the data commons' website. Properties that represent these links such as \u201csubjects.submitter_id\u201d or \u201cstudies.submitter_id\u201d are array variables and can take either a single submitter_id or a comma-separated list of <code>submitter_id</code>s in the case that a single record links to multiple records in its parent node.</p> <p>For example, there are four cases in two studies in one <code>project</code>. The <code>study</code> node was made with two study <code>submitter_id</code>s: \"study-01\" and \"study-02\". The \"case.tsv\" file uploaded to describe the study participants enrolled will have a corresponding study.</p> case submitter_id studies.submitter_id 1 case_1 study-01 2 case_2 study-02 3 case_3 study-01 4 case_4 study-01 <p>In this example cases 1, 3, and 4 all belong to \"study-01\", but case 2 belongs to \"study-02\". All the cases have different <code>submitter_id</code>s and these will be used in the subtending node that refers to a specific case.</p> <p>NOTE: The <code>submitter_id</code> needs to be unique not only within one node, but across all nodes in a project. The combination of <code>submitter_id</code> and <code>project_id</code> must be unique.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-structured-data/#specifying-multiple-links","title":"Specifying Multiple Links","text":"<p>Links can be one-to-one, many-to-one, one-to-many, and many-to-many. Since a single study participant can be enrolled in multiple studies, and a single study will have multiple cases enrolled in it, this link is \"many-to-many\". On the other hand, since a single study cannot be linked to multiple projects, but a single project can have many studies linked to it, the study -&gt; project link is \"many-to-one\". Properties that represent links, like \u201csubjects.submitter_id\u201d or \u201cstudies.submitter_id\u201d are array variables and can take either a single submitter_id or a comma-separated list of submitter_ids in the case that a single record links to multiple records in its parent node. Using the example above, the entry in the <code>studies.submitter_id</code> can be \"study-01, study-02\".</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-structured-data/#begin-metadata-tsv-submissions","title":"Begin Metadata TSV Submissions","text":"<p>To submit a TSV in the data portal:</p> <ol> <li> <p>Login to the data portal for the commons.</p> </li> <li> <p>Click on \"Submit Data\" in the top navigation bar.</p> <p></p> </li> <li> <p>Click on \"Submit Data\" beside the project of interest to submit metadata.</p> </li> <li> <p>Click on \"Upload File\".</p> <p></p> </li> <li> <p>Navigate to the TSV from your local directory and click \"open\". The contents of the TSV should appear in the grey box below.</p> </li> <li> <p>Click \"Submit\".</p> </li> </ol> <p>A message should appear that indicates either success (green, \"succeeded: 200\") or failure (grey, \"failed: 400\"). Further details can be reviewed by clicking on \"DETAILS\", which displays the API response in JSON form. Each record/entity that was submitted, gets a true/false value for \"valid\" and lists \"errors\" if it was not valid.</p> <p>For anything other than success, check the other fields for any information on the error with the submission. The most descriptive information will be found in the individual entity transaction logs. Each line in the TSV will have its own output with the following attributes:</p> Text Only<pre><code>JSON\n{\n    \"action\": \"update/create\",\n    \"errors\": [\n        {\n            \"keys\": [\n                \"species (the property name)\"\n            ],\n            \"message\": \"'Homo sapien' is not one of ['Drosophila melanogaster', 'Homo sapiens', 'Mus musculus', 'Mustela putorius furo', 'Rattus rattus', 'Sus scrofa']\",\n            \"type\": \"ERROR\"\n        }\n    ],\n    \"id\": \"1d4e9bb0-515d-4158-b14b-770ab5077d8b (the GUID created for this record)\",\n    \"related_cases\": [],\n    \"type\": \"case (the node name)\",\n    \"unique_keys\": [\n        {\n            \"project_id\": \"training (the project name)\",\n            \"submitter_id\": \"training-case-02 (the record/entity submitter_id)\"\n        }\n    ],\n    \"valid\": false,\n    \"warnings\": []\n}\n</code></pre> <p>The \"action\" above can be used to identify if the node was newly created or updated. Updating a node is submitting to a node with the same <code>submitter_id</code> and overwriting the existing node entries. Other useful information includes the \"id\" for the record. This is the GUID for the record and is unique throughout the entirety of the data commons. The other \"unique_key\" provided is the tuple \"project_id\" and \"submitter_id\", which is to say the \"submitter_id\" combined with the \"project_id\" is a universal identifier for this record.</p> <p>To confirm that a data file is properly registered, enter the GUID of a data file record in the index API endpoint of the data commons: usually \"https://gen3.datacommons.io/index/GUID\", where \"https://gen3.datacommons.io\" is the URL of the Gen3 data portal and GUID is the specific GUID of a registered data file. This should display a JSON response that contains the URL that was registered. If the record was not registered successfully, it is likely an error message will occur. An error that says \"access denied\" might also occur if the user is not logged in or the session has timed out. Note, that for these user guides, https://gen3.datacommons.io is an example URL and can be replaced with the URL from other data commons powered by Gen3.</p> <p>Note: Gen3 users can also submit metadata using the Gen3 SDK, which is a Python library containing functions for sending standard requests to the Gen3 APIs. For example, the function <code>submit_file</code> from the Gen3Submission class will submit data in a spreadsheet file containing multiple records in rows to a Gen3 Commons. The code is open-source and available on GitHub along with documentation for using it. Furthermore, this section describes steps on how to get started.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-structured-data/#review-submitted-structured-data","title":"Review submitted structured data","text":"<p>To review the content of submitted data, you can start from the directions above and instead of selecting \"Upload\" in Step 4, you can review the graph below.  You can select particular nodes to view individual records where you have the option to delete, view, or download.</p> <p>Note: Users who are not authorized to submit data may see a \u201cBrowse Data\u201d button instead of \u201cSubmit Data\u201d.  These users will still have access to view the graph and individual nodes, but not to upload or delete.</p> <p>The number you see underneath the node name, for example \u2018subject\u2019, reflects the number of records in that node of the project. The \u201cToggle View\u201d button is used to show or hide nodes in the data model that the project has no records for.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-structured-data/#tsv-formatting-checklist","title":"TSV Formatting Checklist","text":"<ol> <li>Specify the node <code>type</code> for every row. This is the name of the node (or <code>node_id</code>), and it must be exactly the same for every row.</li> <li>Specify the <code>submitter_id</code> of every record by entering a unique text identifier in each row. Make sure you don't use the same value in more than one row of your TSV because every record in a project must have a unique <code>submitter_id</code>!</li> <li>Specify the links to the parent node(s) for each record. Note: parent records must exist before submitting child records! You can specify either the links with either the <code>parents.submitter_id</code> or the <code>parents.id</code></li> <li>Fill in all required properties. Every row in the TSV must have a value for all required properties. Optional properties can be filled in for only some rows or the column can be left out entirely.</li> </ol> <p>Templates can be downloaded from the data dictionary page of your commons.  See the Gen3 Data Hub as an example and click on the TSV option for each node.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-structured-data/#troubleshooting-and-finishing-the-submission","title":"Troubleshooting and Finishing the Submission","text":"<p>If the submission throws errors or claims the submission to be invalid, it will be the submitter's task to fix the submission. The best first step is to go through the outputs from the individual entities, as seen in the previous section. The error fields will give a rough description of what failed the validation check. The most common problems are simple issues such as spelling errors, mislabeled properties, or missing required fields.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/","title":"Unstructured Data","text":"","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#unstructured-data-data-files","title":"Unstructured Data (Data Files)","text":"<p>Unstructured data are simply data files that have do not necessarily conform to any particular schema or format.  Some data files may be consistently structured (e.g .bam or .png), but Gen3 treats these simply as files and does not check whether they conform to a particular internal format or not.  To make data available to end users you must first upload the files and associate them with the appropriate node in the data dictionary.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#standard-submission-process","title":"Standard Submission Process","text":"","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#1-prepare-project-in-submission-portal","title":"1. Prepare Project in Submission Portal","text":"<p>In order to upload data files you must at minimum have a <code>program</code>, <code>project</code>, and at least one record in the <code>core_metadata_collection</code> node or other data containing node. To review how to submit the program and project nodes see here.</p> <p>This documentation will utilize the core_metadata_collection node but other nodes can be used depending on your unique data model. If your project already has at least one record in a node of this type, you can skip to step 2.</p> <p>Do the following to create your first <code>core_metadata_collection</code> record:</p> <ol> <li>Go to your data commons' submission portal website</li> <li>Click on 'Submit Data'</li> <li>Find your project in the list of Projects and click 'Submit Data'</li> <li> <p>Click 'Use Form Submission' and choose <code>core_metadata_collection</code> from the dropdown list (or edit and upload this TSV by clicking 'Upload File' then 'Submit')</p> <p></p> <p></p> </li> <li> <p>Fill in the required information (see note below)</p> </li> <li>Click 'Upload submission json from form' and then 'Submit'</li> <li>Make note of the <code>submitter_id</code> of your <code>core_metadata_collection</code> record for step 3 below</li> </ol> <p>Note: Minimally, <code>submitter_id</code> and <code>projects.code</code> are required properties. The project <code>code</code> is the name of your project without the \"program-\" prefix. For example, if your project URL is https://gen3.datacommons.io/example-training, your project's <code>code</code> would be 'training', the <code>program</code> would be 'example', and your <code>project_id</code> would be the combination: 'example-training'.</p> <p>You should have received the message:</p> Text Only<pre><code>succeeded: 200\nSuccessfully created entities: 1 of core_metadata_collection\n</code></pre> <p>If you received any other message, then check the 'Details' to help determine the error.</p> <p>To view the records in the <code>core_metadata_collection</code> node in your project, you can go to: https://gen3.datacommons.io/example-training/search?node_type=core_metadata_collection (replacing the <code>gen3.datacommons.io</code> with your commons base URL and <code>example-training</code> with an actual project_id).</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#2-upload-data-files-to-object-storage","title":"2. Upload Data Files to Object Storage","text":"<p>Adding files to your new Gen3 project can be done using one of two methods. The gen3-client tool (shown in steps below) offers users an easy way to upload files to Amazon S3 buckets while simultaneously indexing the files and assigning them each a unique GUID or object_id. Alternatively, if you are comfortable scripting and require something other than the default AWS bucket used by the gen3-client or your data files are already uploaded to their storage location in the cloud, we offer an option for indexing data already found in the cloud. This method offers several other benefits including the possibility of using multiple cloud resources and submitting multiple batches of data set files at once. The following documentation will focus on using the gen3-client to upload data files, including spreadsheets, sequencing data (BAM, FASTQ), assay results, images, PDFs, etc., to Amazon S3 cloud storage.</p> <ol> <li>Download the latest compiled binary for your operating system.</li> <li> <p>Configure a profile with credentials downloaded from your Profile:</p> <p></p>Text Only<pre><code>./gen3-client configure --profile=&lt;profile_name&gt; --cred=&lt;credentials.json&gt; --apiendpoint=&lt;api_endpoint_url&gt;\n</code></pre> 3. Upload Files: single data file, a directory of files, or matching files: Text Only<pre><code>./gen3-client upload --profile=&lt;profile_name&gt; --upload-path=~/files/example.txt\n\n./gen3-client upload --profile=&lt;profile_name&gt; --upload-path=~/files/\n\n./gen3-client upload --profile=&lt;profile_name&gt; --upload-path=~/files/*.txt\n</code></pre> </li> </ol> <p>For detailed instructions on configuring and using the gen3-client, visit the Gen3 client documentation.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#3-map-uploaded-files-to-a-data-file-node","title":"3. Map Uploaded Files to a Data File Node","text":"<p>Once data files are successfully uploaded, the files must be mapped to the appropriate node in the data model before they're accessible to authorized users.</p> <ol> <li> <p>Go to your data commons submission portal website.</p> </li> <li> <p>Click 'Submit Data'.</p> <p></p> </li> <li> <p>Click 'Map My Files' button.</p> <p></p> </li> <li> <p>Select the files to map using the checkboxes and click 'Map Files' button.</p> <p></p> </li> <li> <p>Select the project and node that the files belong to.</p> <p></p> </li> <li> <p>Fill in the values of any required properties and click 'Submit' button.</p> <p></p> </li> </ol> <p>Note: The required property 'Type' in step 6 is the node's name (the 'type' of node) and should be the same as the value selected from the node dropdown list in step 5.</p> <p>You should receive the message \"# files mapped successfully!\" upon success.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#indexing-files-already-found-in-the-cloud","title":"Indexing files already found in the cloud","text":"<p>This method is appropriate if you are comfortable scripting and require something other than the default AWS bucket used by the gen3-client or your data files are already uploaded to their storage location in the cloud. This method offers several other benefits including the possibility of using multiple cloud resources and submitting multiple batches of data set files at once. The following documentation will focus on using the gen3-client to upload data files, including spreadsheets, sequencing data (BAM, FASTQ), assay results, images, PDFs, etc., to Amazon S3 cloud storage. If these criteria do not apply then you should use the Standard upload process instead.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#1-prepare-project-with-the-gen3-sdk-tools","title":"1. Prepare Project with the Gen3 SDK tools","text":"<p>Though not strictly required to be done as a first step, a Gen3 project must be present in the Sheepdog microservice to associate data files to before file indexing can take place. To achieve this, the Gen3 Submission SDK has a comprehensive set of tools to enable users to script submission of programs and projects.  Alternatively, the GUI submission platform can be used to create a project.</p> <p>Sample Code for submission of a Program and Project to a data commons: </p>Text Only<pre><code>import gen3\nfrom gen3.submission import Gen3Submission\n\nGen3Submission.create_program(program_json)\nGen3Submission.create_project('test_program', project_json)\n</code></pre>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#2-selection-and-granting-gen3-secure-access-to-cloud-resources","title":"2. Selection and Granting Gen3 Secure Access to Cloud Resources","text":"<p>As Gen3 is considered \"cloud agnostic\", any or even multiple cloud resources can be configured to contain data for controlled end-user access.  If your data is already located in the cloud, please see the following section for considerations in the structure and permissions settings.</p> <p>End-user access to cloud resources is enabled by signed-urls with authorization checks within Gen3 to ensure valid and secure access.  Policies within the respective cloud resources should be configured in the Gen3 Fence Microservice to allow the; Gen3 Auth Service Bot - AWS, Gen3 Auth Service Bot - Azure, or Gen3 Auth Service Bot - Google to have access for the end user.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#aws-s3-example-bucket-policy-for-read-access","title":"AWS S3 example bucket policy for READ access:","text":"Text Only<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowListLocation\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": [\n                    \"arn:aws:iam::895962626746:user/fence_bot\"\n                ]\n            },\n            \"Action\": [\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": \"arn:aws:s3:::&lt;YOURBUCKETHERE&gt;\"\n        },\n        {\n            \"Sid\": \"AllowGetObject\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::895962626746:user/fence_bot\"\n            },\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::&lt;YOURBUCKETHERE&gt;/*\"\n        }\n    ]\n}\n</code></pre> <p>The location for the example AWS configuration posted above is available here.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#3-upload-files-to-object-storage-with-cloud-resource-command-line-interface","title":"3. Upload files to Object Storage with Cloud Resource Command Line Interface","text":"<p>Data can be uploaded to a single or separate cloud resources as long as requirements for access and authorization are met.  In order to support the many advantages of using Gen3\u2019s standard tooling for CLI-DFS, data needs to first be organized and copied to cloud buckets following the guidelines detailed below.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#allocating-data-in-buckets-based-on-user-access","title":"Allocating Data in Buckets Based on User Access","text":"<p>Gen3 has the capability to grant access granularity at the bucket level designation only.  In this way data a particular bucket should only be associated with a single user access.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#bucket-allocation-example","title":"Bucket Allocation Example:","text":"<p>A user\u2019s authorization may look something like:</p> <p>A user has read access to phs001416.c1, phs001416.c2, phs000974.c2</p> <p>The data in buckets could be separated by phsid+consent code combinations (as this is the smallest granularity of data access required).</p> <p>The following bucket structure supports the ingestion of dbGaP\u2019s MESA and FHS projects (from TOPMed). Each project has 2 distinct consent groups, and the data is mirrored on both AWS and Google buckets.</p> <p>TOPMed-MESA (phs001416.c1 and .c2) TOPMed-FHS (phs000974.c1 and .c2)</p> Project AWS buckets Google buckets MESA (consent group 1) s3://nih-nhlbi-topmed-released-phs001416-c1 gs://nih-nhlbi-topmed-released-phs001416-c1 MESA (consent group 2) s3://nih-nhlbi-topmed-released-phs001416-c2 gs://nih-nhlbi-topmed-released-phs001416-c2 FHS (consent group 1) s3://nih-nhlbi-topmed-released-phs000974-c1 gs://nih-nhlbi-topmed-released-phs000974-c1 FHS (consent group 2) s3://nih-nhlbi-topmed-released-phs000974-c2 gs://nih-nhlbi-topmed-released-phs000974-c2 <p>With a setup similar to this, Gen3 is able to support signed URLs and fully configured end-user access.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#bucket-population","title":"Bucket Population","text":"<p>Once a data allocation scheme is determined, data can be uploaded accordingly to cloud buckets.  It should be noted that while Amazon AWS and Google are the most supported cloud providers, Gen3 is cloud agnostic.  Any method and hierarchy structure can be used for upload as long as a the same parent directory is maintained with end user access.</p> <p>Regardless of the cloud platform, the CLI-DFS workflow requires file data gathered from its cloud location.  Information such as file name, location, size, and md5sum are usually available from cloud platforms.  Documentation for AWS, Google and Microsoft Azure should provide guidance to acquiring this information.</p> <p>Note: The recommended (detailed here) way for Gen3 to provide controlled access to data is via Signed URLs. Signed URLs are the only fully cloud-agnostic method supported by Gen3 and additionally are supported by all major cloud resource providers. They also allow for short-lived, temporary access to the data for reduced risk.  Lastly, utilizing signed URLs places very few restrictions on the organization of data within could bucket(s).</p> <p>The files relevant to a Gen3 CLI-DFS Workflow submission:</p> <ul> <li> <p>Bucket mapping file: File that maps authorization designations to parent level bucket locations.</p> </li> <li> <p>Bucket manifest file: Created for each submission and contains file level information (i.e. name, size, md5sum).</p> </li> <li> <p>Indexing manifest: Created for each submission and submits both authorization and file level information into the Indexd microservice.</p> </li> </ul> <p>The creation and submission of these files is covered below.</p> <p>Note: The recommended (and simplest) way for Gen3 to provide controlled access to data is via Signed URLs. Signed URLs are the only fully cloud-agnostic method supported by Gen3 and additionally are supported by all major cloud resource providers. They also allow for short-lived, temporary access to the data for reduced risk.  Lastly, utilizing signed URLs places very few restrictions on the organization of data within could bucket(s).</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#4-create-bucket-mapping-and-manifest-files","title":"4. Create Bucket Mapping and Manifest Files","text":"<p>The below is the Gen3 recommended indexing file schema. While possible to utilize other configurations, they likely require significantly more administrative effort to maintain correct permissions in the cloud platform(s).</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#bucket-mapping-file","title":"Bucket Mapping File","text":"<p>A Bucket mapping file is used maintain clear links between designated project authorization and parent level bucket locations. It should be maintained for the entire commons and appended when new datasets are ingested. The bucket mapping file should minimally contain the following fields and be presented in a tab delimited format.</p> <p>The below example has 4 different authorizations for 8 bucket locations</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#example-bucket-mapping-file","title":"Example Bucket Mapping File","text":"bucket authz s3://nih-nhlbi-topmed-phs001416-c1 phs001416.c1 gs://nih-nhlbi-topmed-phs001416-c1 phs001416.c1 s3://nih-nhlbi-topmed-phs001416-c2 phs001416.c2 gs://nih-nhlbi-topmed-phs001416-c2 phs001416.c2 s3://nih-nhlbi-topmed-phs000974-c1 phs000974.c1 gs://nih-nhlbi-topmed-phs000974-c1 phs000974.c1 s3://nih-nhlbi-topmed-phs000974-c2 phs000974.c2 gs://nih-nhlbi-topmed-phs000974-c2 phs000974.c2 <p>In the situation where Gen3 must support cloud-specific data access methods, Gen3 also requires the authz or acl column which contain the granular access control which would represent access to the entire bucket).</p> <p>The authz column coordinates with the user permissions set in the Gen3 microservices Fence and Arborist.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#bucket-manifest-file","title":"Bucket Manifest File","text":"<p>The bucket manifest file should contain individual file level metadata for a single batch of ingestion files. This means there will be several bucket manifest files per data commons. It is recommended that they are represented in a tab separated variable format and in each, a row should minimally contain the following information:</p> <ul> <li>File name</li> <li>File size</li> <li>File hash via md5sum</li> <li>Exact file url in the bucket location</li> </ul> <p>If files are mirrored between cloud locations, bucket urls can be appended together with a whitespace delimiter.</p> <p>In the below example of a Bucket manifest file, please note the mirrored file bucket locations in S3 and GCP:</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#example-bucket-manifest-file","title":"Example Bucket Manifest File","text":"File_name File_size md5sum bucket_urls examplefile.txt 123456 sample_md5 s3://nih-phs001416-c1/exfile.txt gs://nih-phs001416-c1/exfile.txt otherexamplefile.txt 123456 different_md5 s3://nih-nhlbi-topmed-released-phs001416-c1/otherexamplefile.txt gs://nih-nhlbi-topmed-released-phs001416-c1/otherexamplefile.txt examplefile.txt 123456 sample_md5 s3://nih-nhlbi-topmed-released-phs001416-c2/examplefile.txt gs://nih-nhlbi-topmed-released-phs001416-c2/examplefile.txt otherexamplefile.txt 123456 different_md5 s3://nih-nhlbi-topmed-released-phs001416-c2/otherexamplefile.txt gs://nih-nhlbi-topmed-released-phs001416-c2/otherexamplefile.txt","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#5-create-indexing-manifest-file","title":"5. Create Indexing Manifest File","text":"<p>An Indexing Manifest File is submitted to the Indexd microservice and is a combination of both the Bucket Mapping and Manifest file information.</p> <p>While the two preceding files are not strictly necessary for maintenance and operation of a Gen3 data commons, they are recommended for ease of maintenance. For instance, if multiple authorization designations are required within a single bucket location, administrators will need to set them individually directly in the cloud platform as Gen3 has no capability to interact with cloud resource permissions in that manner.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#indexd-microservice-overview","title":"Indexd Microservice Overview","text":"<p>The Indexd microservice is used by Gen3 to maintain an index of all files in a data commons and serves as the data source by several other microservices to build various features of Gen3 data commons. A central part of what enables Gen3's Indexd is the integration of a Globally Unique Identifier (GUID) to each element added to the microservice.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#globally-unique-identifier-guid","title":"Globally Unique Identifier (GUID)","text":"<p>GUIDs are primarily used to track and provide the current location of data and is designed to persist even as data is moved or copied. Information regarding the concept of GUIDs, GUID generation and look up of particular GUIDs can be found at dataguids.org.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#indexing-manifest-components-and-structure","title":"Indexing Manifest Components and Structure","text":"<p>By default GUIDs will be added to rows that lack an entry for that field when an indexing manifest is submitted to Indexd. GUIDs that are minted in this way are both available by querying Indexd or by referencing the submission output file that is generated.</p> <p>As the Indexing Manifest is the file that is submitted to the Indexd microservice, it must be submitted in a tab separated variable file (.tsv) and contain the following fields:</p> <ul> <li>Globally Unique Identifier (GUID) - Either generated by indexd microservice at the time of submission or provided by the user prior to submission</li> <li>File name</li> <li>File size</li> <li>File hash via md5sum</li> <li>Exact file url in the bucket location</li> <li>authz or acl authorization designation</li> </ul> <p>Users may notice that with the exception of GUIDs, this file is a combination of the Bucket Mapping and Manifest files.  If either AWS or Google cloud resources are used, Gen3 offers tools to produce bucket manifest files available at the following links:</p> <ul> <li>AWS S3 Bucket Manifest Generation</li> <li> <p>Google Bucket Manifest Generation</p> <p>Note: Bucket manifest generation scripts require using Gen3's full deployment code and, depending on the amount of data, calculating checksums for files can be costly and take time.</p> </li> </ul> <p>The below is an example of a Indexing Manifest File:</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#example-indexing-manifest-file","title":"Example Indexing Manifest File","text":"guid File_name File_size md5sum bucket_urls auth dg.4503/02... ...7103bbe examplefile.txt 34141 c79... ...dbd s3://nih-phs001416-c1/exfile.txt gs://nih-phs001416-c1/exfile.txt [phs001416.c1] dg.4503/00... ...0211dfg otherexamplefile.txt 562256 65a... ...bca s3://nih-nhlbi-topmed-released-phs001416-c1/otherexamplefile.txt gs://nih-nhlbi-topmed-released-phs001416-c1/otherexamplefile.txt [phs001416.c1] dg.4503/00... ...7103bbe examplefile.txt 36564 dca... ...774 s3://nih-nhlbi-topmed-released-phs001416-c2/examplefile.txt gs://nih-nhlbi-topmed-released-phs001416-c2/examplefile.txt [phs001416.c2] dg.4503/01... ...0410nnd otherexamplefile.txt 2675 742... ...f1b s3://nih-nhlbi-topmed-released-phs001416-c2/otherexamplefile.txt gs://nih-nhlbi-topmed-released-phs001416-c2/otherexamplefile.txt [phs001416.c2]","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#6-submit-file-indexing-manifest-to-indexd","title":"6. Submit file Indexing Manifest to Indexd","text":"<p>Once created, Gen3 offers an Indexing SDK toolkit to build, validate and map all files into a Gen3 datacommons. The SDK functions reconcile and add data to the indexd microservice.</p> <p>Sample code for validation and submission of a constructed indexing manifest file to indexd. </p>Text Only<pre><code>import gen3\nfrom gen3.tools.indexing import validate_manifest_format\nfrom gen3.tools.indexing.index_manifest import index_object_manifest\n\nfile_path = &lt;.tsv_indexd_file_addition&gt;\n\ngen3.tools.indexing.validate_manifest_format.is_valid_manifest_format(file_path)\n\ngen3.tools.indexing.index_manifest.index_object_manifest(commons_url=commons_url,\n                                                                       manifest_file=file_path,\n                                                                       thread_num=8,\n                                                                       auth=authentication_object,\n                                                                       output_filename=index_manifest[:-4] + '_output.tsv')\n</code></pre> <p>Please refer to the authentication sdk for set up of the authentication_object used above</p> <p>Note: Users in the Gen3-Community have published a repo that index large pre-existing s3 buckets (disclaimer: CTDS is not responsible for the content and opinions on the third-party repos).</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#7-map-files-to-a-data-node-with-the-gen3-sdk","title":"7. Map files to a Data Node with the Gen3 SDK","text":"<p>Once indexing is complete, Gen3 offers a Submission SDK toolkit to map indexed data files to nodes designated to contain data in the data dictionary via the Sheepdog microservice.  Unless single data files are being ingested, the SDK submission toolkit generally requires a tab separated variable file, and specific nodes requirements for each data file type can be specified in the data dictionary. After mapping in Sheepdog is complete the file metadata will be mapped from the program and project administrative nodes (previously created) to its respective data containing nodes. The mapping in sheepdog is the basis for other search and query services either natively in sheepdog or after other extraction, transformation and load (ETL) services have been performed.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#gen3-client-instructions-for-uploading-data","title":"Gen3 client instructions for uploading data","text":"<p>The gen3-client provides an easy-to-use, command-line interface for uploading and downloading data files to and from a Gen3 data commons from the terminal or command prompt, respectively.  Only information related to uploading is included below.  For instruction related to download please review the Downloading Files Using the Gen3-client section.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#1-installation-instructions","title":"1. Installation Instructions","text":"<p>Installation instructions are covered in the Downloading Files Using the Gen3-client section.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#2-configure-a-profile-with-credentials","title":"2. Configure a Profile with Credentials","text":"<p>Profile configuration instructions are covered in the Downloading Files Using the Gen3-client section.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#3-upload-data-files-using-the-gen3-client","title":"3.  Upload Data Files using the Gen3 Client","text":"<p>For the typical data contributor, the <code>gen3-client upload</code> command should be used to upload data files to a Gen3 Data Commons. The commands <code>upload-single</code> and <code>upload-multiple</code> are used only in special cases, for example, when a file or collection of files are uploaded to specific GUIDs after generating structured data records for the files. These two commands are described in further detail in sections 3 and 4 below.</p> <p>When data files are uploaded to a Gen3 data common's object storage, they are assigned a unique, 128-bit ID called a GUID, which stands for \"globally unique identifier\". GUIDs are generated by the system software, not provided by users, and they are stored in the property <code>object_id</code> of a data_file's structured data.</p> <p>When using the <code>gen3-client upload</code> command, a random, unique GUID will be generated and assigned to each data file that has been submitted, and an entry in the indexd database will be created for that file, which associates the storage location of the file with the file's object_id (\"did\" in the indexd record, see below for more details).</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#options-and-user-input-flags","title":"Options and User Input Flags","text":"<p>The following flags can be used with the <code>gen3-client upload</code> command:</p> Flag name Required? Default value Explanation Sample usage profile Yes N/A The profile name that user wishes to use from the config file. --profile=demo upload-path Yes N/A The directory or file in which contains file(s) to be uploaded. --upload-path=../data_folder/ batch No false If set to `true`, gen3-client will upload multiple files simultaneously. The maximum number of file can be uploaded at a same time is specified by the `numparallel` option --batch=true numparallel No 3 Number of uploads to run in parallel. Must be used in together with the `batch` option. --numparallel=5 include-subdirname No false Include subdirectory names in file name. --include-subdirname=true force-multipart No false Force to use multipart upload if possible. --force-multipart=true <p>Example of a single file upload: </p>Text Only<pre><code>~&gt; gen3-client upload --profile=demo --upload-path=test.txt\n2019/11/19 12:45:41 Finish parsing all file paths for \"/Users/demo/Documents/test.txt\"\n\nThe following file(s) has been found in path \"/Users/demo/Documents/test.txt\" and will be uploaded:\n    /Users/demo/Documents/test.txt\n\n2019/11/19 12:45:41 Uploading data ...\ntest.txt  25 B / 25 B [=======================================================================================================================================] 100.00% 0s\n2019/11/19 12:45:41 Successfully uploaded file \"/Users/demo/Documents/test.txt\" to GUID 1a82043e-02ec-4974-a803-7c0fd33ecfd7.\n2019/11/19 12:45:41 Local succeeded log file updated\n\n\nSubmission Results\nFinished with 0 retries | 1\nFinished with 1 retry   | 0\nFinished with 2 retries | 0\nFinished with 3 retries | 0\nFinished with 4 retries | 0\nFinished with 5 retries | 0\nFailed                  | 0\nTOTAL                   | 1\n</code></pre> <p>Example of uploading all files within an folder: </p>Text Only<pre><code>~/Documents&gt; gen3-client upload --profile=demo --upload-path=test_dir\n2019/11/19 13:12:47 Finish parsing all file paths for \"/Users/demo/Documents/test_dir\"\n\nThe following file(s) has been found in path \"/Users/demo/Documents/test_dir\" and will be uploaded:\n    /Users/demo/Documents/test_dir/test.doc\n    /Users/demo/Documents/test_dir/test.jpg\n    /Users/demo/Documents/test_dir/test_1.txt\n    /Users/demo/Documents/test_dir/test_2.txt\n\n2019/11/19 13:12:48 Uploading data ...\ntest.doc  46 B / 46 B [=================================================================================================================================================================] 100.00% 0s\n2019/11/19 13:12:48 Successfully uploaded file \"/Users/demo/Documents/test_dir/test.doc\" to GUID 7d1b41d9-002e-46d0-8934-6606d246ca30.\n2019/11/19 13:12:48 Local succeeded log file updated\n2019/11/19 13:12:48 Uploading data ...\ntest.jpg  50 B / 50 B [=================================================================================================================================================================] 100.00% 0s\n2019/11/19 13:12:48 Successfully uploaded file \"/Users/demo/Documents/test_dir/test.jpg\" to GUID 59059e8d-29bf-4f8b-b9a4-2cd0ef2420f6.\n2019/11/19 13:12:48 Local succeeded log file updated\n2019/11/19 13:12:48 Uploading data ...\ntest_1.txt  30 B / 30 B [===============================================================================================================================================================] 100.00% 0s\n2019/11/19 13:12:48 Successfully uploaded file \"/Users/demo/Documents/test_dir/test_1.txt\" to GUID 6f6686f1-45f2-4e8d-a997-a669b9419fd3.\n2019/11/19 13:12:48 Local succeeded log file updated\n2019/11/19 13:12:48 Uploading data ...\ntest_2.txt  27 B / 27 B [===============================================================================================================================================================] 100.00% 0s\n2019/11/19 13:12:49 Successfully uploaded file \"/Users/demo/Documents/test_dir/test_2.txt\" to GUID d8ec2f5a-0990-495f-8192-ca2f037d6236.\n2019/11/19 13:12:49 Local succeeded log file updated\n\n\nSubmission Results\nFinished with 0 retries | 4\nFinished with 1 retry   | 0\nFinished with 2 retries | 0\nFinished with 3 retries | 0\nFinished with 4 retries | 0\nFinished with 5 retries | 0\nFailed                  | 0\nTOTAL                   | 4\n</code></pre> <p>Example of upload using wildcard. Here we specify <code>*txt</code> in the <code>--upload-path</code> to get only files with a \"txt\" extension in the \"test_dir\" directory: </p>Text Only<pre><code>~/Documents&gt; gen3-client upload --profile=demo --upload-path=test_dir/*txt\n2019/11/19 15:49:07 Created folder \"/Users/demo/.gen3/logs/\"\n2019/11/19 15:49:07 Finish parsing all file paths for \"/Users/demo/Documents/test_dir/*txt\"\n\nThe following file(s) has been found in path \"/Users/demo/Documents/test_dir/*txt\" and will be uploaded:\n    /Users/demo/Documents/test_dir/test_1.txt\n    /Users/demo/Documents/test_dir/test_2.txt\n\n2019/11/19 15:49:07 Uploading data ...\ntest_1.txt  30 B / 30 B [===============================================================================================================================================================] 100.00% 0s\n2019/11/19 15:49:07 Successfully uploaded file \"/Users/demo/Documents/test_dir/test_1.txt\" to GUID 956890a9-b8a7-4abd-b8f7-dd0020aaf562.\n2019/11/19 15:49:07 Local succeeded log file updated\n2019/11/19 15:49:07 Uploading data ...\ntest_2.txt  27 B / 27 B [===============================================================================================================================================================] 100.00% 0s\n2019/11/19 15:49:07 Successfully uploaded file \"/Users/demo/Documents/test_dir/test_2.txt\" to GUID 6cf194f1-c68e-4976-8ca4-a0ce9701a9f3.\n2019/11/19 15:49:07 Local succeeded log file updated\n\n\nSubmission Results\nFinished with 0 retries | 2\nFinished with 1 retry   | 0\nFinished with 2 retries | 0\nFinished with 3 retries | 0\nFinished with 4 retries | 0\nFinished with 5 retries | 0\nFailed                  | 0\nTOTAL                   | 2\n</code></pre> <p>Example using two wildcards in one path. Here we add <code>test_*/</code> to the <code>--upload-path</code> to upload files in more than one directory, and then we add <code>*.jpg</code> to add only the files from those directories with a \".jpg\" extension: </p>Text Only<pre><code>~/Documents&gt; gen3-client upload --profile=demo --upload-path=./test_*/*.jpg\n2019/11/19 15:53:12 Finish parsing all file paths for \"/Users/demo/Documents/test_*/*.jpg\"\n\nThe following file(s) has been found in path \"/Users/demo/Documents/test_*/*.jpg\" and will be uploaded:\n    /Users/demo/Documents/test_dir/test.jpg\n    /Users/demo/Documents/test_dir_2/test_2.jpg\n\n2019/11/19 15:53:12 Uploading data ...\ntest.jpg  50 B / 50 B [=================================================================================================================================================================] 100.00% 0s\n2019/11/19 15:53:13 Successfully uploaded file \"/Users/demo/Documents/test_dir/test.jpg\" to GUID 9bd009b6-e518-4fe5-9056-2b5cba163ca3.\n2019/11/19 15:53:13 Local succeeded log file updated\n2019/11/19 15:53:13 Uploading data ...\ntest_2.jpg  50 B / 50 B [===============================================================================================================================================================] 100.00% 0s\n2019/11/19 15:53:13 Successfully uploaded file \"/Users/demo/Documents/test_dir_2/test_2.jpg\" to GUID 3d275025-8b7b-4f84-9165-72a8a174d642.\n2019/11/19 15:53:13 Local succeeded log file updated\n\n\nSubmission Results\nFinished with 0 retries | 2\nFinished with 1 retry   | 0\nFinished with 2 retries | 0\nFinished with 3 retries | 0\nFinished with 4 retries | 0\nFinished with 5 retries | 0\nFailed                  | 0\nTOTAL                   | 2\n</code></pre>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#local-submission-history","title":"Local Submission History","text":"<p>The application will keep track of which local files have already been submitted to avoid potential duplication in submissions. This information is kept in a .JSON file in the \"logs\" directory under the same user folder as where the <code>config</code> file lives, for example:</p> Text Only<pre><code>Mac/Linux: /Users/demo/.gen3/logs/&lt;your_config_name&gt;_succeeded_log.json\nWindows: C:\\Users\\demo\\.gen3\\logs\\&lt;your_config_name&gt;_succeeded_log.json\n</code></pre> <p>Each object in the succeeded log file is a key/value pair of the full path of a file and the GUID it is associated with.</p> <p>Example of a succeeded log JSON File:</p> Text Only<pre><code>{\n \"/Users/demo/test.gif\":\"65f5d77c-1b2a-4f41-a2c9-9daed5a59f14\"\n}\n</code></pre> <p>When you run a <code>gen3-client upload</code> command, the client will check the succeeded_log.json log file for the files found in the provided <code>--upload-path</code>. If a file in the <code>--upload-path</code> is found in the succeeded log file, it will be skipped. For example: </p>Text Only<pre><code>~/Documents&gt; gen3-client upload --profile=demo --upload-path=test.txt\n2019/11/19 16:00:42 Finish parsing all file paths for \"/Users/demo/Documents/test.txt\"\n\nThe following file(s) has been found in path \"/Users/demo/Documents/test.txt\" and will be uploaded:\n    /Users/demo/Documents/test.txt\n\n2019/11/19 16:00:42 File \"/Users/demo/Documents/test.txt\" has been found in local submission history and has been skipped for preventing duplicated submissions.\n\n\nSubmission Results\nFinished with 0 retries | 0\nFinished with 1 retry   | 0\nFinished with 2 retries | 0\nFinished with 3 retries | 0\nFinished with 4 retries | 0\nFinished with 5 retries | 0\nFailed                  | 0\nTOTAL                   | 0\n</code></pre> <p>In the rare case that you need to upload the same file again, the success log file will need to be moved, modified, renamed, or deleted. Alternatively, the file itself can be moved or renamed, as the information stored in the succeeded_log.json is the file's full path.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#indexd-records","title":"Indexd records","text":"<p>When files are successfully uploaded by the gen3-client, the software service indexd creates a record for that file in the file index database, which can be accessed at the /index endpoint. For example, if the file's GUID is <code>dg.OADC/59097e14-2c04-4cc2-96f2-9d81f1da2ecb</code>, enter the following URL in a browser or request it via the API to view its indexd record: https://gen3.datacommons.io/index/dg.OADC/59097e14-2c04-4cc2-96f2-9d81f1da2ecb.</p> Text Only<pre><code>{\n  \"acl\": [\n    \"OpenAccess\",\n    \"CCLE\"\n  ],\n  \"authz\": [\n    \"/programs/OpenAccess/projects/CCLE\"\n  ],\n  \"baseid\": \"3f9e1af8-31a8-48b0-b4ea-e04a938e9ca1\",\n  \"content_created_date\": null,\n  \"content_updated_date\": null,\n  \"created_date\": \"2022-07-11T16:47:20.989103\",\n  \"description\": null,\n  \"did\": \"dg.OADC/59097e14-2c04-4cc2-96f2-9d81f1da2ecb\",\n  \"file_name\": \"CCLE_NP24.2009_Drug_data_2015.02.24.csv\",\n  \"form\": null,\n  \"hashes\": {\n    \"crc\": \"cc06bdf5\",\n    \"md5\": \"8358d8a5c0d96ef5a0982178c885e99e\",\n    \"sha1\": \"5b3a66e288faf7386edf939d98f215a928f9c202\",\n    \"sha256\": \"9dce1a7d4c27627bcb3b5c2e6d14873955aac2906e5d44ad344c265a491404e5\",\n    \"sha512\": \"18fbe8ee00311129b22b729107cbfd9a63ef18e3c22366d3c7909996b0c3d3f1b19c1606fffa0d33f336f4084ead3512cd599537979aa281b3c3ad208cd8de27\"\n  },\n  \"metadata\": {\n\n  },\n  \"rev\": \"fd770f06\",\n  \"size\": 2393993,\n  \"updated_date\": \"2022-07-11T16:54:41.813024\",\n  \"uploader\": null,\n  \"urls\": [\n    \"s3://oadc-data-bucket/dg.OADC/59097e14-2c04-4cc2-96f2-9d81f1da2ecb/CCLE_NP24.2009_Drug_data_2015.02.24.csv\"\n  ],\n  \"urls_metadata\": {\n    \"s3://oadc-data-bucket/dg.OADC/59097e14-2c04-4cc2-96f2-9d81f1da2ecb/CCLE_NP24.2009_Drug_data_2015.02.24.csv\": {\n\n    }\n  },\n  \"version\": null\n}\n</code></pre>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#mapping-uploaded-files","title":"Mapping uploaded files","text":"<p>Files that have been successfully uploaded now have a GUID associated with them, and there is also an associated record in the indexd database. However, in order for the files to show up in the data portal, the files have to be registered in the PostgreSQL database. In other words, indexd records exist for the files, but sheepdog records (that is, structured metadata in the graph model) don't exist yet. Thus, the files aren't yet associated with any particular program, project, or node. To create the structured data records for the files via the sheepdog service, the Data Portal offers a \"Map My Files\" UI, which can be reviewed here.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#removing-unwanted-uploaded-files","title":"Removing unwanted uploaded files","text":"<p>Before the files are mapped to a project's node in the data model, the files can be deleted both from indexd and from the cloud location by sending a delete request to the fence endpoint <code>/user/data/</code>. For example, to delete the file we checked in the index above, we'd send a delete API request to this URL: https://gen3.datacommons.io/user/data/dg.OADC/59097e14-2c04-4cc2-96f2-9d81f1da2ecb</p> <p>For example, running this script will delete all the user's unmapped files from indexd and from the storage location using the fence endpoint: </p>Text Only<pre><code>~&gt; python delete_uploaded_files.py -a https://gen3.datacommons.io -u user@gen3.org -c ~/Downloads/demo-credentials.json\nFound the following guids for uploader user@gen3.org: ['3d275025-8b7b-4f84-9165-72a8a174d642', '5bcd2a59-8225-44a1-9562-f74c324d8dec', '6cf194f1-c68e-4976-8ca4-a0ce9701a9f3', '956890a9-b8a7-4abd-b8f7-dd0020aaf562', '9bd009b6-e518-4fe5-9056-2b5cba163ca3']\nSuccessfully deleted GUID 3d275025-8b7b-4f84-9165-72a8a174d642\nSuccessfully deleted GUID 5bcd2a59-8225-44a1-9562-f74c324d8dec\nSuccessfully deleted GUID 6cf194f1-c68e-4976-8ca4-a0ce9701a9f3\nSuccessfully deleted GUID 956890a9-b8a7-4abd-b8f7-dd0020aaf562\nSuccessfully deleted GUID 9bd009b6-e518-4fe5-9056-2b5cba163ca3\n</code></pre>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#3-how-to-upload-a-single-data-file-using-a-guid","title":"3. How to Upload a Single Data File Using a GUID","text":"<p>If a data file has already been assigned a GUID via registration in a Gen3 data commons' indexd database, then the gen3-client can be used to upload the file associated with that GUID to object storage via the <code>gen3-client upload-single</code> command.</p> <p>NOTE: For most data uploaders, using the <code>gen3-client upload</code> command followed by mapping files in the data portal is the preferred method for uploading files. See this section of the documentation for details.</p> <p>The GUID or <code>object_id</code> property for a submitted data file can then be obtained via graphQL query or viewing the data file JSON record in the graphical model of the project.</p> <p>Example Usage:</p> Text Only<pre><code>gen3-client upload-single --profile=&lt;profile_name&gt; --guid=&lt;GUID&gt; --file=&lt;filename&gt;\n\ngen3-client upload-single --profile=demo --guid=b4642430-8c6e-465a-8e20-97c458439387 --file=test.gif\n\nUploading data ...\ntest.gif  3.64 MiB / 3.64 MiB [==========================================================================================] 100.00%\nSuccessfully uploaded file \"test.gif\" to GUID b4642430-8c6e-465a-8e20-97c458439387.\n1 files uploaded.\n</code></pre>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#4-how-to-upload-multiple-data-files-using-a-manifest","title":"4. How to Upload Multiple Data Files Using a Manifest","text":"<p>Users can automate the bulk upload of data files by providing the gen3-client with an upload manifest. The upload manifest should follow the same format as the download manifest, which is described in the previous section. Minimally, the manifest file is a JSON that contains <code>object_id</code> fields. The value of each <code>object_id</code> field is the GUID (globally unique identifier) of a data file that will be uploaded. In this mode, we assume the filenames of data files to be uploaded are the same as the GUIDs.</p> <p>Example of manifest.json (Minimal):</p> Text Only<pre><code>[\n  {\n    \"object_id\": \"a12ff17c-2fc0-475a-9c21-50c19950b082\"\n  },\n  {\n    \"object_id\": \"b12ff17c-2fc0-475a-9c21-50c19950b082\"\n  },\n  {\n    \"object_id\": \"c12ff17c-2fc0-475a-9c21-50c19950b082\"\n  }\n]\n</code></pre> <p>The gen3-client will upload all the files in the provided manifest using the <code>gen3-client upload-multiple</code> command.</p> <p>Example Usage:</p> Text Only<pre><code>gen3-client upload-multiple --profile=&lt;profile_name&gt; --manifest=&lt;manifest_file&gt; --upload-path=&lt;path_for_files&gt;\n\ngen3-client upload-multiple --profile=demo --manifest=manifest.json --upload-path=upload\n\nUploading data ...\na12ff17c-2fc0-475a-9c21-50c19950b082  3.64 MiB / 3.64 MiB [==========================================================================================] 100.00%\nb12ff17c-2fc0-475a-9c21-50c19950b082  3.63 MiB / 3.63 MiB [==========================================================================================] 100.00%\nc12ff17c-2fc0-475a-9c21-50c19950b082  3.65 MiB / 3.65 MiB [==========================================================================================] 100.00%\nSuccessfully uploaded file \"a12ff17c-2fc0-475a-9c21-50c19950b082\" to GUID a12ff17c-2fc0-475a-9c21-50c19950b082.\nSuccessfully uploaded file \"b12ff17c-2fc0-475a-9c21-50c19950b082\" to GUID b22ff17c-2fc0-475a-9c21-50c19950b082.\nSuccessfully uploaded file \"c12ff17c-2fc0-475a-9c21-50c19950b082\" to GUID c22ff17c-2fc0-475a-9c21-50c19950b082.\n3 files uploaded.\n</code></pre>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#5-quick-start-for-experienced-users-or-cheat-sheet","title":"5. Quick Start for Experienced Users or Cheat Sheet","text":"<p>Quick start instructions are covered in the Downloading Files Using the Gen3-client section.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/submit-unstructured-data/#6-working-from-the-command-line","title":"6. Working from the Command line","text":"<p>Working from the command line instructions are covered in the Downloading Files Using the Gen3-client section.</p>","tags":["submission"]},{"location":"gen3-resources/operator-guide/tutorial_alloy/","title":"Grafana Alloy Helm Chart","text":""},{"location":"gen3-resources/operator-guide/tutorial_alloy/#grafana-alloy-helm-chart","title":"Grafana Alloy Helm Chart","text":"<p>Please click HERE to view the Alloy Helm Chart.</p>"},{"location":"gen3-resources/operator-guide/tutorial_alloy/#overview","title":"Overview","text":"<p>This document provides a guide for deploying Grafana Alloy to your Kubernetes cluster using Helm. Grafana Alloy is a powerful observability tool that collects and ships logs and metrics from your services to Grafana Loki and Mimir for storage and analysis. By deploying Alloy, you can gain deep insights into your system\u2019s performance, track key metrics, and troubleshoot issues efficiently.</p> <p>In this deployment, the Alloy ConfigMap plays a crucial role in configuring which logs are collected for Loki and which metrics are gathered for Mimir. It also specifies the endpoints for Loki and Mimir where the data will be sent.</p> <p>Before deploying Alloy, it is important to first deploy the \"observability\" Helm chart, as it provides the necessary components and configuration for Alloy to function properly. Please refer to the SETUP.md observability chart documentation for instructions on how to set it up before proceeding with the Alloy deployment.</p>"},{"location":"gen3-resources/operator-guide/tutorial_alloy/#configuring-alloy","title":"Configuring Alloy","text":""},{"location":"gen3-resources/operator-guide/tutorial_alloy/#helm-chart-configuration","title":"Helm Chart Configuration","text":"<p>The Alloy configuration is the key component that allows users to customize what logs are collected for Loki and which metrics are collected for Mimir. Through this configuration, you can define the specific endpoints where logs and metrics should be sent, ensuring that data is properly routed for observability and analysis.</p> <p>In this configuration, it is important to replace the placeholder hostnames (*.example.com) with the actual Loki and Mimir hostnames that were configured in the \"observability\" Helm chart. This ensures that logs are sent to the correct Loki endpoint and metrics are forwarded to the appropriate Mimir endpoint, allowing your observability stack to function effectively. Additionally, you can fine-tune the alloyConfigmapData to suit your environment's needs. Please click here to see in-depth documentation on how to do so.</p> YAML<pre><code>    // Write Endpoints\n    // prometheus write endpoint\n    prometheus.remote_write \"default\" {\n      external_labels = {\n        cluster = \"{{ .Values.cluster }}\",\n        project = \"{{ .Values.project }}\",\n      }\n      endpoint {\n        url = \"https://mimir.example.com/api/v1/push\"\n\n        headers = {\n          \"X-Scope-OrgID\" = \"anonymous\",\n        }\n\n      }\n    }\n\n    // loki write endpoint\n    loki.write \"endpoint\" {\n      external_labels =  {\n        cluster = \"{{ .Values.cluster }}\",\n        project = \"{{ .Values.project }}\",\n      }\n      endpoint {\n        url = \"https://loki.example.com/loki/api/v1/push\"\n      }\n    }\n</code></pre>"},{"location":"gen3-resources/operator-guide/tutorial_alloy/#helm-chart-links","title":"Helm Chart Links","text":"<p>The link below will take you to the Grafana Alloy chart, providing a comprehensive list of configurable options to help you further customize your setup.</p> <p>Alloy Helm Chart</p> <p>By following this guide, you'll successfully configure Alloy to send logs and metrics to Grafana Loki and Mimir. The setup will ensure that Alloy collects the necessary observability data from your environment and forwards logs to Loki and metrics to Mimir for analysis and storage. This configuration will allow you to monitor your system's logs and metrics efficiently through Grafana.</p>"},{"location":"gen3-resources/operator-guide/tutorial_faro/","title":"Grafana Alloy and Faro","text":""},{"location":"gen3-resources/operator-guide/tutorial_faro/#grafana-alloy-and-faro","title":"Grafana Alloy and Faro","text":"<p>Please click HERE to view the Faro Collector Helm Chart.</p>"},{"location":"gen3-resources/operator-guide/tutorial_faro/#overview","title":"Overview","text":"<p>This guide provides a step-by-step approach to configuring an Alloy instance to collect Grafana Faro logs sent over the internet, similar to Real User Monitoring (RUM). The Portal service generates Faro logs, which Alloy collects and forwards to Loki for storage and analysis in Grafana. Additionally, this guide explains how to enable metrics in the Fence service and adjust the Faro URL in the Gen3 Portal configuration to route metrics to your Alloy instance. Future updates will enable more Gen3 services to offer metric collection.</p> <p>Before deploying Alloy, it is important to first deploy the \"observability\" Helm chart, as it provides the necessary components and configuration for Alloy to function properly. Please refer to the SETUP.md observability chart documentation for instructions on how to set it up before proceeding with the Alloy deployment.</p>"},{"location":"gen3-resources/operator-guide/tutorial_faro/#why-does-faro-require-an-internet-facing-ingress","title":"Why Does Faro Require an Internet-Facing Ingress?","text":"<p>Grafana Faro collects Real User Monitoring (RUM) data, such as performance metrics, errors, and user interactions, via the Fence and Portal services. This data is sent from user devices to the backend, which in this case is Alloy. To enable this communication, an internet-facing ingress is required to expose the Faro endpoint to the public, allowing users' browsers to send RUM data to the Alloy instance over the internet.</p>"},{"location":"gen3-resources/operator-guide/tutorial_faro/#configuring-alloy-for-faro-logs","title":"Configuring Alloy for Faro Logs","text":""},{"location":"gen3-resources/operator-guide/tutorial_faro/#helm-chart-configuration","title":"Helm Chart Configuration","text":"<p>The ingress is configured with AWS ALB (Application Load Balancer) to expose the Alloy Faro port (12347) to the internet. The alb.ingress.kubernetes.io/scheme annotation ensures that the ALB is internet-facing, allowing users to send logs from their browsers to Alloy.</p> <p>When configuring the Faro collector, you will need to update the hosts section of the values.yaml file to match the hostname you plan to use for the Faro collector. For example, replace \"faro.example.com\" with your desired hostname.</p> <p>Additionally, it is highly recommended that you uncomment and adjust the annotations provided for AWS ALB (Application Load Balancer) to fit your environment. These annotations will help ensure proper configuration of the load balancer, SSL certificates, and other key settings. For instance, make sure to replace the placeholder values such as \"cert arn\", \"ssl policy\", and \"environment name\" with your specific details.</p> YAML<pre><code>alloy:\n  extraPorts:\n    - name: \"faro\"\n      port: 12347\n      targetPort: 12347\n      protocol: \"TCP\"\n  clustering:\n    enabled: true\n  configMap:\n    name: alloy-gen3\n    key: config\n\ningress:\n  enabled: true\n  ingressClassName: \"alb\"\n  annotations:\n    alb.ingress.kubernetes.io/certificate-arn: &lt;cert arn&gt;\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\":443}]'\n    alb.ingress.kubernetes.io/load-balancer-attributes: idle_timeout.timeout_seconds=600\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/ssl-policy: &lt;ssl policy&gt;\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    alb.ingress.kubernetes.io/tags:  Environment=&lt;environment&gt;\n    alb.ingress.kubernetes.io/target-type: ip\n  labels: {}\n  path: /\n  faroPort: 12347\n  hosts:\n    - faro.example.com\n\nalloy-configmap-data: |\n  logging {\n    level    = \"info\"\n    format   = \"json\"\n  }\n\n  otelcol.exporter.otlp \"tempo\" {\n    client {\n      endpoint = \"http://grafana-tempo-distributor.monitoring:4317\"\n      tls {\n        insecure = true\n        insecure_skip_verify = true\n      }\n    }\n  }\n\n  loki.write \"endpoint\" {\n    endpoint {\n      url = \"http://grafana-loki-gateway.monitoring:80/loki/api/v1/push\"\n    }\n  }\n\n  faro.receiver \"default\" {\n    server {\n      listen_address = \"0.0.0.0\"\n      listen_port = 12347\n      cors_allowed_origins = [\"*\"]\n    }\n\n    extra_log_labels = {\n      service = \"frontend-app\",\n      app_name = \"\",\n      app_environment = \"\",\n      app_namespace = \"\",\n      app_version = \"\",\n    }\n    output {\n      logs   = [loki.write.endpoint.receiver]\n      traces = [otelcol.exporter.otlp.tempo.input]\n    }\n  }\n</code></pre>"},{"location":"gen3-resources/operator-guide/tutorial_faro/#helm-chart-links","title":"Helm Chart Links","text":"<p>The link below will take you to the Grafana Alloy chart, providing a comprehensive list of configurable options to help you further customize your setup.</p> <p>Alloy Helm Chart</p>"},{"location":"gen3-resources/operator-guide/tutorial_faro/#enabling-faro-metrics-in-fence","title":"Enabling Faro Metrics in Fence","text":"<p>Fence now has built-in Faro metrics. To enable these metrics, you must update your Fence deployment.</p> <p>*** Note: you must be using Fence version 10.2.0 or later</p>"},{"location":"gen3-resources/operator-guide/tutorial_faro/#step-1-enable-prometheus-metrics-in-the-fence-pod","title":"Step 1: Enable Prometheus Metrics in the Fence Pod","text":"<p>Update your Fence deployment with the following annotations to allow Prometheus to scrape the metrics:</p> YAML<pre><code>fence:\n    podAnnotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: \"true\"\n</code></pre>"},{"location":"gen3-resources/operator-guide/tutorial_faro/#step-2-enable-metrics-in-the-fence-configuration","title":"Step 2: Enable Metrics in the Fence Configuration","text":"<p>Modify the FENCE_CONFIG_PUBLIC section to enable Prometheus metrics:</p> YAML<pre><code>fence:\n    FENCE_CONFIG_PUBLIC:\n        ENABLE_PROMETHEUS_METRICS: true\n        ENABLE_DB_MIGRATION: true\n</code></pre>"},{"location":"gen3-resources/operator-guide/tutorial_faro/#updating-faro-url-in-gen3-portal","title":"Updating Faro URL in Gen3 Portal","text":"<p>If you need to change the Faro URL that metrics are sent to, you will need to update the \"grafanaFaroUrl\" field by modifying the \"gitops.json\" value in your values.yaml. You can refer to this link for more information.</p> YAML<pre><code>portal:\n    # -- (map) GitOps configuration for portal\n    gitops:\n    # -- (string) multiline string - gitops.json\n        json: |\n            {\n            \"grafanaFaroConfig\": {\n                \"grafanaFaroEnable\": true, // optional; flag to turn on Grafana Faro RUM, default to false\n                \"grafanaFaroNamespace\": \"DEV\", // optional; the Grafana Faro RUM option specifying the application\u2019s namespace, for example: prod, pre-prod, staging, etc. Can be determined automatically if omitted. But it is highly recommended to customize it to include project information, such as 'healprod'\n                \"grafanaFaroUrl\": \"\", // optional: the Grafana Faro collector url. Defaults to https://faro.example.com/collect\n                \"grafanaFaroSampleRate\": 1, // optional; numeric; the Grafana Faro option specifying the percentage of sessions to track: 1 for all, 0 for none. Default to 1 if omitted\n            },\n</code></pre> <p>By following this guide, you'll have successfully set up Alloy to receive Grafana Faro logs and metrics while exposing the service over the internet using Kubernetes ingress. You\u2019ll also be able to monitor Faro metrics through Fence and make necessary configurations in Gen3 Portal for seamless Faro integration.</p>"},{"location":"gen3-resources/operator-guide/tutorial_fence_usersync_job/","title":"Fence Usersync Cronjob","text":""},{"location":"gen3-resources/operator-guide/tutorial_fence_usersync_job/#fence-usersync-cronjob","title":"Fence Usersync CronJob","text":"<p>If <code>.Values.usersync.usersync</code> is set to true, the Fence usersync-cron.yaml will be deployed to the cluster.</p> <p>User lists can be synced from three sources:</p> <ol> <li> <p>A ftp/sftp server that hosts user csv files that follows the format provided by dbgap, enabled if <code>.Values.usersync.syncFromDbgap</code> is set to \"true\". Please follow the Sftp Setup guide before enabling this option.</p> </li> <li> <p>A user.yaml file that is pulled from the S3 bucket specified in the <code>.Values.usersync.userYamlS3Path</code> field is used to update fence's user-access database. Please note an IAM policy with S3 read is required for this option. Please follow S3 user.yaml Setup guide below.</p> </li> <li> <p>If the <code>.Values.usersync.userYamlS3Path</code> string is set to \"none\", the user.yaml file specified in the fence values.yaml HERE will be used.</p> </li> </ol>"},{"location":"gen3-resources/operator-guide/tutorial_fence_usersync_job/#s3-setup","title":"S3 Setup","text":"<p>Please see this documentation that details user.yaml formatting.</p> <p>You can pull this file from an S3 bucket that is set in the <code>.Values.usersync.userYamlS3Path</code> field. Then input the IAM credentials for a user that has read access to the specified S3 bucket in the <code>.Values.secrets.awsAccessKeyId</code> and <code>.Values.secrets.awsSecretAccessKey</code> fields.</p> <p>You can utilize a local secret to avoid pasting credentials in the values.yaml file. Just set <code>.global.aws.useLocalSecret.enabled</code> to true and supply your secret name.</p> <p>Notice: The Gen3 Helm chart has various jobs and uses for an IAM user. To enhance code reusability, we've implemented the option for jobs and services to share the same AWS IAM global user. If you would like to use the same IAM user for Fence Usersync, External Secrets, etc.- you can follow this guide that details how to setup a Helm global user.</p> <p>As previously mentioned, if the <code>.Values.usersync.userYamlS3Path</code> string is set to \"none\", the user.yaml file from Fence values.yaml will be used.</p>"},{"location":"gen3-resources/operator-guide/tutorial_fence_usersync_job/#dbgap","title":"Dbgap","text":""},{"location":"gen3-resources/operator-guide/tutorial_fence_usersync_job/#sftp-setup","title":"Sftp Setup","text":"<p>You can configure one or more dbGaP SFTP servers to sync telemetry files from. To configure one single dbGaP server, add credentials and information to the fence-config.yaml under dbGaP, this is outlined here.</p> <p>To configure additional dbGaP servers, include in the config.yaml a list of dbGaP servers under dbGaP, like so:</p> <p>``` dbGaP: - info:     host:     username:     password:     ...   protocol: 'sftp'   ...   ... - info:     host:     username:     ... ````</p> <p>You can find more detailed information on the setup with examples here.</p> <p>For an example of a dbGap auth file (csv), please see this example example for formatting.</p>"},{"location":"gen3-resources/operator-guide/tutorial_fence_usersync_job/#dbgap-options","title":"Dbgap Options","text":"<p>Set <code>.Values.usersync.addDbgap</code> to \"true\" to attempt a dbgap sync and fall back on user.yaml.</p> <p>Set <code>.Values.usersync.onlyDbgap</code> to \"true\" to run only a dbgap sync and ignore the user.yaml.</p>"},{"location":"gen3-resources/operator-guide/tutorial_fence_usersync_job/#slack-options","title":"Slack Options","text":"<p>Set <code>.Values.usersync.slack_webhook</code> to configure a webhook endpoint to be used for regular usersync updates to Slack.</p> <p>Set <code>.Values.usersync.slack_send_dbgap</code> to \"true\" to echo the files that are being seen on dbgap ftp to Slack.</p>"},{"location":"gen3-resources/operator-guide/tutorial_fence_usersync_job/#other-customizations","title":"Other Customizations","text":"<p>The <code>.Values.usersync.schedule</code> option can be set to customize the cron schedule expression. The default setting is to have the job run once every 30 minutes.</p> <p>The <code>.Values.usersync.custom_image</code> can be set to override the default \"awshelper\" image for the init container of the userync cronjob.</p>"},{"location":"gen3-resources/operator-guide/tutorial_global_IAM_helm_user/","title":"Create an AWS IAM Global User","text":""},{"location":"gen3-resources/operator-guide/tutorial_global_IAM_helm_user/#aws-iam-global-user","title":"AWS IAM Global User","text":"<p>For Helm code reusability, we have added the functionality to use one IAM user for various jobs/services.</p> <p>We are currently in the process of integrating this user into our Terraform code. In the meantime, you can manually create a global user by referring to this guide.</p>"},{"location":"gen3-resources/operator-guide/tutorial_global_IAM_helm_user/#what-this-user-can-do-in-helm","title":"What this user can do in Helm","text":"<ul> <li>Fence Usersync Job</li> <li>ES Index Restore</li> <li>Restore PGdump</li> <li>External Secrets</li> <li>AWS ES Proxy Service</li> </ul> <p>Example policy containing all the proper permissions: </p>Text Only<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": [\n                \"arn:aws:s3:::$BUCKET/$ENVIRONMENT/*\",\n                # Fence Usersync Job: Name of the userYamlS3Path containing the user.yaml file\n                \"arn:aws:s3:::$BUCKET/$ENVIRONMENT/$VERSION/elasticsearch/*\",\n                # ES Index Restore Job: Name of the dbRestoreBucket with the proper path to the ES dump files.\n                \"arn:aws:s3:::$BUCKET/$ENVIRONMENT/$VERSION/pgdumps/*\"\n                # DB PG Dump Restore Job: Name of the dbRestoreBucket with the proper path to the SQL dump files.\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:ListSecrets\",\n                \"secretsmanager:GetSecretValue\"\n            ],\n            \"Resource\": [\n                \"*\"\n                # External Secrets: Leave as is to allow External Secrets access to your secrets in Secrets Manager.\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"es:*\",\n            \"Resource\": \"arn:aws:es:REGION:ACCOUNT_ID:domain/CLUSTER_NAME/*\"\n            # AWS ES Proxy Service: Arn of your Elasticsearch Cluster in AWS.\n        }\n    ]\n}\n</code></pre>"},{"location":"gen3-resources/operator-guide/tutorial_global_IAM_helm_user/#after-creating-the-user","title":"After Creating the User","text":"<p>In order to integrate the user in Helm, paste in the values of your Access and Secret Access key in <code>.Values.global.aws.awsAccessKeyId</code> and <code>.Values.global.aws.awsSecretAccessKey</code></p>"},{"location":"gen3-resources/operator-guide/tutorial_jobs_overview/","title":"About Jobs in Gen3","text":""},{"location":"gen3-resources/operator-guide/tutorial_jobs_overview/#jobs-in-gen3","title":"Jobs in Gen3","text":"<p>A job is a finite unit of work that runs to completion (usually within a containerized environment). In Gen3, jobs are generally tasks that load, gather, or dump data. These might be background maintenance cronjobs set to run on a schedule (in our production data commons, an example is the usersync cronjob in Fence that runs to update policies based on the user.yaml), or jobs that are triggered as needed, perhaps manually by users (e.g., PFB creation) or in response to some event in the Gen3 instance (e.g., the ssjdispatcher indexing job when data files are uploaded).  </p> <p>Here is a list of existing Kubernetes jobs in Gen3; note that a few are designed for specific data commons and require a specialized Gen3 setup to work as expected. You can read more about what each job does in this README (it may be missing some more recently-created jobs).  </p> <p>We have highlighted some specific jobs you will likely need early in your Gen3 development in this documentation.  </p> <ul> <li>Fence Usersync CronJob</li> <li>ETL job (described in the ETL configuration documentation)</li> <li>Database Creation Job (described in the Databases in Gen3 Helm documentation)</li> </ul>"},{"location":"gen3-resources/operator-guide/tutorial_observability/","title":"Observability","text":""},{"location":"gen3-resources/operator-guide/tutorial_observability/#observability-helm-chart","title":"Observability Helm Chart","text":"<p>Please click HERE to view the Observability Helm Chart.</p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#overview","title":"Overview","text":"<p>The Observability Helm chart provides an all-in-one solution for deploying Mimir, Loki, and Grafana to your Kubernetes cluster, enabling a complete observability stack for metrics, logs, and visualization.</p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#grafana","title":"Grafana","text":"<p>A leading open-source platform for data visualization and monitoring. Grafana allows you to create rich, interactive dashboards from a variety of data sources, making it easy to analyze metrics and logs from your systems.</p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#mimir","title":"Mimir","text":"<p>Grafana Mimir is a highly scalable time-series database optimized for storing and querying metrics. It enables powerful alerting and querying for real-time monitoring of your infrastructure and applications.</p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#loki","title":"Loki","text":"<p>Grafana Loki is a log aggregation system designed to efficiently collect, store, and query logs from your applications. It works seamlessly with Grafana, providing an integrated way to visualize logs alongside metrics.</p> <p>By deploying this Helm chart, you'll set up these three components together, allowing you to monitor your systems and applications comprehensively with metrics from Mimir, logs from Loki, and dashboards and alerts in Grafana.</p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#alloy","title":"Alloy","text":"<p>Grafana Alloy is a powerful observability tool that collects and ships logs and metrics from your services to Grafana Loki and Mimir for storage and analysis.</p> <p>Note: Grafana Alloy is deployed in a separate Helm Chart. You will need to follow the instructions outlined in alloy.md after completing the following guide.</p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#faro-collector-alloy","title":"Faro Collector (Alloy)","text":"<p>Alloy Faro Collector is a specialized configuration of Alloy that enables it to gather Real User Monitoring (RUM) data from Portal through Grafana Faro. In this role, Alloy acts as an ingestion point for RUM data.</p> <p>Note: The Faro Collector is deployed in a separate Helm Chart. You will need to follow the instructions outlined in faro.md after completing the following guide.</p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#general-architecture","title":"General Architecture","text":"<p>In this setup, Loki and Mimir are configured with internal ingress resources, enabling Alloy to send metrics and logs securely via VPC peering connections. Both Loki and Mimir write the ingested data to Amazon S3 for scalable and durable storage. This data can be queried and visualized through Grafana, which is hosted behind an internet-facing ingress. Access to Grafana can be restricted using CIDR ranges defined through the ALB ingress annotation: alb.ingress.kubernetes.io/inbound-cidrs: \"cidrs\". Additionally, the chart supports SAML authentication for Grafana, configured through the grafana.ini field, ensuring secure user access.</p> <p></p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#fips-compliant-images","title":"Fips compliant images","text":"<p>Gen3 provides FIPS-compliant images, which are set as the default in the values file for Grafana, Mimir, and Loki. These images are self-hosted and maintained by the Gen3 platform team, ensuring secure and compliant operations. The Platform Team is responsible for managing image upgrades, and service versions will be updated as deemed necessary by the team.</p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#helm-chart-links","title":"Helm Chart Links","text":"<p>The links below will take you to the Grafana LGTM chart, as well as the Grafana, Loki, and Mimir charts, providing a comprehensive list of configurable options to help you further customize your setup.</p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#link-to-lgtm-helm-chart","title":"Link to lgtm Helm chart","text":"<ul> <li>LGTM Helm Chart</li> </ul>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#full-configuration-options-for-all-components","title":"Full Configuration Options for all Components","text":"<ul> <li>Grafana</li> <li>Loki</li> <li>Mimir</li> </ul>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#affinity-rules","title":"Affinity Rules","text":"<p>The affinity rule in the values.yaml file controls pod scheduling to specific nodes or zones. By default, pods are restricted to nodes in us-east-1a using a node label (topology.kubernetes.io/zone).</p> <p>Customize these rules to align with your cluster\u2019s zones or labels to ensure pods can schedule properly. Mismatched configurations can lead to scheduling failures.</p> YAML<pre><code>      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              # -- (string) Node label key for affinity. Ensures pods are scheduled on nodes in the specified zone.\n              - key: topology.kubernetes.io/zone\n                # -- (string) Operator to apply to the node selector. 'In' means the node must match one of the values.\n                operator: In\n                # -- (list) List of values for the node selector, representing allowed zones.\n                values:\n                - us-east-1a\n</code></pre>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#irsa-role-setup","title":"IRSA Role Setup","text":"<p>This Helm chart automatically creates a service account named \"observability\" for use with Loki and Mimir. To ensure proper access to the storage buckets holding Loki and Mimir data, you\u2019ll need to associate an AWS IAM Role with this service account. Configure the role with the necessary permissions to access the relevant S3 buckets, and then provide the role\u2019s ARN in the appropriate section of your values.yaml file.</p> YAML<pre><code>lgtm:\n  #  -- (map) Configuration for IRSA role to use with service accounts.\n  role:\n    # -- (string) The arn of the aws role to associate with the service account that will be used for Loki and Mimir.\n    # Documentation on IRSA setup https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html\n    arn:\n</code></pre>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#configuring-grafana","title":"Configuring Grafana","text":"<p>When configuring the Grafana, you will need to update the hosts section of the values.yaml file to match the hostname you plan to use. For example, replace \"grafana.example.com\" with your desired hostname.</p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#ingress","title":"Ingress","text":"<p>Grafana will require an internet-facing ingress in order to access the visualizations, alerts, etc. It is highly recommended that you uncomment and adjust the annotations provided for AWS ALB (Application Load Balancer) to fit your environment (if deploying via AWS). These annotations will help ensure proper configuration of the load balancer, SSL certificates, and other key settings. For instance, make sure to replace the placeholder values such as \"cert arn\", \"ssl policy\", and \"environment name\" with your specific details. Access to Grafana can be restricted using CIDR ranges defined through the ALB ingress annotation: alb.ingress.kubernetes.io/inbound-cidrs: \"cidrs\".</p> YAML<pre><code>grafana:\n  ingress:\n    # -- (bool) Enable or disable ingress for Grafana.\n    enabled: true\n    # -- (map) Annotations for Grafana ingress.\n    annotations:\n      annotations: {}\n        ## Recommended annotations for AWS ALB (Application Load Balancer).\n        # alb.ingress.kubernetes.io/ssl-redirect: '443'\n        # alb.ingress.kubernetes.io/certificate-arn:  &lt;cert arn&gt;\n        # alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\":443}]'\n        # alb.ingress.kubernetes.io/load-balancer-attributes: idle_timeout.timeout_seconds=600\n        # alb.ingress.kubernetes.io/scheme: internet-facing\n        # alb.ingress.kubernetes.io/ssl-policy: &lt;ssl policy&gt;\n        # alb.ingress.kubernetes.io/tags: Environment=&lt;environment&gt;\n        # alb.ingress.kubernetes.io/target-type: 'ip'\n        # alb.ingress.kubernetes.io/inbound-cidrs: &lt;cidrs&gt;\n    # -- (list) Hostname(s) for Grafana ingress.\n    hosts:\n      - grafana.example.com\n    # -- (string) Ingress class name to be used (e.g., 'alb' for AWS Application Load Balancer).\n    ingressClassName: \"alb\"\n</code></pre>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#built-in-gen3-alerts","title":"Built-in Gen3 Alerts","text":"<p>This Helm chart comes equipped with built-in Gen3 alerts, defined in the 'alerting' section of the values.yaml. These alerts enable you to immediately leverage your logs and metrics as soon as Grafana is up and running.</p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#built-in-gen3-dashboards","title":"Built-in Gen3 Dashboards","text":"<p>You can utilize Gen3-specific visualizations by visiting our grafana-dashboards repo.</p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#configuring-mimir","title":"Configuring Mimir","text":"<p>When configuring the Mimir, you will need to update the hosts section of the values.yaml file to match the hostname you plan to use. For example, replace \"mimir.example.com\" with your desired hostname.</p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#ingress_1","title":"Ingress","text":"<p>Mimir will require an internal ingress in order to access the visualizations, alerts, etc. It is highly recommended that you uncomment and adjust the annotations provided for AWS ALB (Application Load Balancer) to fit your environment (if deploying via AWS). These annotations will help ensure proper configuration of the load balancer, SSL certificates, and other key settings. For instance, make sure to replace the placeholder values such as \"cert arn\", \"ssl policy\", and \"environment name\" with your specific details.</p> YAML<pre><code>mimir:\n    ingress:\n      #  -- (map) Annotations to add to mimir ingress.\n      annotations: {}\n        ## Recommended annotations for AWS ALB (Application Load Balancer).\n        # alb.ingress.kubernetes.io/certificate-arn: &lt;cert arn&gt;\n        # alb.ingress.kubernetes.io/ssl-redirect: '443'\n        # alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\":443}]'\n        # alb.ingress.kubernetes.io/load-balancer-attributes: idle_timeout.timeout_seconds=600\n        # alb.ingress.kubernetes.io/scheme: internal\n        # alb.ingress.kubernetes.io/ssl-policy: &lt;ssl policy&gt;\n        # alb.ingress.kubernetes.io/tags: Environment=&lt;environment name&gt;\n        # alb.ingress.kubernetes.io/target-type: ip\n      # -- (bool) Enable or disable mirmir ingress.\n      enabled: true\n      # -- (string) Class name for ingress.\n      ingressClassName: \"alb\"\n      # -- (map) Additional paths to add to the ingress.\n      paths:\n        # -- (list) Additional paths to add to the query frontend.\n        query-frontend:\n          - path: /prometheus/api/v1/query\n      # -- (list) hostname for mimir ingress.\n      hosts:\n       - mimir.example.com\n</code></pre>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#storage-configuration","title":"Storage Configuration","text":"<p>The structuredConfig section in Mimir\u2019s configuration defines how backend storage is set up to persist metrics and time-series data. This configuration ensures that data is safely stored and retrievable over time, even if Mimir instances restart or scale.</p> <p>If you are utilizing Amazon S3 for storage, make sure to uncomment \"bucket_name\" and input a value.</p> YAML<pre><code>mimir:\n    # -- (map) Structured configuration settings for mimir.\n    structuredConfig:\n    common:\n        storage:\n        # -- (string) Backend storage configuration. For example, s3 for AWS S3 storage.\n        backend: s3\n        s3:\n            # -- (string) The S3 endpoint to use for storage. Ensure this matches your region.\n            endpoint: s3.us-east-1.amazonaws.com\n            # -- (string) AWS region where your S3 bucket is located.\n            region: us-east-1\n            # # -- (string) Name of the S3 bucket used for storage.\n            # bucket_name: &lt;bucket name&gt;\n</code></pre>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#mimir-components","title":"Mimir Components","text":"<p>Mimir is a high-performance time-series database, typically used for storing and querying metrics. 1. Alertmanager    - Pods: <code>grafana-mimir-alertmanager-*</code>    - Purpose: Manages alert notifications and routing.    - Function: Sends alerts to different channels like email, Slack, etc., based on defined rules.</p> <ol> <li>Compactor</li> <li>Pods: <code>grafana-mimir-compactor-*</code></li> <li>Purpose: Compacts time-series data to optimize storage.</li> <li> <p>Function: Periodically reduces the size of stored metrics by merging smaller chunks.</p> </li> <li> <p>Distributor</p> </li> <li>Pods: <code>grafana-mimir-distributor-*</code></li> <li>Purpose: Accepts incoming metric data and distributes it to ingesters.</li> <li> <p>Function: Acts as a load balancer for metric ingestion.</p> </li> <li> <p>Ingester</p> </li> <li>Pods: <code>grafana-mimir-ingester-*</code></li> <li>Purpose: Temporarily holds and processes incoming metric data.</li> <li> <p>Function: Ingesters store time-series data in memory before flushing to long-term storage.</p> </li> <li> <p>Querier</p> </li> <li>Pods: <code>grafana-mimir-querier-*</code></li> <li>Purpose: Handles metric queries.</li> <li> <p>Function: Retrieves time-series data from ingesters and long-term storage for queries.</p> </li> <li> <p>Query Frontend</p> </li> <li>Pods: <code>grafana-mimir-query-frontend-*</code></li> <li>Purpose: Coordinates and optimizes query execution.</li> <li> <p>Function: Distributes query workloads to ensure performance and efficiency.</p> </li> <li> <p>Query Scheduler</p> </li> <li>Pods: <code>grafana-mimir-query-scheduler-*</code></li> <li>Purpose: Schedules query jobs across queriers.</li> <li> <p>Function: Ensures balanced query processing across components.</p> </li> <li> <p>Ruler</p> </li> <li>Pods: <code>grafana-mimir-ruler-*</code></li> <li>Purpose: Evaluates recording and alerting rules.</li> <li> <p>Function: Generates time-series data or alerts based on predefined rules.</p> </li> <li> <p>Store Gateway</p> </li> <li>Pods: <code>grafana-mimir-store-gateway-*</code></li> <li>Purpose: Provides access to long-term storage.</li> <li>Function: Optimizes retrieval of historical data from object stores.</li> </ol>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#configuring-loki","title":"Configuring Loki","text":"<p>When configuring the Loki, you will need to update the hosts section of the values.yaml file to match the hostname you plan to use. For example, replace \"loki.example.com\" with your desired hostname.</p>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#ingress_2","title":"Ingress","text":"<p>Loki will require an internal ingress in order to access the visualizations, alerts, etc. It is highly recommended that you uncomment and adjust the annotations provided for AWS ALB (Application Load Balancer) to fit your environment (if deploying via AWS). These annotations will help ensure proper configuration of the load balancer, SSL certificates, and other key settings. For instance, make sure to replace the placeholder values such as \"cert arn\", \"ssl policy\", and \"environment name\" with your specific details.</p> YAML<pre><code>loki:\n    ingress:\n    #  -- (map) Annotations to add to loki ingress.\n    annotations: {}\n        ## Recommended annotations for AWS ALB (Application Load Balancer).\n        # alb.ingress.kubernetes.io/certificate-arn: &lt;cert arn&gt;\n        # alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\":443}]'\n        # alb.ingress.kubernetes.io/load-balancer-attributes: idle_timeout.timeout_seconds=600\n        # alb.ingress.kubernetes.io/scheme: internal\n        # alb.ingress.kubernetes.io/ssl-policy: &lt;ssl policy&gt;\n        # alb.ingress.kubernetes.io/ssl-redirect: '443'\n        # alb.ingress.kubernetes.io/tags: Environment=&lt;environment&gt;\n        # alb.ingress.kubernetes.io/target-type: ip\n    # -- (bool) Enable or disable loki ingress.\n    enabled: true\n    # -- (string) Class name for ingress.\n    ingressClassName: \"alb\"\n    # -- (list) Hosts for loki ingress.\n    hosts:\n        # -- (string) Hostname for loki ingress.\n        - host: loki.example.com\n</code></pre>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#storage-configuration_1","title":"Storage Configuration","text":"<p>The structuredConfig section in Loki\u2019s configuration defines how backend storage is set up to persist log data. This configuration ensures that logs are safely stored and retrievable over time, even if Loki instances restart or scale.</p> <p>If you are utilizing Amazon S3 for storage, make sure to uncomment \"bucketnames\" and input a value.</p> YAML<pre><code>loki:\n    # -- (map) Structured configuration settings for Loki.\n    structuredConfig:\n        common:\n            # -- (string) Path prefix where Loki stores data.\n            path_prefix: /var/loki\n            storage:\n            # -- (null) Filesystem storage is disabled.\n            filesystem: null\n            s3:\n                # -- (string) AWS region for S3 storage.\n                region: us-east-1\n                # # -- (string) S3 bucket names for Loki storage.\n                # bucketnames: &lt;bucket name&gt;\n</code></pre>"},{"location":"gen3-resources/operator-guide/tutorial_observability/#loki-components","title":"Loki Components","text":"<p>Loki is used for log aggregation, querying, and management. Each Loki component has a specialized role in the log pipeline. 1. Distributor    - Pods: <code>grafana-loki-distributor-*</code>    - Purpose: Accepts log entries and forwards them to ingesters.    - Function: It load-balances logs from sources and ensures efficient distribution to ingesters.</p> <ol> <li>Gateway</li> <li>Pods: <code>grafana-loki-gateway-*</code></li> <li>Purpose: Acts as an API gateway or entry point for requests.</li> <li> <p>Function: Can be used for proxying queries to the appropriate backend components.</p> </li> <li> <p>Ingester</p> </li> <li>Pods: <code>grafana-loki-ingester-*</code></li> <li>Purpose: Receives and stores log entries in chunks.</li> <li> <p>Function: Ingesters temporarily hold logs in memory and periodically flush them to storage (like S3 or other object stores).</p> </li> <li> <p>Querier</p> </li> <li>Pods: <code>grafana-loki-querier-*</code></li> <li>Purpose: Handles log queries from users.</li> <li> <p>Function: Retrieves logs from ingesters and long-term storage for querying purposes.</p> </li> <li> <p>Query Frontend</p> </li> <li>Pods: <code>grafana-loki-query-frontend-*</code></li> <li>Purpose: Distributes and coordinates queries.</li> <li>Function: Splits large queries into smaller ones for faster execution by the queriers.</li> </ol>"},{"location":"gen3-resources/operator-guide/tutorial_secrets-mgr/","title":"Secrets Manager","text":""},{"location":"gen3-resources/operator-guide/tutorial_secrets-mgr/#secrets-manager","title":"Secrets Manager","text":"<p>If you are doing a \u201cproduction\u201d deployment, it\u2019s highly recommended to use a secrets manager.</p> <p>We use a tool called External Secrets Operator, which was created by the Kubernetes community to manage external secrets in a Kubernetes cluster. It allows you to fetch and sync external secret values from various external secret management systems into Kubernetes secrets. One of the external secret management systems it can connect to is AWS Secrets Manager. AWS Secrets Manager allows for the secure storing of your secrets as well as the ability to periodically and automatically rotate your secrets.</p> <p>This document will guide you through setting up the essential resources to access your secrets in AWS Secrets Manager and download the External Secrets Operator Helm chart. This way, you can effectively utilize your stored secrets with Helm.</p>"},{"location":"gen3-resources/operator-guide/tutorial_secrets-mgr/#download-external-secrets-operator-and-create-resources-in-aws","title":"Download External Secrets Operator and Create Resources in AWS.","text":"<p>You can use the following Bash script to apply the External Secrets Operator to your cluster and create the necessary AWS resources. Fill in the variables below to get started:</p> <p>Notice: The Gen3 Helm chart has various jobs and uses for an IAM user. To enhance code reusability, we've implemented the option for jobs and services to share the same AWS IAM global user. If you would like to use the same IAM user for External Secrets and jobs like Fence Usersync or our \"AWS ES Proxy Service\", you can follow this guide that details how to setup a Helm global user. In case you opt for a global IAM user, please comment out the \"create_iam_policy\" and \"create_iam_user\" functions at the end of the script.</p> Text Only<pre><code>#!/bin/bash\n\nAWS_ACCOUNT=\"&lt;Your AWS Account ID&gt;\"\nregion=\"us-east-1\"\niam_policy=\"external_secrets_policy\"\niam_user=\"external_secrets_user\"\n\nhelm_install()\n{\n    echo \"# ------------------ Install external-secrets via helm --------------------------#\"\n    helm repo add external-secrets https://charts.external-secrets.io\n    helm install external-secrets \\\n    external-secrets/external-secrets \\\n    -n external-secrets \\\n    --create-namespace \\\n    --set installCRDs=true\n}\n\ncreate_iam_policy()\n{\n    echo \"# ------------------ create iam policy for aws secrets manager --------------------------#\"\n    POLICY_ARN=$(aws iam create-policy --policy-name $iam_policy --policy-document '{\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"secretsmanager:ListSecrets\",\n                    \"secretsmanager:GetSecretValue\"\n                ],\n                \"Resource\": [\n                    \"*\"\n                ]\n            }\n        ]\n    }')\n\n    iam_policy_arn=$(aws iam list-policies --query \"Policies[?PolicyName=='$iam_policy'].Arn\" --output text)\n    echo \"Policy Arn: $iam_policy_arn\"\n    # return $iam_policy_arn\n}\n\ncreate_iam_user()\n{\n    echo \"# ------------------ create user $iam_user --------------------------#\"\n    aws iam create-user --user-name $iam_user\n\n    echo \"# ------------------ add iam user $iam_user to policy $iam_policy --------------------------#\"\n    aws iam attach-user-policy --user-name $iam_user --policy-arn $iam_policy_arn\n    echo \"aws iam attach-user-policy --user-name $iam_user --policy-arn $iam_policy_arn\"\n\n    echo \"# ------------------ create access key and secret key for external-secrets --------------------------#\"\n    aws iam create-access-key --user-name $iam_user &gt; keys.json\n    access_key=$(jq -r .AccessKey.AccessKeyId keys.json)\n    secret_key=$(jq -r .AccessKey.SecretAccessKey keys.json)\n    kubectl create secret generic \"$iam_user\"-secret --from-literal=access-key=$access_key --from-literal=secret-access-key=$secret_key\n    rm keys.json\n}\n\nhelm_install\n#comment out the below if using global iam user.\ncreate_iam_policy\ncreate_iam_user\n</code></pre> <p>Please note that Terraform for the creation and population of Gen3 Secrets in AWS Secrets Manager is in development currently. This Terraform will also create the Iam user and policies necessary to access these secrets.</p>"},{"location":"gen3-resources/operator-guide/tutorial_secrets-mgr/#enabling-external-secrets-in-helm-charts","title":"Enabling External Secrets in Helm charts","text":"<p>Our Helm architecture includes a comprehensive umbrella chart designed to streamline the deployment of external secrets across both the umbrella chart itself and its associated subcharts. By configuring the <code>.Values.global.externalSecrets.deploy</code> setting within this umbrella chart, users can effortlessly initiate the deployment of all related external secret resources. This includes the external secret resources within the subcharts and the secret store required for their management.</p>"},{"location":"gen3-resources/operator-guide/tutorial_secrets-mgr/#global-deployment-of-external-secrets","title":"Global Deployment of External Secrets","text":"<p>Upon deployment of the umbrella chart, the <code>.Values.global.externalSecrets.deploy</code> setting automatically provisions external secret resources for every subchart. This occurs regardless of the individual external secrets deployment settings within subcharts, even if they are explicitly set to <code>false</code>. This feature ensures a unified approach to secret management across all components of the architecture.</p>"},{"location":"gen3-resources/operator-guide/tutorial_secrets-mgr/#selective-secret-management","title":"Selective Secret Management","text":"<p>For users requiring a more selective application of external secrets \u2014 targeting specific secrets while excluding others \u2014 the system is designed to accommodate such scenarios with ease.</p> <p>External secret resources will only attempt to replace Kubernetes secrets when a corresponding secret is successfully located within the Secrets Manager. In instances where a specific secret is not found, the External Secrets resource will indicate a <code>SecretSyncedError</code>, signaling the absence of the targeted resource within the Secrets Manager. This error is acceptable and helpful for users who want to enable the use of AWS Secrets Manager for some, but not all the secrets in a specific Helm chart.</p> <p>However, if you wish to utilize External Secrets for managing non-database secrets while still automating the creation of your database secrets, you can configure this behavior explicitly. Set <code>.Values.global.externalSecrets.dbCreate</code> to true alongside <code>.Values.global.postgres.dbCreate</code> or <code>.Values.postgres.dbCreate</code> to initiate the database creation job. This configuration will result in the creation of the necessary databases with their credentials stored securely within Kubernetes Secrets. Subsequently, you also choose to create Secrets in Secrets manager with the values that were generated from the dbCreate job if you wish to store these credentials long term.</p> <p>By default, the following services will not create the Helm internal secrets when Secrets Manager is enabled: - Audit - Fence - Indexd - Manifestservice - Metadata</p> <p>This is because CD tools like Argocd will have trouble syncing resources if the K8s secret was generated via Helm and External Secrets continues to override it. You can configure Helm to still create these secrets with External Secrets enabled by setting the appropriate variable to true.</p> <p>For example, to ensure the \"audit-g3auto\" secret is still created by Helm, you would need to set the following in your values.yaml file: </p>Text Only<pre><code>audit:\n    # -- (map) External Secrets settings.\n    externalSecrets:\n    # -- (string) Will create the Helm \"audit-g3auto\" secret even if Secrets Manager is enabled. This is helpful if you are wanting to use External Secrets for some, but not all secrets.\n    createK8sAuditSecret: true\n</code></pre>"},{"location":"gen3-resources/operator-guide/tutorial_secrets-mgr/#independent-subchart-deployment","title":"Independent Subchart Deployment","text":"<p>In scenarios where subcharts are deployed independently, outside the scope of the umbrella chart, it is crucial to set the <code>.Values.global.externalSecrets.deploy</code> directive within the <code>values.yaml</code> file for each specific service.</p> <p>Additionally, to facilitate the creation of a Secret Store capable of authenticating with AWS Secrets Manager, the <code>.Values.global.externalSecrets.separateSecretStore</code> should be set to true in the relevant charts. This configuration is essential for establishing proper authentication mechanisms for secret retrieval.</p>"},{"location":"gen3-resources/operator-guide/tutorial_secrets-mgr/#configuring-separate-secret-stores","title":"Configuring Separate Secret Stores","text":"<p>The <code>.Values.global.externalSecrets.separateSecretStore</code> setting can also be applied within the context of the umbrella chart deployment. Utilizing this setting allows for the creation of distinct Secret Stores dedicated to individual services. This approach is particularly beneficial for environments where it is preferable to limit access to Secrets Manager, ensuring that services only have access to the secrets explicitly required for their operation.</p>"},{"location":"gen3-resources/operator-guide/tutorial_secrets-mgr/#helm-iam-user","title":"Helm IAM User","text":"<p>If you are using a separate IAM user for AWS Secrets Manager please follow the below instructions:</p> <p>This script Bash script at the beginning of this document should have created a secret titled \"NameofIAMuser-user-secret\" in your cluster. You will need to retrieve these values to input into your Helm chart for the Secret Store to authenticate with AWS Secrets Manager.</p> <p>Access Key: </p>Text Only<pre><code>kubectl get secret \"your secret name\" -o jsonpath=\"{.data.access-key}\" | base64 --decode\n</code></pre> <p>Secret Access Key: </p>Text Only<pre><code>kubectl get secret \"your secret name\" -o jsonpath=\"{.data.secret-access-key}\" | base64 --decode\n</code></pre> <p>You can paste the IAM access key and secret access key in the <code>.Values.secrets.awsAccessKeyId</code>/<code>.Values.secrets.awsSecretAccessKey</code> fields in the values.yaml file for the chart(s) you would like to use external secrets for.</p> <p>If you are deploying external secrets with the Gen3 umbrella chart, you can utilize a local secret to avoid pasting credentials in the values.yaml file. Just set <code>.global.aws.useLocalSecret.enabled</code> to true and supply your secret name.</p> <p>Please note that only some Helm charts are compatible with External Secrets currently. We hope to expand this functionality in the future. If a chart is able to use External Secrets, you can see a <code>.Values.externalSecrets</code> section in the values.yaml file.</p>"},{"location":"gen3-resources/operator-guide/tutorial_secrets-mgr/#secret-store-service-account","title":"Secret Store Service Account","text":"<p>We have recently implemented a new feature that allows users to utilize a service account for their secret store (for external secrets) instead of using IAM keys. This enhances security by leveraging Kubernetes service accounts and AWS roles for authentication.</p> <p>To enable and configure the service account, set the following values under the \"global\" section of the values.yaml file: </p>Text Only<pre><code># -- (map) Service account and AWS role for authentication to AWS Secrets Manager\nsecretStoreServiceAccount:\n  # -- (bool) Set true if deploying to AWS and want to use service account and IAM role instead of aws keys. Must provide role-arn.\n  enabled: false\n  # -- (string) Name of the service account to create\n  name: secret-store-sa\n  # -- (string) AWS Role ARN for Secret Store to use\n  roleArn:\n</code></pre> For detailed instructions on setting up a service account in AWS that allows for its use in conjunction with a service account, please refer to this documentation."},{"location":"gen3-resources/operator-guide/tutorial_secrets-mgr/#how-external-secrets-works","title":"How External Secrets Works.","text":"<p>External Secrets relies on three main resources to function properly. (The below have links to examples of each resource) 1. Aws-config- Contains Access and Secret Access keys used by the Secret Store to authenticate with AWS Secrets Manager 2. Secret Store- Resource to Authenticate with AWS Secrets Manager 3. External Secret- References the Secret Store and is used as a \"map\" to tell External Secrets Operator what secret to grab from External Secrets and the name of the Kubernetes Secret to create locally.</p> Text Only<pre><code>Anatomy of an ExternalSecret:\n```\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  # Name of the External Secret resource\n  name: audit-g3auto\nspec:\n  #How often to Sync with AWS Secrets Manager\n  refreshInterval: 5m\n  secretStoreRef:\n    # The name of the Secret Store to use.\n    name: {{include \"cluster-secret-store\" .}}\n    kind: SecretStore\n  target:\n    # What Kubernetes secret to create from the secret pulled from AWS Secrets Manager.\n    name: audit-g3auto\n    creationPolicy: Owner\n  data:\n  # the key inside the new Kubernetes secret\n  - secretKey: audit-service-config.yaml\n      remoteRef:\n      #name of secret in AWS Secrets Manager\n      key: {{include \"audit-g3auto\" .}}\n```\n</code></pre>"},{"location":"gen3-resources/operator-guide/tutorial_secrets-mgr/#customizing-the-aws-secrets-manager-secrets-name","title":"Customizing the AWS Secrets Manager Secrets Name.","text":"<p>When pulling a secret from AWS Secrets Manager, you want to ensure that the External Secret resource is referencing the proper name of the secret in AWS Secrets Manager. You can customize the name of the secret to pull from in the <code>.Values.externalSecrets</code> section of a Chart. You can see the name for the confiugrable secrets in a chart by looking in this section as well.</p> <p>Any string you put in this section will override the name of the secret that is pulled from AWS Secrets Manager NOT the name of the Kubernetes secret that is created from the External Secret resource.</p>"},{"location":"gen3-resources/operator-guide/helm/","title":"Helm to Configure and Deploy Gen3","text":""},{"location":"gen3-resources/operator-guide/helm/#helm-to-configure-and-deploy-gen3","title":"Helm to Configure and Deploy Gen3","text":""},{"location":"gen3-resources/operator-guide/helm/#-jump-to-quick-start-videos-","title":"-Jump to quick-start videos-","text":"<p>Helm plays a crucial role in simplifying the deployment and management of Gen3 components within your environment. It is a Kubernetes package manager that allows you to define, install, and upgrade even complex applications with ease.</p> <p>If you haven't already installed Helm for your Gen3 deployment:</p> <p>Installation: Install Helm by following the official Helm installation guide for your specific platform.</p>"},{"location":"gen3-resources/operator-guide/helm/#why-is-helm-a-good-choice-for-gen3-deployment","title":"Why is Helm a good choice for Gen3 deployment?","text":"<p>Gen3 Helm includes many sensible defaults for configuration. If you are just beginning to look into Gen3 and are uncertain about configuration decisions or options - you can do minimal configuration and the defaults will help you end up with a functional generic Gen3 instance to play with.</p> <p>But -- Gen3 is also highly configurable. If you are an experienced Gen3 user looking to make sophisticated configurations, Helm will facilitate that, as well.</p>"},{"location":"gen3-resources/operator-guide/helm/#role-of-helm-in-gen3-deployment","title":"Role of Helm in Gen3 Deployment","text":"<p>In a Gen3 deployment, Helm serves as the primary tool for:</p> <ul> <li> <p>Defining Deployments: Helm uses configuration files called charts to define how Gen3 components should be deployed. These charts encapsulate the necessary configuration, dependencies, and deployment logic.</p> </li> <li> <p>Installation: Helm streamlines the process of installing Gen3 components into your Kubernetes cluster. With Helm, you can easily deploy Gen3 services, databases, and other essential components.</p> </li> <li> <p>Configuration Management: Helm simplifies the management of configuration settings for Gen3 services. You can customize settings, such as database connection details, service replicas, and more, through Helm values.</p> </li> <li> <p>Upgrades and Rollbacks: As Gen3 evolves, Helm enables you to effortlessly upgrade your deployment to the latest versions. In case of issues, it also provides the ability to roll back to previous configurations.</p> </li> </ul>"},{"location":"gen3-resources/operator-guide/helm/#helm-charts-valuesyamls-templates-and-more","title":"Helm Charts, values.yamls, templates, and more","text":"<p>In the Gen3 Helm repo, in the <code>/helm</code> directory, there are directories for each of the Gen3 microservices, as well as a directory called <code>/gen3</code>. Each of these folders is called a Helm \"chart\". Helm charts are packages of pre-configured Kubernetes resources that define and deploy microservices. A chart contains all of the resource definitions necessary to run the service inside of a Kubernetes cluster. In any Helm chart, there are several items:</p> <ul> <li>values.yaml: The values.yaml in a Helm chart is what defines the variables relevant for the chart</li> <li>chart.yaml: The chart.yaml is what makes the directory a chart; it defines the dependencies for the chart</li> <li>templates: The templates have incomplete fields that are completed with values from the values.yaml. Templates generate manifest files that Kubernetes can understand.</li> </ul>"},{"location":"gen3-resources/operator-guide/helm/#umbrella-chart-for-coordinated-deployment-gen3-chart","title":"Umbrella Chart for Coordinated Deployment (Gen3 Chart)","text":"<p>To coordinate the deployment of our microservices and additional development-related resources, we use the <code>/gen3</code> Helm chart as the Gen3 \"umbrella\" chart. This \"gen3\" chart serves as the central point for deploying and managing our application as a whole.</p> <p>Values in the Gen3 umbrella values.yaml (also called the Gen3 values.yaml) can override default values in the microservice charts.</p> <p>For example: in the Guppy microservice chart, the default is that Guppy is not enabled (<code>enabled: false</code>). We can turn on Guppy from the Gen3 values yaml by adding a <code>guppy</code> block and including <code>enabled: true</code>, without making any changed to the Guppy values.yaml.</p> <p>Most users should only be making configuration changes in the Gen3 values.yaml, using blocks specific for the service to be configured. Users generally should NOT be editing anything charts for the microservices.</p> <p>The <code>gen3</code> chart includes the following components:</p> <ul> <li>Microservices Helm Charts: All individual microservice Helm charts are incorporated into the <code>gen3</code> chart, ensuring a coordinated and cohesive deployment of the entire application stack.</li> <li>Development Resources: Development tools and resources, such as PostgreSQL and Elasticsearch, are included in the <code>gen3</code> chart to streamline the development and testing process. These resources are essential for replicating the production environment in a development context.</li> </ul>"},{"location":"gen3-resources/operator-guide/helm/#common-chart-for-shared-components","title":"Common Chart for Shared Components","text":"<p>To streamline the deployment of shared components and utilities across our microservices, we have a dedicated <code>common</code> Helm chart. The common chart includes various components and configurations that are shared among multiple microservices. These components typically include:</p> <ul> <li> <p>Database Setup Jobs: Common database setup tasks, such as schema initialization and data migration jobs, are defined within the common chart. This ensures consistent database management across microservices.</p> </li> <li> <p>Secrets Management: Shared secrets and credentials required by multiple microservices are securely managed within the common chart. This centralization enhances security and simplifies secrets management.</p> </li> <li> <p>Shared Components: Other components and utilities that are reused across microservices. This promotes code reuse and maintainability.</p> </li> </ul> <p>By centralizing these common features in a dedicated chart, we reduce redundancy, ensure consistency, and simplify the maintenance and updates of shared components.</p>"},{"location":"gen3-resources/operator-guide/helm/#the-benefits-of-charts","title":"The benefits of charts","text":"<ul> <li> <p>Isolation: Each microservice operates independently within its own Helm chart, allowing for isolation and decoupling of services. This isolation enhances fault tolerance and simplifies updates and maintenance.</p> </li> <li> <p>Customization: Microservices can have their specific configurations and dependencies defined within their Helm charts, making it easier to tailor each service to its unique requirements.</p> </li> </ul>"},{"location":"gen3-resources/operator-guide/helm/#getting-started-with-helm-quick-start-videos","title":"Getting started with Helm - Quick-Start Videos","text":""},{"location":"gen3-resources/operator-guide/helm/#deploy-with-kind-and-helm","title":"Deploy with Kind and Helm","text":"<p>Quick-start to deploy Gen3 with Kind &amp; Helm</p> <p>This video walks through basic set up a Gen3 data commons on your laptop using Kind and Gen3 Helm charts.</p> <p></p>"},{"location":"gen3-resources/operator-guide/helm/#deploy-with-minikube-and-helm","title":"Deploy with Minikube and Helm","text":"<p>Quick-start to deploy Gen3 with Minikube &amp; Helm</p> <p>This video walks through basic set up a Gen3 data commons on your laptop using Minikube and Gen3 Helm charts.</p> <p></p>"},{"location":"gen3-resources/operator-guide/helm/#additional-post-deployment-configuration-tips","title":"Additional post-deployment configuration tips","text":"<p>Gen3 Helm Charts: SSL, Permissions, Test Data, Tabs, and more</p> <p>This video covers several aspects of configuration after deploying Gen3, including:</p> <ul> <li>How to generate SSL certificates for secure communication</li> <li>Updating permissions to ensure seamless access control</li> <li>Submitting test data to validate your configurations</li> <li>Enabling persistence for data continuity</li> <li>Configuring the Workspace Tab for launching notebooks</li> <li>Configuring the Exploration Tab for advanced data discovery</li> </ul> <p></p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-databases/","title":"Databases in Gen3 Helm charts","text":""},{"location":"gen3-resources/operator-guide/helm/helm-deploy-databases/#databases-in-gen3-helm-charts","title":"Databases in Gen3 Helm charts","text":"<p>This document describes how databases are provisioned and used in Gen3 when deploying with Helm charts.</p> <p>We highly recommend using a managed PostgreSQL service like AWS RDS/Aurora, or managing PostgreSQL outside of Helm, for production Gen3 deployments.</p> <p>The bundled PostgreSQL used for development is deployed with the Bitnami chart: https://bitnami.com/stack/postgresql/helm</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-databases/#database-credentials","title":"Database credentials","text":"<p>Every service requiring a PostgreSQL database has credentials stored in a Kubernetes secret.</p> <p>Example (secret values base64 decoded):</p> YAML<pre><code>apiVersion: v1\nkind: Secret\ndata:\n  database: fence_gen3\n  dbcreated: true\n  host: gen3-postgresql\n  password: example_pass\n  port: 5432\n  username: fence_gen3\n</code></pre> <p>Each service consumes this secret and mounts the values as environment variables to access the database.</p> <p>For production, provide PostgreSQL credentials via these values:</p> Text Only<pre><code>global:\n  postgres:\n    dbCreate: true\n    master:\n      host: insert.postgres.hostname.here\n      username: postgres\n      password: &lt;Insert.Password.Here&gt;\n      port: \"5432\"\n\nfence:\n  postgres:\n    host: postgres.example.com\n    username: fence\n    port: 5432\n\nperegrine:\n  postgres:\n    host: postgres.example.com\n    username: peregrine\n    port: 5432\n\nsheepdog:\n  postgres:\n    host: postgres.example.com\n    username: sheepdog\n    port: 5432\n</code></pre> <p>These values can provision and configure databases for Gen3.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-databases/#auto-generated-passwords","title":"Auto-generated passwords","text":"<p>If a per-service password is not specified, Helm will auto-generate a random password. For example:</p> YAML<pre><code>global:\n  postgres:\n    master:\n      password: example_master_password\n\nfence:\n  postgres:\n    host: postgres.example.com\n    username: fence\n\nperegrine:\n  postgres:\n    host: postgres.example.com\n    username: peregrine\n</code></pre> <p>Helm will create a random password for Fence and Peregrine.</p> <p>Note: The lookup function used to generate passwords does not work in ArgoCD. When using ArgoCD, you must explicitly define a password for each service.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-databases/#automatic-database-creation-through-jobs","title":"Automatic database creation through jobs","text":"<p>The <code>dbCreate</code> flag can be set globally or per service.</p> <p>Setting <code>global.postgres.dbCreate: true</code> or <code>&lt;service&gt;.postgres.dbCreate: true</code> kicks off the database setup job for that service.</p> <p>The setup job:</p> <ul> <li>Checks if the database exists</li> <li>Creates it if needed</li> <li>Grants privileges</li> <li>Updates <code>dbcreated: true</code> in the secret</li> <li>Services wait for this dbcreated flag before starting. - This ensures the database is ready before the service is started.</li> </ul> <p>If the database setup job fails, it will not set <code>dbcreated: true</code> in the secret.</p> <p>When a service starts up, it will look for this value in the secret. If dbcreated is not present or false, the service will fail to start and log an error that it cannot find dbcreated in the database secret.</p> <p>This prevents services starting up before the database is ready and properly configured. The root cause of the failure can be investigated by checking the logs and status of the setup job.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/","title":"Example - Minimal Local Deployment","text":""},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#minimal-local-deployment-of-gen3-using-helm","title":"Minimal local deployment of Gen3 using Helm","text":"<p>An important strength of Gen3 deployment with Helm is that the Gen3 Helm repo provides reasonable default configurations, permitting speedy generic Gen3 deployment with relatively few user-specified values. This means that users can employ a step-wise approach to deployment that limits the variables at each step, significantly simplifying troubleshooting for any problems that are encountered during this initial deployment.</p> <p>Therefore, new Gen3 operators should consider first deploying the minimal, most generic Gen3 deployment with Helm. It will allow them to establish that all of the necessary software is installed and functional, and lets them walk through basic Gen3 deployment on their computer in 30-60 minutes (assuming pre-installation of all necessary software). Once a Gen3 operator has the minimal Gen3 deployed locally, they can begin customizing in a systematic way that simplifies troubleshooting by limiting the changes made at each step.</p> <p>The simplest minimal deployment is deploying locally to your localhost, without any SSL certificate or host domain. Although you can deploy a Gen3 instance with all the default values in this way, full configuration and some Gen3 tools depend on the site having an SSL certificate.</p> <p>We provide instructions for minimal deployment with or without an SSL certificate and host domain in the example below. The bulk of the instructions are identical, regardless. In the few places where instructions are different, we provide distinct instructions for these using the following visual callouts:</p> <p>Local deployment with no SSL certificate</p> <p>The information specific to local deployment with no SSL will be here. Note that, although you can deploy Gen3 with no SSL, some tools or features may require that the Gen3 site has an SSL for full functionality.</p> <p>Local deployment to a host domain with SSL</p> <p>The information specific to deployment to a host domain with an SSL will be here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#prerequisites-to-locally-deploy-generic-gen3","title":"Prerequisites to locally deploy generic Gen3","text":"<ul> <li>Install Helm (https://helm.sh/docs/intro/install/) - there are options for installation using a package installer, or downloading directly, for various OS. Note that we will need the Helm CLI, which may not come with all packages.</li> <li>Install Docker (https://docs.docker.com/desktop/)</li> <li>Install kubectl (https://kubernetes.io/docs/tasks/tools/#kubectl)</li> <li>Prepare to install Kind https://kind.sigs.k8s.io/docs/user/quick-start/ (we include installation as part of the instructions below)</li> <li>Install Certbot (or some other tool to get your SSL Certificate)</li> <li>You will need a host site you control (we will use user.dev-site.net to signify this)</li> <li>Install K9s (https://k9scli.io/) - this is used to give a visual view of what\u2019s going on with your deployment</li> </ul>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#use-kind-to-set-up-a-local-kubernetes-cluster-in-docker","title":"Use Kind to set up a local Kubernetes cluster in Docker","text":""},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#install-kind-in-the-same-folder-as-your-docker-binary","title":"Install Kind in the same folder as your Docker binary","text":"<p>First, identify the path to your Docker binary using the <code>which docker</code> command:</p> Text Only<pre><code>(base) MacBook-Pro ~ % which docker\n/usr/local/bin/docker\n</code></pre> <p>Install Kind in the same folder as your docker binary. Here are installation options for different OS: https://kind.sigs.k8s.io/docs/user/quick-start/#installation. Since I have an M1 Mac, I used:</p> Text Only<pre><code>[ $(uname -m) = arm64 ] curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.24.0/kind-darwin-arm64\nchmod +x ./kind\nmv ./kind /some-dir-in-your-PATH/kind\n</code></pre> <p>But, I replaced the <code>.../some-dir-in-your-PATH/kind</code> section in the last line above with the path to my Docker binary (be sure to remove the <code>docker</code> part and replace with <code>kind</code>). I also needed to use <code>sudo</code> in the last line for it to run successfully, so the last line was:</p> Text Only<pre><code>sudo mv ./kind /usr/local/bin/kind\n</code></pre> <p>After installing, run <code>kind</code> to be sure it\u2019s functional - the output shown demonstrates it is working.</p> Text Only<pre><code>(base) MacBook-Pro ~ % kind\nkind creates and manages local Kubernetes clusters using Docker container 'nodes'\nUsage:\n  kind [command]...\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#make-sure-docker-is-installed-and-running","title":"Make sure Docker is installed and running","text":"<p>On Mac or Windows, all you need to do is open Docker Desktop. On Linux, you can use the Docker CLI to run either <code>sudo systemctl start docker</code> or <code>sudo service docker start</code>.</p> <p>Info</p> <p>If you run <code>docker ps</code> and you see this output, Docker is not running: <code>Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?</code></p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#set-up-kubernetes-in-docker","title":"Set up Kubernetes in Docker","text":"<p>Using Kind, we will set up a local Kubernetes cluster.</p> <p>Since we will rely on port 80/443, we also need to forward that to the Kind cluster. The cluster can be created using this command:</p> Text Only<pre><code>cat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n    kubeadmConfigPatches:\n      - |\n        kind: InitConfiguration\n        nodeRegistration:\n          kubeletExtraArgs:\n            node-labels: \"ingress-ready=true\"\n    extraPortMappings:\n      - containerPort: 80\n        hostPort: 80\n        protocol: TCP\n      - containerPort: 443\n        hostPort: 443\n        protocol: TCP\nEOF\n</code></pre> <p>This output indicates it is working: </p>Text Only<pre><code>Creating cluster \"kind\" ...\n...\nSet kubectl context to \"kind-kind\"\nYou can now use your cluster with:\nkubectl cluster-info --context kind-kind\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#set-up-an-ingress-controller","title":"Set up an ingress controller","text":"<p>Next, we need to set up an ingress controller.</p> <p>You can monitor how this setup is going by opening <code>http://localhost/</code> in your browser.</p> <ul> <li>Before you deploy nginx controller, you should see nothing at localhost</li> <li>After you deploy nginx controller, you should see <code>404 Not Found</code></li> </ul> <p>For this example, we will use nginx-ingress: https://kind.sigs.k8s.io/docs/user/ingress/:</p> Text Only<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\n</code></pre> <p>The manifest contains kind-specific patches to forward the hostPorts to the ingress controller, set taint tolerations, and schedule it to the custom labeled node.</p> <p>This output is what is expected when successfully created: </p>Text Only<pre><code>namespace/ingress-nginx created\nserviceaccount/ingress-nginx created\n...\ningressclass.networking.k8s.io/nginx created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created\n</code></pre> <p>Now the ingress is all set up. To ensure that you do not proceed until it is ready to process requests, run:</p> Text Only<pre><code>kubectl wait --namespace ingress-nginx \\\n    --for=condition=ready pod \\\n    --selector=app.kubernetes.io/component=controller \\\n    --timeout=90s\n</code></pre> <p>It\u2019s ready when the output indicates the condition is met.</p> <p>Info</p> <p>If you do not see <code>condition met</code>, it means the pod isn\u2019t healthy. This could be for a variety of reasons (e.g., not able to pull images, out of space); the message that appears instead of <code>condition met</code> should help point toward the problem.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#obtain-certificate-and-create-k8s-secret","title":"Obtain certificate and create K8s secret","text":"<p>Local deployment with no SSL certificate</p> <p>You can deploy locally with no SSL or host domain, although some features and tools may have limited functionality without an SSL. If this is how you will deploy, you do not need to create a certificate. You can skip to Create a minimal values.yaml.</p> <p>Local deployment to a host domain with SSL</p> <p>If you choose to deploy with an SSL certificate and you do not yet have one, follow the instructions in this section. If you already have an SSL certificate, you can skip to Create a minimal values.yaml</p> <p>You will need to have a host site that you own to proceed. Here, our host site is <code>user.dev-site.net</code>.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#certbot-to-generate-a-certificate","title":"Certbot to generate a certificate","text":"<p>A certificate can be generated using Certbot. You can look here to get the specific installation instructions for your environment: https://certbot.eff.org/instructions</p> <p>Once installed, you can run a commmand like this to generate a certificate:</p> Text Only<pre><code>sudo certbot certonly --manual --preferred-challenges=dns -d user.dev-site.net\n</code></pre> <p>Here is the expected output from this command. Note that it will ask you to create a DNS TXT record to verify domain ownership - don\u2019t press Enter until you have verified that the DNS TXT record is published (see below)</p> Text Only<pre><code>(base) saravolkdegarcia@Saras-MacBook-Pro ~ % sudo certbot certonly --manual --preferred-challenges=dns -d user.dev-site.net\nPassword:\nSaving debug log to /var/log/letsencrypt/letsencrypt.log\nEnter email address (used for urgent renewal and security notices)\n (Enter 'c' to cancel): s\u2026.edu\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nPlease read the Terms of Service at\nhttps://letsencrypt.org/documents/LE-SA-v1.4-April-3-2024.pdf. You must agree in\norder to register with the ACME server. Do you agree?\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n(Y)es/(N)o: y\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nWould you be willing, once your first certificate is successfully issued, to\nshare your email address with the Electronic Frontier Foundation, a founding\npartner of the Let's Encrypt project and the non-profit organization that\ndevelops Certbot? We'd like to send you email about our work encrypting the web,\nEFF news, campaigns, and ways to support digital freedom.\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n(Y)es/(N)o: n\nAccount registered.\nRequesting a certificate for user.dev-site.net\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nPlease deploy a DNS TXT record under the name:\n_acme-challenge.user.dev-site.net.\nwith the following value:\n0LiInLyaHawLgZHfRN_OA_oFUa6QhdtkDQo6P-47lkE\nBefore continuing, verify the TXT record has been deployed. Depending on the DNS\nprovider, this may take some time, from a few seconds to multiple minutes. You can\ncheck if it has finished deploying with aid of online tools, such as the Google\nAdmin Toolbox: https://toolbox.googleapps.com/apps/dig/#TXT/_acme-challenge.user.dev-site.net.\nLook for one or more bolded line(s) below the line ';ANSWER'. It should show the\nvalue(s) you've just added.\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nPress Enter to Continue\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#deploy-the-dns-txt-record","title":"Deploy the DNS TXT record","text":"<p>The details for this will vary depending on your DNS hosting provider. A general overview can be found here: https://www.id123.io/knowledgebase/adding-a-txt-record-to-a-dns-server/</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#verify-the-dns-txt-record-has-been-deployed","title":"Verify the DNS TXT record has been deployed","text":"<p>After you have deployed the DNS TXT record, you can use any DNS lookup tool (e.g.,  https://mxtoolbox.com/SuperTool.aspx) to verify that it has been deployed. To verify deployment, copy/paste the name of the DNS record from the challenge request, then click the dropdown to select TXT Lookup. Then, click the TXT Lookup button.</p> <p></p> <p>You should see that it is able to find the record and it matches what was specified in the challenge request (see below). If you don\u2019t see it when you check, try again in a couple minutes.</p> <p></p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#trigger-the-challenge-verification","title":"Trigger the challenge verification","text":"<p>Once you verify that you can see the record, then press Enter in your terminal to continue. It will finish issuing the certificate, and it will show you the paths at which the certificate and key are saved:</p> Text Only<pre><code>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nPress Enter to Continue\nSuccessfully received certificate.\nCertificate is saved at: /etc/letsencrypt/live/user.dev-site.net/fullchain.pem\nKey is saved at:         /etc/letsencrypt/live/user.dev-site.net/privkey.pem\nThis certificate expires on 2024-12-31.\nThese files will be updated when the certificate renews.\nNEXT STEPS:\n- This certificate will not be renewed automatically. Autorenewal of --manual certificates requires the use of an authentication hook script (--manual-auth-hook) but one was not provided. To renew this certificate, repeat this same certbot command before the certificate's expiry date.\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#create-a-minimal-valuesyaml","title":"Create a minimal values.yaml","text":"<p>Once you have a certificate, you can use this to create a minimal Gen3 values.yaml for Helm deployment. This provides any values for the Gen3 umbrella chart that are different than the default configuration. You can create this file in your preferred IDE or text editor. Save it as \u201cvalues.yaml\u201d</p> <p>Local deployment with no SSL certificate</p> <p>If you are deploying locally and you are deploying with all the default values configured in the Helm Gen3 repo, you do not need to have a Gen3 values.yaml at this stage. When you are ready to override any default values, you can create a Gen3 values.yaml at that point.</p> <p>Local deployment to a host domain with SSL</p> <p>You will need a Gen3 values.yaml to provide your SSL certificate and point to your host site. Use the instructions in this sections to create one.</p> <p>A minimal values.yaml will have a <code>global</code> section with nested <code>hostname</code> and <code>tls</code> sections. (I have elided most of the key and certificate body, but left the structure visible so you can see how it was organized.) Note that there is one private key, but two certificates in my example. You may have more or fewer certificates; be sure you include all certificates from your output. (These should have all the certificates in the SSL certificate chain.)</p> Text Only<pre><code>global:\nhostname: user.dev-site.net\n\n    tls:\n        key: |\n        -----BEGIN PRIVATE KEY-----\n        M...W\n        -----END PRIVATE KEY-----\n    cert: |\n        -----BEGIN CERTIFICATE-----\n        MII...jk=\n        -----END CERTIFICATE-----\n        -----BEGIN CERTIFICATE-----\n        MIIEV...yH04=\n        -----END CERTIFICATE-----\n</code></pre> <p>You can get the body of the key and the certificates using the <code>sudo cat</code> command with the path to your key and certificates, as indicated in the output at the end of the certificate creation:</p> <p></p>Text Only<pre><code>sudo cat /etc/letsencrypt/live/user.dev-site.net/fullchain.pem\nsudo cat /etc/letsencrypt/live/user.dev-site.net/privkey.pem\n</code></pre> The output from these <code>cat</code> commands will be the key and certificate values that you need to add to the values.yaml.  <p>Security Note</p> <p>Remember to keep your key private! Protect the values.yaml from public view.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#adding-to-the-gen3-valuesyaml-later","title":"Adding to the Gen3 values.yaml later","text":"<p>Deploying initially with a minimal values.yaml will allow you to limit the number of places you need to troubleshoot if your Gen3 deployment has problems. It is much easier to troubleshoot problems if there are fewer variables that could be the problem. Proceed with your first deployment using only the minimal values.yaml (or no Gen3 values.yaml at all, if deploying without SSL).</p> <p>However, once you know you have your instance up, you will want to expand the values.yaml to customize your instance. There is information at the end of this tutorial that will help you customize your values.yaml.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#deploy-gen3-with-helm","title":"Deploy Gen3 with Helm","text":"<p>Add the Gen3 Helm repo locally to your Helm installation:</p> Text Only<pre><code>helm repo add gen3 https://helm.gen3.org\n</code></pre> <p>Info</p> <p>if you already had this repo added to your Helm installation, you can run <code>helm repo update</code> to pull in any changes since you added it.</p> <p>Deploy Gen3 using your values.yaml with the command below.</p> <ul> <li>Where <code>dev</code> is what we\u2019re calling our release</li> <li><code>gen3/gen3</code> means the chart we\u2019re deploying is <code>gen3</code> (after slash) from the helm repo called <code>gen3</code> (before slash)</li> <li>If you have no Gen3 values.yaml, remove the <code>-f values.yaml</code> part</li> <li>If the values.yaml is named differently, update the name</li> <li>If the values.yaml is located somewhere other than the directory you are in for your terminal, change directories to where the values.yaml is before running the command, or else include the path to the values.yaml from the current directory</li> </ul> Text Only<pre><code>helm upgrade --install dev gen3/gen3 -f values.yaml\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#use-k9s-to-monitor-progress-of-deployment","title":"Use K9s to monitor progress of deployment","text":"<p>Note: Ctrl+C will allow you to exit K9s and return to the terminal.</p> <p>Type <code>k9s</code> into your terminal to start K9s and let you see the pods as they come up.</p> <p></p> <p>At the start, some services are not running because they\u2019re waiting for their databases to be created. You\u2019ll see, for example \u201carborist-dbcreate\u201d complete before \u201carborist-deployment\u201d can start. Your Gen3 deployment is ready when all the pods in the status column have the \u201cRunning\u201d or \u201cComplete\u201d status.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#troubleshooting-pods-that-are-stuck-in-pending","title":"Troubleshooting pods that are stuck in Pending","text":"<p>You can use the arrow keys to navigate through the list to select a particular pod to examine. For example, our portal-deployment pod is stuck in \u201cpending\u201d. Use the arrow keys to highlight the portal pod, and then you can type \u201cd\u201d to \u201cdescribe\u201d the pod.</p> <p></p> <p></p> <p>In the Describe screen, you can scroll up or down to view more details with the arrow keys. Here, we scroll down to bottom to look at Events. We see that there is a message about Insufficient memory.</p> <p></p> <p>Scrolling up, we see that it is requesting 4 Gi:</p> <p></p> <p>To solve this problem, there are a couple of options:</p> <p>First, we can go to Docker Desktop and give the container more memory. But since it currently has 8 Gi, that seems like it should be enough.</p> <p></p> <p>So instead, we can add to the minimal values.yaml to tell the portal to request less memory by using a pre-built image (which will require fewer resources from your computer). <code>portal</code> should be on the same level of indent as <code>global</code>.</p> <p>Info</p> <p>The pre-built portal image is fine for development work. However, you cannot use this in production because you cannot change the data dictionary used for it. </p> Text Only<pre><code>global: ...\n\nportal:\n    resources:\n        requests:\n        cpu: \"0.2\"\n            memory: 100Mi\n    image:\n        repository: quay.io/cdis/data-portal-prebuilt\n        tag: dev\n</code></pre> <p>After saving the changes to your values.yaml, return to your terminal window. You can use Ctrl+C to get out of k9s and return to the terminal. Re-run the command below to restart (will restart only the changed pods).</p> Text Only<pre><code>helm upgrade --install dev gen3/gen3 -f values.yaml\n</code></pre> <p>Here, if you look again in k9s, you will see another portal pod coming up (the lower one). Once it is healthy, the old pod (the upper one) will just be deleted.</p> <p></p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#point-your-domain-to-localhost","title":"Point your domain to localhost","text":"<p>Local deployment with no SSL certificate</p> <p>If you are deploying directly to localhost with no SSL, you can skip to View Your New Deployment.</p> <p>Local deployment to a host domain with SSL</p> <p>Follow the instructions below to point your host site to the localhost.</p> <p>In your terminal, run the following command:</p> Text Only<pre><code>kubectl get ingress\n</code></pre> <p>Here, you want to look in the output for something in the address. If you don\u2019t have anything under <code>ADDRESS</code>, something may be wrong. In our output (below), we do see \u201clocalhost\u201d, so it looks good. (If there is not anything, use K9s to explore what could be wrong.)</p> Text Only<pre><code>NAME           CLASS    HOSTS                    ADDRESS     PORTS     AGE\nrevproxy-dev   &lt;none&gt;   user.dev-site.net   localhost   80, 443   6m41s\n</code></pre> <p>Now, we need to point the user.dev-site.net domain to localhost. To do that, we need to update the file <code>/etc/hosts</code>. You can use Nano or Vi or whatever your preferred command-line text editor is - here, we\u2019re using Nano. Run this command from inside the Helm directory:</p> Text Only<pre><code>sudo nano /etc/hosts\n</code></pre> <p>This will open the file in the text editor. You will always have 127.0.0.1 for localhost as the first line. Create a new line after that to make an entry for our host site. Since we\u2019re running this locally, we will use the IP for our localhost. But note: if you\u2019re running this in a remote host, that should be the IP address of the remote host.</p> <p></p> <p>Save your file changes and return to your terminal.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#view-your-new-deployment","title":"View your new deployment!","text":"<p>Type your host site name (<code>localhost</code> if you deployed without an SSL) in your browser, and view your newly-deployed Gen3!</p> <p></p> <p>Info</p> <p>If you deploy with the minimal values.yaml described above, your Gen3 instance will deploy, and it will show a Google login button, but you will not be able to log in. This is expected. You will get the authorization error below. See the next section for more information about adding login options.</p> <p></p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#customizing-your-gen3-deployment","title":"Customizing your Gen3 deployment","text":"<p>Now that you have successfully deployed your minimal Gen3 instance, you can customize the deployment away from the default options.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#adding-to-the-gen3-valuesyaml","title":"Adding to the Gen3 values.yaml","text":""},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#how-to-deploy-updates-to-the-valuesyaml","title":"How to deploy updates to the values.yaml","text":"<p>After saving any changes to your values.yaml, return to your terminal window. (If necessary, use Ctrl+C to get out of K9s and return to the terminal.) Re-run the command below to restart your Gen3 with the new values.yaml. You can monitor the progress of the new deployment</p> Text Only<pre><code>helm upgrade --install dev gen3/gen3 -f values.yaml\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#adding-login-options-to-your-deployment","title":"Adding login options to your deployment","text":"<p>Note: If you deploy with the minimal values.yaml, your Gen3 instance will deploy, and it will show a Google login button, but you will not be able to log in. Instead, you will get an authorization error. Below are several options for adding login capabilities to your Gen3 deployment.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#mock-authorization-for-development-only","title":"Mock authorization (for development only)","text":"<p>To deploy an instance that allows a mock authorization, add these Arborist and Fence config sections to the minimal values.yaml described above</p> <p>Warning</p> <p>Mock authorization should only be configured for development or testing purposes - do not use this in production.</p> Text Only<pre><code>global:\nhostname: user.dev-site.net\n\n    tls:\n    [key and cert info]\n\n    # Deploy postgres/elasticsearch in same deployment for development purposes.\n    dev: true\n\narborist:\n    enabled: true\n\nfence:\n    FENCE_CONFIG:\n        # if true, will bypass OIDC login, and login a user with username \"test\"\n        # WARNING: DO NOT ENABLE IN PRODUCTION (for testing purposes only)\n        MOCK_AUTH: true\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#google-login","title":"Google login","text":"<p>To deploy an instance that will allow you to log in with Google, see here: https://github.com/uc-cdis/gen3-helm?tab=readme-ov-file#google-login-generation</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-example/#other-customization-options-in-the-gen3-valuesyaml","title":"Other customization options in the Gen3 values.yaml","text":"<p>An important advantage to using Helm to manage Gen3 deployment is that, in the absence of any customization, Helm will use reasonable default options. This is why the minimal yaml works to permit successful deployment; if there is not a specific selection made about a Gen3 feature, Helm will use a built-in default.</p> <p>To customize your Gen3 deployment, you will override the default options by defining other values to use. You can see many examples of different customization options in our example Gen3 values.yaml here</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-overview/","title":"Deployment Overview","text":""},{"location":"gen3-resources/operator-guide/helm/helm-deploy-overview/#deployment-overview","title":"Deployment Overview","text":"<ol> <li> <p>Install Helm: Ensure you have Helm installed on your local machine. You can install Helm by following the instructions provided on the Helm website: Helm Installation Guide.</p> </li> <li> <p>Prepare a Kubernetes Cluster: Make sure you have a Kubernetes cluster up and running with an Ingress Controller configured. You can use a cloud provider's managed Kubernetes service or set up your own Kubernetes cluster using tools like Kubernetes in Docker (KIND) or Minikube</p> <p>Note: We do not recommend Rancher Desktop for deploying Helm; we have sometimes seen issues with PostgreSQL while using Rancher. Instead, we recommend KIND or Minikube, which are lightweight and appropriate for dev setup. </p> </li> <li> <p>Database Services Configuration:</p> <ul> <li>Determine where you want to run the database services (Elasticsearch and PostgreSQL).</li> <li>For a development environment, you can set <code>global.dev</code> to <code>true</code> in your configuration. In this mode, Gen3 will deploy these services with minimal persistence and resource consumption.</li> <li>For non-development environments (such as production), it is recommended to run these services externally from the Gen3 Helm charts. You will need to configure these services separately.</li> </ul> </li> <li> <p>Prepare a <code>values.yaml</code> Configuration File:</p> <ul> <li>Create a <code>values.yaml</code> file to customize the Gen3 deployment. This file will contain various configuration settings for your Gen3 deployment.</li> <li>Refer to the <code>Configuration services with Helm</code> section (see navigation menu on the left) for a full list of configurations for each Gen3 service. Customize the configuration according to your requirements.</li> </ul> </li> <li> <p>Prepare SSL Certificate:</p> <ul> <li>Obtain an SSL/TLS certificate for securing your Gen3 deployment. You can use a certificate authority (CA) or use Let's Encrypt with Certbot for free certificates.</li> <li> <p>Ensure you have a valid domain name for your Gen3 deployment.</p> <p>A certificate can be created using certbot. It will ask you to create a DNS TXT record to verify domain ownership.</p> Text Only<pre><code>sudo certbot certonly --manual --preferred-challenges=dns -d fairtox.com\n</code></pre> <p>Complete the DNS challenge, wait for DNS (1-5 min) and then click continue.</p> <p>Once you have the certificate, create a Kubernetes secret with it.</p> Text Only<pre><code>kubectl create secret tls &lt;secret-name&gt; --cert=&lt;path-to-certificate.pem&gt; --key=&lt;path-to-key.pem&gt;\n</code></pre> <p>We will use this secret later on in our deployment.</p> </li> </ul> </li> <li> <p>Deployment with Helm:</p> <ul> <li>Deploy Gen3 using Helm. Use the following command, replacing <code>[RELEASE_NAME]</code> with your desired Helm release name and <code>[VALUES_FILE]</code> with the path to your <code>values.yaml</code> file:</li> </ul> Bash<pre><code>helm install [RELEASE_NAME] gen3/gen3 -f [VALUES_FILE]\n</code></pre> </li> </ol>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-production-example/","title":"Example - Production Deployment","text":""},{"location":"gen3-resources/operator-guide/helm/helm-deploy-production-example/#tutorial-for-production-deployment","title":"Tutorial for Production Deployment","text":"<p>This guide walks you through deploying Gen3 in a production environment on AWS using Infrastructure as Code (IaC), Kubernetes, and GitOps best practices. This approach ensures a robust, scalable, and repeatable deployment process.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-production-example/#key-benefits","title":"Key Benefits","text":"<ul> <li>Infrastructure as Code (IaC)<ul> <li>Efficiency: Streamlines and automates infrastructure provisioning, eliminating error-prone manual steps.</li> <li>Reproducibility: Easily create consistent environments, reducing deployment inconsistencies.</li> <li>Scalability: Quickly scale up or down to meet demand, adapting to changing requirements.</li> </ul> </li> <li>Kubernetes<ul> <li>Robust Platform: Leverages Kubernetes for container orchestration, providing scalability and resilience for your Gen3 applications.</li> <li>Extensive Ecosystem: Tap into a vast array of tools and resources to customize and extend your Kubernetes deployment.</li> </ul> </li> <li>GitOps<ul> <li>Version Control: Manage your infrastructure and application configurations in Git for better tracking and rollback capabilities.</li> <li>Automation: Automate deployments and updates, reducing manual intervention and risk of errors.</li> </ul> </li> <li>Secrets Management<ul> <li>Security: Securely store and manage sensitive credentials (e.g., database passwords, API keys) using AWS Secrets Manager.</li> <li>Seamless Integration: Easily retrieve secrets from within your applications, adhering to security best practices.</li> </ul> </li> </ul>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-production-example/#1-admin-vm-jump-box-setup","title":"1. Admin VM (Jump Box) Setup","text":"<p>The Admin VM, also known as a jump box or bastion host, serves as your secure entry point into your production environment. You'll control your Gen3 deployment from this machine, so it's crucial to follow best practices to protect it:</p> <ol> <li>Create a Dedicated EC2 Instance:</li> <li>Launch a new EC2 instance specifically for your Admin VM. Avoid using existing instances or shared resources.</li> <li> <p>Choose an instance type with appropriate resources for your workload (e.g., t3.medium or similar).</p> </li> <li> <p>Security Groups (Firewall):</p> </li> <li>Restrict inbound traffic to the Admin VM:<ul> <li>Allow SSH access (port 22) only from your trusted IP addresses or a specific bastion host security group.</li> <li>Limit other incoming traffic (e.g., RDP) as needed.</li> </ul> </li> <li> <p>Outbound traffic can be more permissive, allowing access to your Gen3 infrastructure components and other necessary AWS services.</p> </li> <li> <p>SSH Key Pair:</p> </li> <li>Create a new SSH key pair specifically for the Admin VM. Avoid using default or shared key pairs.</li> <li> <p>Securely store the private key on your local machine and never share it.</p> </li> <li> <p>OS Hardening:</p> </li> <li>Choose a minimal base operating system (e.g., Ubuntu Server, Amazon Linux 2) and apply updates regularly.</li> <li>Disable unnecessary services and protocols.</li> <li> <p>Enforce strong password policies or, ideally, use SSH key-based authentication exclusively.</p> </li> <li> <p>Additional Security Measures (Recommended):</p> </li> <li>Enable multi-factor authentication (MFA) for SSH access.</li> <li>Use a centralized logging solution to monitor access and activity on the Admin VM.</li> <li>Regularly review and update security groups as needed.</li> <li> <p>Consider implementing intrusion detection or prevention systems (IDS/IPS).</p> </li> <li> <p>Connect to Admin VM:</p> </li> <li>Use SSH from your local machine with the private key you created:      Bash<pre><code>ssh -i your_private_key.pem ec2-user@&lt;your_admin_vm_public_ip&gt;\n</code></pre></li> </ol>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-production-example/#2-admin-vm-software-installation","title":"2. Admin VM Software Installation","text":"<p>Install the following tools on your Admin VM:</p> <ol> <li>AWS CLI: Pre-installed on most Amazon Linux and Ubuntu instances. If not, follow this guide to install and configure it.</li> <li>Terraform: Install Terraform using this guide.</li> <li>kubectl:  Install kubectl following the instructions here.</li> <li>Helm: Install Helm using this guide.</li> <li>k9s (Optional): Install k9s herefor a terminal-based UI for your Kubernetes cluster.</li> </ol>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-production-example/#3-infrastructure-deployment-with-terraform","title":"3. Infrastructure Deployment with Terraform","text":"<p>Use the provided Terraform module to create your infrastructure:</p> Terraform<pre><code># ... (Your Terraform configuration)\n</code></pre> <p>Customize: Adjust the Terraform variables to match your desired configuration. Plan &amp; Apply: Run terraform plan to preview the changes and terraform apply to create the infrastructure.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-production-example/#4-gitops-repository-structure-for-helm-charts-and-terraform-outputs","title":"4. GitOps Repository Structure for Helm Charts and Terraform Outputs","text":"<p>This section outlines the steps to create a GitOps repository structure where you can upload a values.yaml file generated by Terraform and manage multiple Helm charts for different environments (referred to as \"commons\").</p> <p>Please see an example of a gen3 gitops repository here: https://github.com/uc-cdis/gitops-example/tree/master</p>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-production-example/#repository-structure","title":"Repository Structure:","text":"<p>The repository will have the following structure:</p> Text Only<pre><code>gitops-repo/\n\u251c\u2500\u2500 commons1/\n\u2502   \u251c\u2500\u2500 Chart.yaml\n\u2502   \u251c\u2500\u2500 values/\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 values.yaml (Terraform output)\n\u2502   \u251c\u2500\u2500 templates/\n\u2502   \u2502   \u2514\u2500\u2500 app.yaml\n\u251c\u2500\u2500 commons2/\n\u2502   \u251c\u2500\u2500 Chart.yaml\n\u2502   \u251c\u2500\u2500 values/\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 values.yaml (Terraform output)\n\u2502   \u251c\u2500\u2500 templates/\n\u2502   \u2502   \u2514\u2500\u2500 app.yaml\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-deploy-production-example/#step-by-step-instructions","title":"Step-by-Step Instructions:","text":"<ol> <li>Create the GitOps Repository</li> <li>Initialize a new Git repository:     Text Only<pre><code>git init gitops-gen3\ncd gitops-gen3\n</code></pre></li> <li>Create a directory for each \"common\" (environment):     Text Only<pre><code>mkdir common1 common2\n</code></pre></li> <li>Setting Up the Common Directory    For each common directory (e.g., common1), create the following structure:    Text Only<pre><code>touch common1/Chart.yaml\n</code></pre>    Example <code>Chart.yaml</code> content:    Text Only<pre><code> apiVersion: v2\n name: common1.org\n description: common1.org argo application\n\n type: application\n\n # This is the chart version. This version number should be incremented each time you make changes\n # to the chart and its templates, including the app version.\n # Versions are expected to follow Semantic Versioning (https://semver.org/)\n version: 0.1.0\n\n # This is the version number of the application being deployed. This version number should be\n # incremented each time you make changes to the application. Versions are not expected to\n # follow Semantic Versioning. They should reflect the version the application is using.\n appVersion: \"1.0\"\n</code></pre></li> <li>Create a values folder to organize values files:    Text Only<pre><code>mkdir common1/values\n</code></pre>    Place the main Terraform output file and additional values files in this folder:    Text Only<pre><code>touch common1/values/values.yaml\ntouch common1/values/fence.yaml\ntouch common1/values/guppy.yaml\netc...\n</code></pre></li> <li> <p>Create a templates folder to house the Argocd application file:     </p>Text Only<pre><code>mkdir common1/templates\ntouch common1/templates/app.yaml\n</code></pre>     Example app.yaml content:     Text Only<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\nname: gen3-commons1\nnamespace: argocd\nfinalizers:\n- resources-finalizer.argocd.argoproj.io\nspec:\nproject: default\nsources:\n    - path: helm/gen3\n    repoURL: https://github.com/uc-cdis/gen3-helm\n    targetRevision: master\n    helm:\n        releaseName: commons1\n        valueFiles:\n        - $values/commons1.org/values/values.yaml\n        - $values/commons1.org/values/fence.yaml\n        - $values/commons1.org/values/portal.yaml\n        - $values/commons1.org/values/guppy.yaml\n        - $values/commons1.org/values/hatchery.yaml\n        - $values/commons1.org/values/etl.yaml\n    - repoURL: 'https://github.com/uc-cdis/gen3-gitops.git'\n    targetRevision: master\n    ref: values\ndestination:\n    server: \"https://kubernetes.default.svc\"\n    namespace: default\nsyncPolicy:\n    syncOptions:\n    - CreateNamespace=true\n    automated:\n    selfHeal: true\n</code></pre> </li> <li> <p>Commit and Push to Repository    PLEASE NOTE!:    It is crucial to ensure that sensitive information, such as secret access keys, database passwords, and any other confidential data, is never uploaded to GitHub. This helps prevent unauthorized access and potential security breaches.</p> </li> </ol> <p>To securely manage sensitive data, we have incorporated external secrets into our Helm charts. Users can utilize this feature to safely handle and store their sensitive information.</p> <p>For more details on managing sensitive data using external secrets, please refer to our External Secrets Operator documentation HERE.</p> Text Only<pre><code>Add and commit your changes:\n```\ngit add .\ngit commit -m \"Initial commit with common1 structure and Terraform output\"\n```\nPush to your remote repository:\n```\ngit remote add origin &lt;remote-repository-url&gt;\ngit push -u origin main\n```\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-config/","title":"Configuring Gen3 Microservices for Helm Deployment","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/#configuring-gen3-microservices-for-helm-deployment","title":"Configuring Gen3 Microservices for Helm Deployment","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/#gen3-is-modular","title":"Gen3 is Modular","text":"<p>Gen3 is a modular system that offers many microservices that allow you to customize your Gen3 instance. Many services are not required for a functional Gen3 instance; the services you need depend on how you plan to use your Gen3 instance.</p> <p>Regardless of use case, all Gen3 instances must have these \"framework services\":</p> <ul> <li>Service to authorize users to see data in the instance (Fence and Arborist)</li> <li>Service to index data files in the instance (Indexd)</li> <li>Service to manage metadata for finding data files (Sheepdog and/or Metadata Service, or other external services you may choose to use to extend Gen3)</li> </ul> <p>The typical use case is that a Gen3 operator wants to set up a Gen3 data commons to share their data. However, there are many other use cases possible because Gen3 is modular and highly configurable (e.g., Gen3 can be used as a data mesh connecting to external data commons).</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#how-to-approach-configuration-in-helm","title":"How to Approach Configuration in Helm","text":"<p>A big advantage of deploying Gen3 with Helm is that it comes preconfigured with many reasonable default settings. That means that you can deploy a minimally-configured \"fully default\" Gen3 instance locally and ensure that all the foundational steps for deployment are working properly, or minimize the places you need to troubleshoot if there are any problems.</p> <p>However, once you have the \"default Gen3\" deployment working, you will want to add your data and configure it so that it reflects the features and appearance you envision.</p> <p>We recommend approaching configuration in a step-by-step approach to minimize the places you may need to troubleshoot if there are problems. Below, we have described such an approach to configuring your Gen3 instance.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#first-deploy-the-minimally-configured-fully-default-gen3","title":"First, deploy the minimally-configured \"fully default\" Gen3","text":"<p>For instructions to do this, see our example Helm deployment. Once you deploy this, you will have a basic default Gen3 portal that will help make sure that your foundational Helm installation and Kubernetes tools are working as expected. Having a functional portal will also help you see what your configurations are working as you continue to configure other services.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#global-block-in-gen3-valuesyaml","title":"Global block in Gen3 values.yaml","text":"<p>The global block of the Gen3 values.yaml is not for a specific service. Instead, this is a place to declare variables that are relevant for many different services. This saves you from needing to repeatedly define these variables for each service where they are required.</p> <p>Some variables that are often configured in the global block are:</p> <ul> <li><code>hostname</code></li> <li><code>revproxyArn</code></li> <li><code>dictionaryUrl</code></li> <li><code>externalSecrets</code></li> <li><code>aws</code></li> </ul> <p>You can see the default values for the global block here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#configure-first-authnauthz","title":"Configure first: AuthN/AuthZ","text":"<p>Fence is the authentication (AuthN) service. It authenticates on the <code>/login</code> endpoint, and also creates presigned URLs in the presigned-url-fence pods. You can find information about configuring Fence here.</p> <p>Fence depends on having a functional user.yaml. A default user.yaml is provided in the values.yaml for the Fence chart. However, it is configured with stand-in information to demonstrate where to add your real email/login and project name values. Therefore, the base user.yaml cannot work as a user.yaml without further configuration in Fence block of your Gen3 values.yaml because some default information is fake. The base user.yaml has blocks that will allow you to grant yourself admin privileges for you to properly use various Gen3 services.</p> <p>If the <code>.Values.usersync.userYamlS3Path</code> string is set to \"none\" (which is what it should be if your user.yaml is not in an S3 bucket), the user.yaml file specified in the Fence values.yaml HERE will be used. Anything you add to the <code>USER_YAML: |</code> section in the Fence block in the Gen3 umbrella values.yaml will override the <code>USER_YAML: |</code> in the Fence values.yaml</p> <p>We have a guide to more extensive configuration of the user.yaml.</p> <p>Arborist is the authorization (AuthZ) service. It works with Fence to assign authorizations to a user based on their authentication information. Information around user authorizations are set within a user.yaml, or telemetry file for dbGaP-authorized users, and put into the Arborist database during the usersync job. You can find information about configuring Arborist here</p> <p>Arborist depends on Fence, so a problem with Fence will cause problems for Arborist.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#data-related-services","title":"Data-related services","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/#configure-indexd","title":"Configure Indexd","text":"<p>Indexd will index files in the commons to be used by Fence to download data. Indexd will assign a GUID (Globally Unique IDentifier) to each file so it can be managed in the data commons. Many data-relevant services depend on Indexd, so it should be functioning before you proceed. The default configuration may be functional for a local deployment for development. You can find information about configuring Indexd here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#data-dictionary-and-data","title":"Data Dictionary and Data","text":"<p>Before proceeding with other service configurations, you will need to have a data dictionary and data to upload.</p> <ul> <li>See our documentation for guidance creating your data dictionary.</li> <li>If you do not yet have data that correlates to the data dictionary you are using, you can create synthetic data based on your data dictionary using our data-simulator tool.</li> </ul> <p>In the Gen3 values.yaml, the path or link to the dictionary file should be the value for the <code>dictionaryUrl</code> field in the <code>global</code> block (see here for an example of the dictionaryUrl field in the default Gen3 values.yaml).</p> <p>Note: if you are creating a data lake for your Gen3 instance, you are not required to have a data dictionary.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#query-page-graph-model-tab","title":"Query page: Graph Model tab","text":"<p>On the Graph Model tab of the Query page, data that has been ingested with Sheepdog can be queried with Peregrine, as long as Guppy is enabled (although Guppy does not need to be configured yet).</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#sheepdog-data-submission","title":"Sheepdog (data submission)","text":"<p>Sheepdog handles data submission. When data files are submitted to a Gen3 Data Commons using Sheepdog, it uses the data dictionary as a schema, and the files are automatically indexed into Indexd. You can find information about configuring Sheepdog here.</p> <p>Sheepdog depends on:</p> <ul> <li>Indexd</li> <li>Fence</li> <li>Sower and ssjdispatcher</li> <li>poetry and Postgres</li> <li>data and a data dictionary</li> </ul>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#peregrine-query-postgres","title":"Peregrine (query Postgres)","text":"<p>Peregrine directly queries data in Postgres. You can find information about configuring Peregrine here.</p> <p>Peregrine depends on:</p> <ul> <li>Sheepdog</li> <li>Fence</li> <li>data in Postgres</li> </ul>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#enable-guppy","title":"Enable Guppy","text":"<p>In the Gen3 values.yaml, by default, Guppy is not enabled. For the Graph Model tab of the Query page to work, Guppy must be enabled (although it does not need to be configured beyond the default values yet).</p> <p>To enable Guppy, you can set <code>enabled: true</code> in the guppy block of the Gen3 values.yaml.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#portal-configuration","title":"Portal configuration","text":"<p>The portal gitops.json should also be configured for things to render properly on the Query page. You can see more about generally configuring the portal here, and you can see the relevant <code>gitops: | json</code> block in the Portal values.yaml here</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#query-page-flat-model-tab","title":"Query page: Flat Model tab","text":"<p>The Flat Model tab of the Query page enables faster search of the data based on pre-selected search fields (these become indices). To get the flattened data, you must identify what fields you want your users to be able to query on (these will also be the fields available on the Explorer page). Along with the data dictionary, these field selections will guide development of the etlMapping.yaml file to describe which which tables and fields to \"ETL\" to ElasticSearch. Tube, the Gen3 ETL service, will use the etlMapping.yaml to run an Extract, Transform, Load (ETL) process on the data in Postgres. Tube will populate ElasticSearch indices to create flattened tables in ElasticSearch (ES).\u00a0Then, the Guppy makes available the ElasticSearch indices created by Tube to quickly traverse the flat data model and find data in ES.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#etlmappingyaml","title":"etlMapping.yaml","text":"<p>The etlMapping.yaml file describes which tables and fields to ETL to ElasticSearch. These are the fields you want to be searchable on the Explorer page or the Flat Model tab of the Query page. You must create an etlMapping.yaml to be able to use Tube or Guppy.</p> <p>Configuring the etlMapping.yaml depends on what users want to search by on the Query page and/or display on the Explorer page. You can read more about configuring an etlMapping.yaml here.</p> <p>The etlMapping.yaml must match the Data Dictionary. It can be validated against the Data Dictionary as described here.</p> <p>You can see the default etlMapping block in the ETL values.yaml here.</p> <p>The etlMapping.yaml depends on the data dictionary.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#tube-etl-elasticsearch","title":"Tube (ETL, ElasticSearch)","text":"<p>The Gen3 Tube ETL is designed to translate data from a graph data model, stored in a PostgreSQL database, to indexed documents in ElasticSearch (ES), which supports efficient ways to query data from the front-end. The purpose of the Gen3 Tube ETL is to create indexed documents to reduce the response time of requests to query data. It is configured through an etlMapping.yaml configuration file, which describes which tables and fields to ETL to ElasticSearch. You can find information about configuring Tube/ETL here.</p> <p>Tube depends on:</p> <ul> <li>Indexd</li> <li>etlMapping.yaml and data dictionary</li> <li>data ingested into Postgres database using the data dictionary as a schema</li> </ul>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#guppy","title":"Guppy","text":"<p>Guppy is used to render the Explorer page and to permit function of the Flat Model tab of the Query page. Guppy makes available the ElasticSearch indices created by Tube to quickly traverse the flat data model and find data in ES. So, after running the ETL, copy the indices into the <code>indices</code> block, as seen here in the guppy values.yaml. You can find information about configuring Guppy here.</p> <p>Guppy depends on:</p> <ul> <li>Tube/ETL - Guppy relies on indices being created by Tube to run. If there are no indices created, Guppy will fail to start up.</li> <li>aws-es-proxy (if deploying Gen3 on AWS) (see configuration info here)</li> </ul>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#portal-configuration_1","title":"Portal configuration","text":"<p>The portal gitops.json should also be configured for things to render properly on the Query page. You can see more about generally configuring the portal here. You must define the guppyConfig; see the example default guppyConfig values here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#explorer-page","title":"Explorer page","text":"<p>The Explorer page is powered by Guppy, so it is already mostly configured once you have finished configuring the Flat Model tab of the Query page. However, there is still some Portal configuration required for it to render properly.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#portal-configuration_2","title":"Portal configuration","text":"<p>The portal gitops.json must have the <code>explorerConfig</code> field configured for the Explorer page to render properly. You can see more about generally configuring the portal here. See the example default dataExplorerConfig values here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#discovery-page","title":"Discovery page","text":"<p>The Discovery page is powered by the Metadata Service. By default, the Discovery page feature is turned off in the portal gitops.json block. To configure the Discovery page, it must be enabled in the portal values.yaml, and the Metadata Service (also called MDS) must be configured and populated.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#portal-enable-the-discovery-page-and-add-the-discoveryconfig","title":"Portal: Enable the Discovery page and add the <code>discoveryConfig</code>","text":"<p>In the <code>portal</code> block of the Gen3 values.yaml, the field gitops.json.featureFlags.discovery should have the value <code>true</code> to enable the Discovery page.</p> <p>To configure the Discovery page, you need to add a <code>discoveryConfig</code> to the gitops.json. Here is an example of <code>discoveryConfig</code> config from a non-Helm deployment. This config is how to edit the table items, advanced search fields, tags, and study page fields (i.e., page that opens up upon clicking on a row). You can see more about generally configuring the portal, including the <code>discoveryConfig</code>, here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#configure-metadata-service","title":"Configure Metadata Service","text":"<p>The Metadata Service (also called MDS) provides an API for retrieving JSON metadata of GUIDs. It is a flexible option for \"semi-structured\" data (key:value mappings). You can find information about configuring Metadata Service here.</p> <p>Discovery page for Gen3 Data Hub Example MDS powering Gen3 Data Hub Discovery page</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#workspaces","title":"Workspaces","text":"<p>Gen3 workspaces use the Ambassador, Hatchery, and Manifestservice services to create and run the workspace in a Gen3 data commons. You can find information about configuring all of these Workspace services here.</p> <p>Workspace services depend on:</p> <ul> <li>Fence</li> <li>Arborist</li> <li>user.yaml (give user who should have access the <code>workspace</code> policy to give them access)</li> </ul>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#ambassador","title":"Ambassador","text":"<p>Ambassador is an envoy proxy. We use this service to proxy traffic toward our workspaces, Hatchery and Jupyter containers.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#hatchery","title":"Hatchery","text":"<p>Hatchery is used to create workspaces. It contains information about workspaces images and resources set around those images to run.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#workspace-token-service","title":"Workspace Token Service","text":"<p>The Gen3 workspace token service acts as an OIDC client which acts on behalf of users to request refresh tokens from Fence. This happens when a user logs into a workspace from the browser. WTS then stores the refresh token for that user, and manages access tokens and refresh tokens for workers that belong to specific users in the workspace.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#manifestservice","title":"Manifestservice","text":"<p>The manifestservice is used by the workspaces to mount files to a workspace. Workspace pods get setup with a sidecar container which will mount files to the data directory. This is used so that users can access files directly on the workspace container. The files pulled are defined by manifests, created through the export to workspace button in the explorer page. These manifests live in an s3 bucket which the manifestservice can query.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#data-commons-notebook-browser","title":"Data Commons Notebook Browser","text":"<p>This is an option if you want to make HTML versions of Jupyter notebooks viewable by commons users. Customize the Notebook Browser page to preview Jupyter Notebooks by adding images, titles, descriptions, and links to the Jupyter Notebook.</p> <ul> <li>Review the code to edit the title (top; notebooks), description (top; notebooks), link, and imageURL (preview image)</li> </ul>"},{"location":"gen3-resources/operator-guide/helm/helm-config/#front-end-portal-configuration-examples","title":"Front End (Portal) Configuration examples","text":"<p>There are different code examples for various aspects of the portal in the document Customize the Front End in the Post-Deployment section. You can see more about generally configuring the portal here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/","title":"AuthN/AuthZ","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#configure-authnauthz-for-helm-deployment","title":"Configure AuthN/AuthZ for Helm Deployment","text":"<p>Authentication (AuthN) and authorization (AuthZ) work together as part of identity and access management (IAM). AuthN is controlled by Fence - it relates to confirming the identity of the user (often through signle sign-on). AuthZ is controlled by Arborist - it determines what an authenticated user can see and do.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#fence-authz","title":"Fence (AuthZ)","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#what-does-it-do","title":"What Does it Do","text":"<p>Fence handles authentication, and is a core service for Gen3 data commons and any other type of Gen3 deployment. It is a required service for a commons to run at all, and will handle authentication on the <code>/login</code> endpoint as well as creating presigned url's in the presigned-url-fence pods.</p> <p>For full functionality in a Gen3 instance, Fence depends on a configured user.yaml unless you enable mock authorization.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#default-settings-for-fence-and-useryaml","title":"Default settings for Fence and user.yaml","text":"<p>If you deploy Helm without customizing any configuration, you can see the default Fence values here.</p> <p>A default user.yaml is provided in the Fence values.yaml. However, it is configured with stand-in information to demonstrate where to add your real email and project name values, so it cannot work as a user.yaml without further configuration because some information is fake. For development work, you can configure Fence to use mock authorization to bypass the need for configuring the user.yaml for</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#how-to-configure-fence","title":"How to configure Fence","text":"<p>For the full set of configuration options, see the Helm README.md for Fence</p> Text Only<pre><code>fence:\n  # Whether or not to deploy the service or not\n  enabled: true\n\n  # What image/ tag to pull\n  image:\n    tag:\n    repository:\n\n  # FENCE_CONFIG\n  FENCE_CONFIG:\n    OPENID_CONNECT:\n      google:\n        client_id: \"insert.google.client_id.here\"\n        client_secret: \"insert.google.client_secret.here\"\n\n  # -- (string) USER YAML. Passed in as a multiline string.\n  USER_YAML: |\n    &lt;the contents of your user.yaml here&gt;\n</code></pre> <p>You need to ensure a proper working fence-config file. Fence is highly configurable and a lot of configuration is commons specific, but some important fields to configure are described in the next section.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#important-fence-config-fields","title":"Important Fence Config fields","text":"<p><code>BASE_URL</code>: This should be (the url of the commons)/user.</p> <p><code>DB</code>: This should contain the psql connection string, which should contain the correct database, user, password and hostname.</p> <p><code>OPENID_CONNECT</code>: This is where different IdP's can be configured. To be able to leverage an IdP as a login option you need to add the client ID's/secrets and any other necesary config to the predefined blocks.</p> <p><code>ENABLED_IDENTITY_PROVIDERS/LOGIN_OPTIONS</code>: Use one of these blocks to enable/configure buttons for logging into the IdP's defined in the <code>OPENID_CONNECT</code> block.</p> <p><code>DEFAULT_LOGIN_IDP</code>/<code>DEFAULT_LOGIN_URL</code>: These blocks will define the default login option, which will be used by most external oidc clients.</p> <p><code>dbGaP</code>: This will be used to connect to an sftp server which will contain telemetry files for usersync. It is necessary for setting up authorizations outside of the useryaml.</p> <p><code>AWS_CREDENTIALS</code>/<code>S3_BUCKETS/DATA_UPLOAD_BUCKET</code>: The AWS_CREDENTIALS block will define credentials for service accounts used to access s3 buckets. The s3 buckets are defined in the S3_BUCKETS block, which will reference a credential in the <code>AWS_CREDENTIALS</code> block. The <code>DATA_UPLOAD_BUCKET</code> block defines the data upload bucket, which is the bucket used in the data upload flow, to upload files to a commons.</p> <p><code>CIRRUS_CFG</code>: If Google buckets are used, you must configure this block. It is used to set up the Google bucket workflow, which essentially creates Google users and Google bucket access groups, which get filled with users and added to bucket policies to allow implicit access to users.</p> <p>For more information about Fence config options, see the config-default.yaml in the Fence repo.</p> <p>You can see examples of Fence configuration overriding defaults in context in the following example Gen3 values.yamls:</p> <ul> <li>aws_dev_values.yaml</li> <li>gke_dev_values.yaml</li> <li>gke_values.yaml</li> </ul>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#how-to-configure-the-useryaml","title":"How to configure the user.yaml","text":"<p>A user.yaml will control access to your data commons. To see how to construct a user.yaml properly:</p> <p>https://github.com/uc-cdis/fence/blob/master/docs/additional_documentation/user.yaml_guide.md</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#fence-pods","title":"Fence Pods","text":"<p>Fence is split into 2 deployments.</p> <ul> <li>There is the regular fence deployment which handles commons authentication.</li> <li>We also split the presigned url feature of fence into a seperate deployment, the presigned-url-fence deployment.</li> </ul> <p>They will both get setup and deployed with a Gen3 installation.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#troubleshooting-fence","title":"Troubleshooting Fence","text":"<p>There are some commons sql queries that can be found here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#mock-authorization-for-development-only","title":"Mock authorization (for development only)","text":"<p>Mock authorization will bypass OIDC login and login a user with username \"test\". To deploy an instance that allows a mock authorization, add these Arborist and Fence config sections to the Gen3 values.yaml</p> <p>Warning</p> <p>Mock authorization should only be configured for development or testing purposes - do not use this in production.</p> Text Only<pre><code>global:\nhostname: [your hostname here]\n\n    tls:\n    [key and cert info]\n\n    # Deploy postgres/elasticsearch in same deployment for development purposes.\n    dev: true\n\narborist:\n    enabled: true\n\nfence:\n    FENCE_CONFIG:\n        # if true, will bypass OIDC login, and login a user with username \"test\"\n        # WARNING: DO NOT ENABLE IN PRODUCTION (for testing purposes only)\n        MOCK_AUTH: true\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#example-google-login","title":"Example: Google login","text":"<p>To deploy an instance that will allow you to log in with Google, see here: https://github.com/uc-cdis/gen3-helm?tab=readme-ov-file#google-login-generation</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#setting-up-oidc-clients","title":"Setting up OIDC clients","text":"<p>OIDC (OpenID Connect) clients allow applications to authenticate with Fence. This setup is often necessary for external users who want to integrate their applications with Gen3. For each application, you'll need to create a unique OIDC client, which will provide a client_id and client_secret for the application to use.</p> <p>Once the client is created, share the client_id and client_secret with the application owner so they can configure their application to authenticate with Fence. To create these clients you will need to exec into a fence container and run the following commands.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#arborist-authn","title":"Arborist (AuthN)","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#what-does-arborist-do","title":"What Does Arborist Do","text":"<p>Arborist is the authorization service. It works with Fence to assign authorizations to a user based on their authentication information. Information around user authorizations are set within a useryaml, or telemetry file for dbgap authorized users, and put into the arborist db during usersync.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#default-settings-for-arborist","title":"Default settings for Arborist","text":"<p>If you deploy Helm without customizing any configuration, you can see the default Arborist values here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#how-to-configure-arborist","title":"How to configure Arborist","text":"<p>For the full set of configuration options, see the Helm README.md for Arborist</p> <p>Some common configuration options include:</p> <p>Postgres configuration</p> Text Only<pre><code># -- (map) To configure postgresql subchart\n# Persistence is disabled by default\npostgresql:\n  primary:\n    persistence:\n      # -- (bool) Option to persist the dbs data.\n      enabled: true\n</code></pre> <p>You can see examples of this configuration in context in the following example Gen3 values.yamls:</p> <ul> <li>aws_dev_values.yaml</li> <li>gke_dev_values.yaml</li> <li>gke_values.yaml</li> </ul> <p>Image repo/ tag</p> Text Only<pre><code>arborist:\n  enabled: true\n\n  # What image/ tag to pull\n  image:\n    tag:\n    repository:\n</code></pre> <p>Common Arborist database SQL queries can be found here. Note: this link is only visible to CTDS employees</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-auth/#relevant-authnauthz-tutorials","title":"Relevant AuthN/AuthZ Tutorials","text":"<p>See the following tutorials for additional information relevant to AuthN/AuthZ.</p> <ul> <li>Fence Usersync CronJob</li> <li>AWS IAM Global User</li> </ul>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/","title":"Data-related Services","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#configure-data-related-services-for-helm-deployment","title":"Configure Data-Related Services for Helm Deployment","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#indexd","title":"Indexd","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#what-does-it-do","title":"What does it do","text":"<p>Indexd is a core service of the commons. It is used to index files within the commons, to be used by Fence to download data.</p> <p>Note: Indexd is used to hold information regarding files in the commons. We can index any files we want, but should ensure that bucket in Indexd are configured within Fence, so that downloading the files will work. To index files, we have a variety of tools. First, data upload will automatically create indexd records for files uploaded. If we want to index files from external buckets, we can also use indexd-utils, or if the commons has dirm setup, create a manifest and upload it to the <code>/indexing</code> endpoint of a commons. From there, GUID's will be created and/or assigned to objects. You can view the information about the records by hitting the <code>(commons url)/index/(GUID)</code> endpoint. To test that the download works for these files, you will want to hit the <code>(commons url)/user/data/download/(GUID)</code> endpoint, while ensuring your user has the proper access to the ACL/AuthZ assigned to the Indexd record.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#default-settings","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default Indexd values in the values.yaml here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#how-to-configure-it","title":"How to configure it","text":"<p>For a full set of configuration see the Helm README.md for Indexd or read the Indexd values.yaml directly.</p> Text Only<pre><code>indexd:\n  enabled: true\n\n  image:\n    repository:\n    tag:\n\n  # default prefix that gets added to all indexd records.\n  defaultPrefix: \"TEST/\"\n\n  # Secrets for fence and sheepdog to use to authenticate with indexd.\n  # If left blank, will be autogenerated.\n  secrets:\n    userdb:\n      fence:\n      sheepdog:\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#sower","title":"Sower","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#what-does-it-do_1","title":"What does it do","text":"<p>Sower is a job dispatching service. Jobs are configured within the manifest, and sower handles dispatching the jobs.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#default-settings_1","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default Sower values in the values.yaml here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#how-to-configure-it_1","title":"How to configure it","text":"<p>For a full set of configuration see the Helm README.md for Sower or read the Sower values.yaml directly.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#sheepdog","title":"Sheepdog","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#what-does-it-do_2","title":"What does it do","text":"<p>Sheepdog is a core service that handles data submission. Data gets submitted to the commons, using the dictionary as a schema, which is reflected within the sheepdog database.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#default-settings_2","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default Sheepdog values in the values.yaml here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#how-to-configure-it_2","title":"How to configure it","text":"<p>For a full set of configuration see the Helm README.md for Sheepdog or read the Sheepdog values.yaml directly.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#peregrine","title":"Peregrine","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#what-does-it-do_3","title":"What does it do","text":"<p>The Peregrine service is used to query data in Postgres. It works similar to Guppy, but relies on querying Postgres directly. It will create the charts on the front page of the commons, as well as the <code>/query</code> endpoint of a commons.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#default-settings_3","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default Peregrine values in the values.yaml here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#how-to-configure-it_3","title":"How to configure it","text":"<p>For a full set of configuration see the Helm README.md for Peregrine or read the Peregrine values.yaml directly.</p> <p>To configure Peregrine, you must have an entry in the versions block. It also requires a dictionary in the global block.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#etl-tube","title":"ETL (Tube)","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#what-does-it-do_4","title":"What does it do","text":"<p>The Gen3 Tube ETL is designed to translate data from a graph data model, stored in a PostgreSQL database, to indexed documents in ElasticSearch (ES), which supports efficient ways to query data from the front-end. The purpose of the Gen3 Tube ETL is to create indexed documents to reduce the response time of requests to query data. It is configured through an etlMapping.yaml configuration file, which describes which tables and fields to ETL to ElasticSearch.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#default-settings_4","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default ETL values in the values.yaml here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#how-to-configure-it_4","title":"How to configure it","text":"<p>For a full set of configuration see the Helm README.md for ETL or read the ETL values.yaml directly.</p> <p>You can configure the ETL like this:</p> Text Only<pre><code>etl:\n  enabled: true\n  esEndpoint: \"\"\n  etlMapping:\n    &lt;your etl mapping here&gt;\n</code></pre> <p>To kick off the ETL job, run this command:</p> Text Only<pre><code>kubectl create job --from=cronjob/etl-cronjob etl\n</code></pre> <p>If you already have a job called etl, run the following. This will delete the old job and create a new instance.</p> Text Only<pre><code>kubectl delete job etl\nkubectl create job --from=cronjob/etl-cronjob etl\n</code></pre> <p>For more information about our ETL, read more in our Tube repo.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#guppy","title":"Guppy","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#what-does-it-do_5","title":"What does it do","text":"<p>Guppy is used to render the Explorer page. It uses Elasticsearch indices to render the page, so it depends on ETL.</p> <p>Note: Guppy relies on indices being created to run; if there are no indices created, Guppy will fail to start up.</p> <p>To create these indices, you can run ETL; however a valid ETL mapping file must be created, and data must be submitted to the commons before you can run ETL.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#default-settings_5","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default Guppy values in the values.yaml here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#how-to-configure-it_5","title":"How to configure it","text":"<p>For a full set of configuration see the Helm README.md for Guppy or read the Guppy values.yaml directly.</p> <p>There is also config that needs to be set within the global block around the tier access level, defining how the explorer page should handle displaying unauthorized files, and the limit to how far unauthroized user can filter down files. Last there is a guppy block that needs to be configured with the elastic search indices guppy will use to render the explorer page.</p> Text Only<pre><code>global:\n  tierAccessLevel: \"(libre|regular|private)\"\n\nguppy:\n  # -- (int) Only relevant if tireAccessLevel is set to \"regular\".\n  # The minimum amount of files unauthorized users can filter down to\n  tierAccessLimit: 1000\n\n  # -- (list) Elasticsearch index configurations\n  indices:\n    - index: dev_case\n      type: case\n    - index: dev_file\n      type: file\n\n  # -- (string) The Elasticsearch configuration index\n  configIndex: dev_case-array-config\n  # -- (string) The field used for access control and authorization filters\n  authFilterField: auth_resource_path\n  # -- (bool) Whether or not to enable encryption for specified fields\n  enableEncryptWhitelist: true\n  # -- (string) A comma-separated list of fields to encrypt\n  encryptWhitelist: test1\n\n\n  # -- (string) Elasticsearch endpoint.\n  # defaults to \"elasticsearch:9200\"\n  esEndpoint: \"\"\n</code></pre> <p>You will also need a mapping file to map the fields you want to pull from postgres into the elasticsearch indices. There are too many fields to describe here, but an example BDC mapping file can be found here.</p> <p>Last, Guppy works closely with Portal to render the Explorer page. You will need to ensure a proper dataExplorer block (see this BDC example) is setup within the gitops.json file, referencing fields that have been pulled from Postgres into the Elasticsearch indices.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#aws-es-proxy","title":"aws-es-proxy","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#what-does-it-do_6","title":"What does it do","text":"<p>aws-es-proxy is a small web server application sitting between Gen3 services and Amazon Elasticsearch service.</p> <p>Note: * This service is only needed when you deploy Gen3 on AWS and use the AWS OpenSearch Service. * This pod can also be used to make direct queries to ElasticSearch. If you know you want to make a manual query to ElasticSearch, you can exec into the aws-es-proxy pod and run the following, filling in the appropriate endpoint you want to hit to query elasticsearch:</p> Text Only<pre><code>kubectl exec -it &lt;aws-es-proxy-pod-name-here&gt; bash\ncurl http://localhost:9200/_cluster/status\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#default-settings_6","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default aws-es-proxy values in the values.yaml here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#how-to-configure-it_6","title":"How to configure it","text":"<p>For a full set of configuration see the Helm README.md for aws-es-proxy or read the aws-es-proxy values.yaml directly.</p> <p>Some important configuration items for aws-es-proxy in helm:</p> Text Only<pre><code># -- AWS user to use to connect to ES\naws-es-proxy:\n  # Whether or not to deploy the service or not\n  enabled: true\n\n  # What image/ tag to pull\n  image:\n    repository:\n    tag:\n\n  # AWS secrets\n  secrets:\n    awsAccessKeyId: \"\"\n    awsSecretAccessKey: \"\"\n\n  # Elasticsearch endpoint in AWS\n  esEndpoint: test.us-east-1.es.amazonaws.com\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#metadata","title":"Metadata","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#what-does-it-do_7","title":"What does it do","text":"<p>The Metadata Service (also called MDS) provides an API for retrieving JSON metadata of GUIDs. It is a flexible option for \"semi-structured\" data (key:value mappings).</p> <p>The GUID (the key) can be any string that is unique within the instance. The value is the metadata associated with the GUID; it is a JSON blob whose structure is not enforced on the server side.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#default-settings_7","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default Metadata values in the values.yaml here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-data-svcs/#how-to-configure-it_7","title":"How to configure it","text":"<p>For a full set of configuration see the Helm README.md for Metadata or read the Metadata values.yaml directly.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-frontend/","title":"Front End","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-frontend/#configure-frontend-for-helm-deployment","title":"Configure Frontend for Helm Deployment","text":"<p>Portal is the front-end service Gen3 currently uses to render the commons webpage. However, we expect to soon offer the Frontend Framework service (not yet available) as the preferred front-end service.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-frontend/#portal","title":"Portal","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-frontend/#what-does-it-do","title":"What Does it Do","text":"<p>Portal is a core service that renders the complete commons webpage, it is the front end service.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-frontend/#default-settings","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default Portal values here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-frontend/#how-to-configure-it","title":"How to Configure it","text":"<p>To configure portal, there must be an entry in the versions block. The portal_app also needs to be defined in the global block. Gitops sets to use the files in the ~/cdis-manifest/(commons url)/ portal directory, dev is the common setup for development environments and there are default gitops.json files for most commons in the data-portal repo that the portal app can be set to.</p> Text Only<pre><code>portal:\n  enabled: true\n\n  gitops:\n    # -- (string) multiline string - gitops.json\n    json: |\n      {}\n    # -- (string) - favicon in base64\n    favicon: \"\"\n    # -- (string) - multiline string - gitops.css\n    css: |\n      /* gitops default css */\n    # -- (string) - logo in base64\n    logo: \"\"\n    # -- (string) - createdby.png - base64\n    createdby: \"\"\n    sponsors:\n</code></pre> <p>You can find more information about portal configuration options here</p> <p>You can also be configure Portal with different images and icons by updating the values; an example from the BDC data commons is here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-frontend/#frontend-framework","title":"Frontend Framework","text":"<p>Note: the Frontend Framework is not yet fully released. You can learn more about it from this Gen3 Community Forum from May 2024.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-frontend/#what-does-it-do_1","title":"What Does it Do","text":"<p>Frontend Framework will be a core service that renders the complete commons webpage; it will replace the Portal as the frontend service.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-frontend/#default-settings_1","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default Frontend Framework values here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-frontend/#how-to-configure-it_1","title":"How to Configure it","text":"<p>More information about this will be provided when the Frontend Framework is released.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-revproxy/","title":"Revproxy","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-revproxy/#configure-revproxy-for-helm-deployment","title":"Configure Revproxy for Helm Deployment","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-revproxy/#revproxy","title":"Revproxy","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-revproxy/#what-does-it-do","title":"What Does it Do","text":"<p>Revproxy is a core service to a commons which handles networking within the Kubernetes cluster.</p> <p>Note: Revproxy is essentially an nginx container, which contains informtation about the endpoints within the cluster. There must be an endpoint set up for Revproxy to be able to send traffic to it and start normally. Because we have many services that may or may not be set up, we only configure Revproxy with the services that are deployed to a commons. The <code>kube-setup-revproxy</code> script will look at current deployments and add configuration files from  here (in the ...) to the pod. So, if a new service is added, you will need to run <code>kube-setup-revproxy</code> to set up the endpoint.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-revproxy/#default-settings","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default Revproxy values in the values.yaml here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-revproxy/#how-to-configure-it","title":"How to configure it","text":"<p>For a full set of configuration see the Helm README.md for Revproxy or read the Revproxy values.yaml directly.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/","title":"Workspaces","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#configure-workspaces-for-helm-deployment","title":"Configure Workspaces for Helm Deployment","text":"<p>Gen3 workspaces are secure data analysis environments in the cloud that can access data objects and metadata from data resources like the data commons or other external resources.</p> <p>By default, Gen3 Workspaces include Jupyter notebooks and RStudio, but can be configured to host many other applications, including analysis workflows, data processing pipelines, or data visualization apps.</p> <p>Gen3 workspaces use the Ambassador, Hatchery, and Manifestservice services to create and run the workspace in a Gen3 data commons.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#ambassador","title":"Ambassador","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#what-does-it-do","title":"What Does it Do","text":"<p>Ambassador is an envoy proxy. We use this service to proxy traffic toward our workspaces, Hatchery and Jupyter containers.</p> <p>Note: Currently, Ambassador is only necessary if there is a Hatchery deployment, as this is used as an envoy proxy primarily for workspaces. This may change in the future.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#default-settings","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default Ambassador values in the values.yaml here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#how-to-configure-it","title":"How to configure it","text":"<p>For a full set of configuration see the Helm README.md for Ambassador or read the Ambassador values.yaml directly.</p> <p>Example configuration using gen3 umbrella chart:</p> Text Only<pre><code>ambassador:\n  # Whether or not to deploy the service or not\n  enabled: true\n\n  # What image/ tag to pull\n  image:\n    repository: quay.io/datawire/ambassador\n    tag: \"1.4.2\"\n    pullPolicy: Always\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#hatchery","title":"Hatchery","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#what-does-it-do_1","title":"What Does it Do","text":"<p>Hatchery is used to create workspaces. It contains information about workspaces images and resources set around those images to run.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#default-settings_1","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default Hatchery values here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#how-to-configure-it_1","title":"How to configure it","text":"<p>For a full set of configuration see the Helm README.md for Hatchery or read the Hatchery values.yaml directly.</p> Text Only<pre><code>hatchery:\n  enabled: true\n  image:\n    repository:\n    tag:\n\n\n  # -- (map) Hatchery sidcar container configuration.\n  hatchery:\n    sidecarContainer:\n      cpu-limit: '0.1'\n      memory-limit: 256Mi\n      image: quay.io/cdis/ecs-ws-sidecar:master\n\n      env:\n        NAMESPACE: \"{{ .Release.Namespace }}\"\n        HOSTNAME: \"{{ .Values.global.hostname }}\"\n\n      args: []\n\n      command:\n      - \"/bin/bash\"\n      - \"./sidecar.sh\"\n\n      lifecycle-pre-stop:\n      - su\n      - \"-c\"\n      - echo test\n      - \"-s\"\n      - \"/bin/sh\"\n      - root\n\n    containers:\n      - target-port: 8888\n        cpu-limit: '1.0'\n        memory-limit: 2Gi\n        name: \"(Tutorials) Example Analysis Jupyter Lab Notebooks\"\n        image: quay.io/cdis/heal-notebooks:combined_tutorials__latest\n        env:\n          FRAME_ANCESTORS: https://{{ .Values.global.hostname }}\n        args:\n        - \"--NotebookApp.base_url=/lw-workspace/proxy/\"\n        - \"--NotebookApp.default_url=/lab\"\n        - \"--NotebookApp.password=''\"\n        - \"--NotebookApp.token=''\"\n        - \"--NotebookApp.shutdown_no_activity_timeout=5400\"\n        - \"--NotebookApp.quit_button=False\"\n        command:\n        - start-notebook.sh\n        path-rewrite: \"/lw-workspace/proxy/\"\n        use-tls: 'false'\n        ready-probe: \"/lw-workspace/proxy/\"\n        lifecycle-post-start:\n        - \"/bin/sh\"\n        - \"-c\"\n        - export IAM=`whoami`; rm -rf /home/$IAM/pd/dockerHome; rm -rf /home/$IAM/pd/lost+found;\n          ln -s /data /home/$IAM/pd/; true\n        user-uid: 1000\n        fs-gid: 100\n        user-volume-location: \"/home/jovyan/pd\"\n        gen3-volume-location: \"/home/jovyan/.gen3\"\n</code></pre>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#workspace-token-service-wts","title":"Workspace Token Service (wts)","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#what-does-it-do_2","title":"What Does it Do","text":"<p>WTS acts as an OIDC client which acts on behalf of users to request refresh tokens from Fence. This happens when a user logs into a workspace from the browser. WTS then stores the refresh token for that user, and manages access tokens and refresh tokens for workers that belong to specific users in the workspace.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#default-settings_2","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default WTS values here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#how-to-configure-it_2","title":"How to configure it","text":"<p>Check out the quick-start guide for WTS with Helm.</p> <p>For a full set of configuration see the Helm README.md for WTS or read the WTS values.yaml directly.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#manifestservice","title":"Manifestservice","text":""},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#what-does-it-do_3","title":"What Does it Do","text":"<p>The manifestservice is used by the workspaces to mount files to a workspace. Workspace pods get setup with a sidecar container which will mount files to the data directory. This is used so that users can access files directly on the workspace container. The files pulled are defined by manifests, created through the export to workspace button in the explorer page. These manifests live in an s3 bucket which the manifestservice can query.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#default-settings_3","title":"Default settings","text":"<p>If you deploy Helm without customizing any configuration, you can see the default Manifestservice values in the values.yaml here.</p>"},{"location":"gen3-resources/operator-guide/helm/helm-config/helm-config-workspaces/#how-to-configure-it_3","title":"How to configure it","text":"<p>For a full set of configuration see the Helm README.md for Manifestservice or read the Manifestservice values.yaml directly.</p> Text Only<pre><code>manifestservice:\n  enabled: true\n\n  manifestserviceG3auto:\n    hostname: testinstall\n    # -- (string) Bucket for the manifestservice to read and write to.\n    bucketName: testbucket\n    # -- (string) Directory name to use within the s3 bucket.\n    prefix: test\n    # -- (string) AWS access key.\n    awsaccesskey: \"\"\n    # -- (string) AWS secret access key.\n    awssecretkey: \"\"\n</code></pre>"},{"location":"gen3-resources/user-guide/","title":"Gen3 User Guide","text":""},{"location":"gen3-resources/user-guide/#gen3-user-guide","title":"Gen3 User Guide","text":"<p>Welcome to the Gen3 User Guide.  This guide is for researchers and data scientists who are accessing a Gen3 Data Commons or mesh to locate, access, or analyze data.  Please refer to your project-specific documentation (if any) for specifics about your system.</p> <p>If you are looking to attain a deeper understanding of Gen3 services in order to update or modify them please read the Gen3 Developer Guide.  If you are looking to deploy, maintain, configure, or submit data please take a look at the Gen3 Operator Guide.</p> <ul> <li>Search for Data - This section includes a walkthrough for how a user can locate data within the Data Portal and by using API or Gen3 SDK tool.</li> <li>Access Data Files - This section includes information on Authentication and Authorization as well as instructions for downloading data files (either externally or from within a Gen3 workspace).</li> <li>Analyze Data - This section covers working with Gen3 workspaces.</li> <li>Data Portal - In this section, we discuss details of each page and how to navigate them in the Gen3 Data Portal.</li> <li>Use the API - In this section, we cover the Gen3 API and how to use it to query and download data and metadata.</li> </ul>"},{"location":"gen3-resources/user-guide/#resources","title":"Resources","text":"<ul> <li>The Gen3 Data Hub is an open-access data commons that can be used as an example system.  It is highlighted throughout the documentation.</li> <li>The Biomedical Research Hub is an example data mesh that is also used to highlight mesh-specific features.</li> </ul>"},{"location":"gen3-resources/user-guide/access-data/","title":"Access Data Files","text":""},{"location":"gen3-resources/user-guide/access-data/#access-data-files","title":"Access Data Files","text":""},{"location":"gen3-resources/user-guide/access-data/#authentication-and-authorization","title":"Authentication and Authorization","text":""},{"location":"gen3-resources/user-guide/access-data/#authentication","title":"Authentication","text":"<p>Authentication refers to how a user identifies themselves to the Gen3 system.  The method of user authentication varies from system to system.  This could include eRA Commons, Google, Microsoft Office 365, InCommons, eduGAIN, ORCID, or generally anything following an OIDC standard.  This is configured by your system operator, and you can find more details in the Gen3 Operator's Guide.</p>"},{"location":"gen3-resources/user-guide/access-data/#authorization","title":"Authorization","text":"<p>Authorization indicates to which data a particular user has access.  Governance practices vary from Gen3 system to system and this can take a variety of forms, but typically falls into two buckets: allow list and dbGaP.   You can find more in-depth details on how this is configured within the Gen3 Operator's Guide.</p>"},{"location":"gen3-resources/user-guide/access-data/#allow-list","title":"Allow list","text":"<p>An allow list is simply a list of users (identified based on your method of authentication) that controls which users have access to which data.  It is in the form of a user.yaml file that is maintained by the operator of your Gen3 system.  You should contact the operator of your system or follow whatever process they have in place to request access.  Gaining access may require you to sign a Data Use Agreement.  Data access is granted at the program or project level.</p>"},{"location":"gen3-resources/user-guide/access-data/#dbgap","title":"dbGaP","text":"<p>Another common authorization mechanism is dbGaP.  In order to obtain access to controlled-access data via dbGaP, PIs must first obtain an NIH eRA Commons account and then obtain authorization to access the data through the NIH database of Genotypes and Phenotypes (dbGaP).</p> <p>To obtain dbGaP access, navigate to the dbGaP Authorized Access site and follow the instructions. This process includes working with your institutional research office, reviewing the consent agreement for the particular project, and writing a Research Use Statement and thus can take a significant amount of time.</p>"},{"location":"gen3-resources/user-guide/access-data/#bulk-allow-list","title":"Bulk allow list","text":"<p>Another option is to use a bulk allow list from an SFTP server in the same format as dbGaP, but not actually controlled by dbGaP.</p>"},{"location":"gen3-resources/user-guide/access-data/#requestor-service","title":"Requestor Service","text":"<p>Operators can also take advantage of the Requestor Service for dynamic authorization. In this case Gen3 interacts with another system where authorization requests are reviewed, approved, denied, or revoked.</p>"},{"location":"gen3-resources/user-guide/access-data/#download-files-using-the-gen3-client","title":"Download Files Using the Gen3-client","text":"<p>The gen3-client provides an easy-to-use, command-line interface for uploading and downloading data files to and from a Gen3 data commons from the terminal or command prompt, respectively.  In some systems \"download\" may be restricted to only within a Gen3 Workspace.  Note that Gen3 also comes with an SDK tool that can perform many of the same functions as the client for downloading along with many other features not found in the client.  You can read more about the Python SDK tool here.</p> <p>This guide has the following sections:</p>"},{"location":"gen3-resources/user-guide/access-data/#installation-instructions","title":"Installation Instructions","text":"<p>A binary executable of the latest version of the gen3-client should be downloaded from Github. Choose the file that matches your operating system (Windows, Linux, or macOS).</p> <p>No installation is necessary. Simply download the correct version for your operating system and unzip the archive. The program is then executed from the command-line by running the command <code>gen3-client &lt;options&gt;</code>. For more detailed instructions, see the section below for your operating system.</p> <p>Note: Do not try to run the program by double-clicking on it. Instead, execute the program from within the shell / terminal / command prompt. The program does not provide a graphical user interface (GUI) at this time; so, commands are sent by typing them into the terminal.</p>"},{"location":"gen3-resources/user-guide/access-data/#mac-os-x-linux-installation-instructions","title":"Mac OS X / Linux Installation Instructions","text":"<ol> <li>Download the latest Mac OS X or Linux version of the gen3-client here.</li> <li>Unzip the archive.</li> <li>Add the unzipped executable to a directory, for example: <code>~/.gen3/gen3-client.exe</code>.</li> <li>Open a terminal window.</li> <li>Add the directory containing the executable to your Path environment variable by entering this command in the terminal: <code>echo 'export PATH=$PATH:~/.gen3' &gt;&gt; ~/.bash_profile</code>.</li> <li>Run <code>source ~/.bash_profile</code> or restart your terminal.</li> <li>Now you can execute the program by opening a terminal window and entering the command <code>gen3-client</code>.</li> </ol>"},{"location":"gen3-resources/user-guide/access-data/#windows-installation-instructions","title":"Windows Installation Instructions","text":"<ol> <li>Download the Windows version of the gen3-client here.</li> <li>Unzip the archive.</li> <li>Add the unzipped executable to a directory, for example: <code>C:\\Program Files\\gen3-client\\gen3-client.exe</code>.</li> <li>Open the Start Menu and type \u201cedit environment variables\u201d.</li> <li>Open the option \u201cEdit the system environment variables\u201d.</li> <li>In the \u201cSystem Properties\u201d window that opens up, on the \u201cAdvanced\u201d tab, click on the \u201cEnvironment Variables\u201d button.</li> <li>In the box labeled \u201cSystem Variables\u201d, find the \u201cPath\u201d variable and click \u201cEdit\u201d.</li> <li>In the window that pops up, click \u201cNew\u201d.</li> <li>Type in the full directory path of the executable file (for example, <code>C:\\Program Files\\gen3-client)</code>.</li> <li>Click \u201cOk\u201d on all the open windows and restart the command prompt if it is already open by entering <code>cmd</code> into the start menu and hitting enter.</li> </ol> <p>Note: To download the latest version of the file from the command-line, use the following commands from your terminal:  </p>Text Only<pre><code># Mac OS:\ncurl https://api.github.com/repos/uc-cdis/cdis-data-client/releases/latest | grep browser_download_url.*osx |  cut -d '\"' -f 4 | wget -qi -\n</code></pre> Text Only<pre><code># Linux:\ncurl https://api.github.com/repos/uc-cdis/cdis-data-client/releases/latest | grep browser_download_url.*linux |  cut -d '\"' -f 4 | wget -qi -\n</code></pre>"},{"location":"gen3-resources/user-guide/access-data/#view-the-help-menu","title":"View the Help Menu","text":"<p>To check that your copy of the client is working and confirm the version, the tool can be run on the command-line in your terminal or command prompt by entering <code>gen3-client</code>. Typing this alone or <code>gen3-client help</code> will display the help menu. For help on a particular command, enter: <code>gen3-client &lt;command&gt; help</code>. Note that you must provide the full path of the tool in order for the commands to run, for example, <code>./gen3-client</code> while working from the directory containing the client. Alternatively, you can add the location of the gen3-client executable to your shell\u2019s PATH environment variable.</p>"},{"location":"gen3-resources/user-guide/access-data/#configure-a-profile-with-credentials","title":"Configure a Profile with Credentials","text":"<p>Before using the gen3-client to upload or download data, the gen3-client needs to be configured with API credentials downloaded from the user\u2019s data commons Profile:</p> <ol> <li>To download the \u201ccredentials.json\u201d from the data commons, the user should start from that common\u2019s Windmill data portal, followed by clicking on \u201cProfile\u201d in the top navigation bar and then creating an API key. In the popup window which informs user an API key has been successfully created, click the \u201cDownload json\u201d button to save a local copy of the API key.</li> </ol> <p></p> <ol> <li>From the command-line, run the <code>gen3-client configure</code> command with the <code>--cred</code>, <code>--apiendpoint</code>, and <code>--profile</code> flags (see examples below).</li> </ol> <p>Example Usage:    </p>Text Only<pre><code>gen3-client configure --profile=&lt;profile_name&gt; --cred=&lt;credentials.json&gt; --apiendpoint=&lt;api_endpoint_url&gt;\n\nMac/Linux:\ngen3-client configure --profile=demo --cred=~/Downloads/demo-credentials.json --apiendpoint=https://gen3.datacommons.io\n\nWindows:\ngen3-client configure --profile=demo --cred=C:\\Users\\demo\\Downloads\\demo-credentials.json --apiendpoint=https://gen3.datacommons.io\n</code></pre> <p>NOTE: For these user guides, https://gen3.datacommons.io is an example URL and can be replaced with the URL of other data commons powered by Gen3.</p> <p>When successfully executed, this will create a configuration file, which contains all the API keys and URLs associated with each commons profile configured, located in the user folder:</p> <p></p>Text Only<pre><code>Version 1.0.0+\nMac/Linux: /Users/demo/.gen3/gen3_client_config.ini\nWindows: C:\\Users\\demo\\.gen3\\gen3_client_config.ini\n</code></pre> Text Only<pre><code>Other older version\nMac/Linux: /Users/demo/.gen3/config\nWindows: C:\\Users\\demo\\.gen3\\config\n</code></pre> <p>NOTE: These keys must be treated like important passwords; never share the contents of the <code>credentials.json</code> and gen3-client <code>gen3_client_config.ini</code> or <code>config</code> file!</p> <p>You should receive an error if you enter an incorrect API endpoint for your credentials. For example:    </p>Text Only<pre><code>~&gt; gen3-client configure --profile=demo --cred=~/Downloads/wrong-credentials.json --apiendpoint=https://nci-crdc-demo.datacommons.io\n2019/11/19 11:58:15 Error occurred when validating profile config: Invalid credentials for apiendpoint 'https://nci-crdc-demo.datacommons.io': check if your credentials are expired or incorrect.\n</code></pre> <p>To confirm you successfully configured a profile with the correct authorization privileges, you can run the <code>gen3-client auth</code> command, which should list your access privileges for each project in the commons you have access to. For example:</p> Text Only<pre><code>~&gt; gen3-client auth --profile=demo\n2019/11/19 11:59:04\nYou have access to the following project(s) at https://nci-crdc-demo.datacommons.io:\n2019/11/19 11:59:04 CPTAC [read read-storage]\n2019/11/19 11:59:04 DCF [create delete read read-storage update upload write-storage]\n</code></pre>"},{"location":"gen3-resources/user-guide/access-data/#download-a-single-data-file-using-a-guid","title":"Download a Single Data File Using a GUID","text":"<p>Files with a valid storage location in the file index database (AKA indexd) can be downloaded using the <code>gen3-client download-single</code> command by providing the file's object_id (AKA GUID or did).</p> <p>For example, the indexd record for object_id \"00149bcf-e057-4ecc-b22d-53648ae0b35f\" points to a location in the GDC.</p> <p>Required Flags: * --profile: The user profile specifying the api-endpoint and credentials. * --guid: The GUID (or \"object_id\" in Postgres or \"did\" in indexd) of the file.</p> <p>Optional Flags: * --download-path: Specify the directory to store files in. * --filename-format: The format of filename to be used, including \"original\", \"guid\" and \"combined\" (default \"original\").</p> <ul> <li>--no-prompt: If set to true, no user prompt message will be displayed regarding the filename-format.</li> <li>--protocol: The protocol to use for file download. Accepted options are: \"s3\", \"http\", \"ftp\", \"https\", and \"gs\".</li> <li>--rename: If \"--filename-format=original\" is used, this will rename files by appending a counter value to its filename when files with the same name are in the download-path, otherwise the original filename will be used.</li> <li>--skip-completed:  If set to true, the name and size of local files in the <code>download-path</code> are compared to the information in the file index database. If a local file in the <code>download-path</code> matches both the name and size, it will not be downloaded.</li> </ul> <p>NOTE: The \"--skip-completed\" option also attempts to resume downloading partially downloaded files using a ranged download. That is, if a local file with the same name exists in the <code>download-path</code>, but the size does not match what is in the file index, the client will attempt to resume the download where it left off.</p> <p>Example Usage:</p> Text Only<pre><code>gen3-client download-single --profile=demo --guid=00149bcf-e057-4ecc-b22d-53648ae0b35f --no-prompt --skip-completed\n</code></pre>"},{"location":"gen3-resources/user-guide/access-data/#multiple-file-download-with-manifest","title":"Multiple File Download with Manifest","text":"<p>A download manifest can be generated using a Gen3 data common's \"Exploration\" tool. To use the \"Exploration\" tool, open the common's Windmill data portal and click on \"Exploration\" in the top navigation bar. After a cohort has been selected, clicking the \"Download Manifest\" button will create the manifest for the selected files. The gen3-client will download all the files in the provided manifest using the <code>gen3-client download-multiple</code> command.</p> <p>NOTE: The download-multiple command supports multi-threaded downloads using the \"--numparallel\" option. While using this option will decrease time to download when downloading a batch of files, it is not recommended to use this option when trying to download extremely large files (50+ GB).</p> <p>NOTE: If a download command is interrupted and results in partially downloaded files, the \"--skip-completed\" option can be used to attempt to resume downloading the partially downloaded files using a ranged download. The gen3-client will compare the file_size and file_name for each file in the \"--download-path\", and resume downloading any files in the manifest that do not match both.</p> <p>Example Usage:</p> Text Only<pre><code>gen3-client download-multiple --profile=&lt;profile_name&gt; --manifest=&lt;manifest_file&gt; --download-path=&lt;path_for_files&gt;\n\ngen3-client download-multiple --profile=demo --manifest=manifest.json --download-path=downloads\n\nFinished downloads/63af95d3-98c3-4d6d-a6be-26398dbfc1d9 6723044 / 6723044 bytes (100%)\nFinished downloads/b30531f6-9caa-4356-a95f-5f4d6a012913 6721797 / 6721797 bytes (100%)\nFinished downloads/fbac9213-3564-422a-8809-119d4401d284 2744320 / 2744320 bytes (100%)\n...\nFinished downloads/bc40b861-c56d-490f-b4a4-f34d3c54de5f 2959360 / 2959360 bytes (100%)\nFinished downloads/24d0be10-d164-48ad-aafa-9fcaac682df9 2570240 / 2570240 bytes (100%)\n330 files downloaded.\n</code></pre>"},{"location":"gen3-resources/user-guide/access-data/#quick-start-for-experienced-users-or-cheat-sheet","title":"Quick Start for Experienced Users or Cheat Sheet","text":""},{"location":"gen3-resources/user-guide/access-data/#mac-os","title":"MAC OS","text":"<ol> <li>Download the latest version of the client: Text Only<pre><code>!curl https://api.github.com/repos/uc-cdis/cdis-data-client/releases/latest | grep browser_download_url.*osx |  cut -d '\"' -f 4 | wget -qi -\n!unzip dataclient_osx.zip\n!mv gen3-client /Users/demo/.gen3\n!rm dataclient_osx.zip`\n</code></pre></li> <li>Configure a profile: Text Only<pre><code>gen3-client configure --profile=demo --cred=~/Downloads/demo-credentials.json --apiendpoint=https://gen3.datacommons.io` \n</code></pre></li> <li>Check your authorization privileges: Text Only<pre><code>gen3-client auth --profile=demo\n</code></pre></li> <li>Upload a file: Text Only<pre><code>gen3-client upload --profile=demo --upload-path=test.txt\n</code></pre></li> <li>Download a file: Text Only<pre><code>gen3-client download-single --profile=demo --guid=39b05d1f-f8a2-478c-a728-c16f6d0d8a7c --no-prompt\n</code></pre></li> </ol>"},{"location":"gen3-resources/user-guide/access-data/#working-from-the-command-line","title":"Working from the Command-line","text":"<p>This section contains some general notes about working from the command-line and includes information on how to set-up your command-line shell to make working with the gen3-client easier.</p>"},{"location":"gen3-resources/user-guide/access-data/#file-paths","title":"File Paths","text":"<p>When you create or download a file on your computer, that file is located in a folder (or directory) in your computer's file system. For example, if you create the text file <code>example.txt</code> in the folder <code>My Documents</code>, the \"full path\" of that file is, for example, <code>C:\\Users\\demo\\My Documents\\example.txt</code> in Windows or <code>/Users/demo/Documents/example.txt</code> in Mac OS X.</p>"},{"location":"gen3-resources/user-guide/access-data/#present-working-directory","title":"Present Working Directory","text":"<p>After opening a shell, command prompt or terminal window, you are \"in\" a folder known as the \"present working directory\". You can change directories with the <code>cd &lt;directory&gt;</code> command in either shell. To view your present working directory, enter the command <code>echo $PWD</code> in a Mac terminal or <code>cd</code> alone in the Windows command prompt.</p> <p>You can list the contents of your present working directory by entering the command <code>ls</code> in the Mac terminal or <code>dir</code> in the Windows command prompt. These files in the present working directory can be accessed by commands you type just by entering their filenames: for example, <code>cat example.txt</code> would print the contents of the file <code>example.txt</code> in the Mac terminal if your present working directory is <code>/Users/demo/Documents</code>. However, if you're in a different directory, you must enter the \"full path\" of the file: for example, if your present working directory is the <code>My Downloads</code> folder instead of <code>My Documents</code>, then you would need to specify the full path of the file and enter the command <code>type \"C:\\Users\\demo\\My Documents\\example.txt\"</code>, to print the file's contents in the Windows command prompt.</p>"},{"location":"gen3-resources/user-guide/access-data/#updating-the-path-environment-variable","title":"Updating the PATH Environment Variable","text":"<p>When working in your shell, you can define variables that help make work easier. One such variable is PATH, which is a list of directories where executable programs are located. By adding a folder to the PATH, programs in that folder can be executed from any other folder/directory regardless of the present working directory.</p> <p>So, by adding the directory containing the gen3-client program to your PATH variable, you can run it from any working directory without specifying the \"full path\" of the program. Simply enter the command <code>gen3-client</code>, and you will run the program.</p> <p>Note: In the case that you haven't properly added the client to your path, the program can still be executed from any directory with the following command: <code>/full/path/to/executable/gen3-client &lt;options&gt;</code>. If you are working in the directory containing the executable, then <code>/full/path/to/executable</code> is simply <code>./</code>. So the command from the executable's directory would be <code>./gen3-client</code>.</p>"},{"location":"gen3-resources/user-guide/access-data/#sending-parameters-to-programs-on-command-line","title":"Sending Parameters to Programs on Command-line","text":"<p>Most programs require some sort of user input to run properly. Some programs will prompt you for input after execution, while other programs are sent this input during execution as \"flags\" (AKA \"arguments\" or \"options\"). The gen3-client uses the latter method of sending user input as command arguments during program execution.</p> <p>For example, when configuring a profile with the client, the user must specify the <code>configure</code> option and also specify the profile name, API endpoint, and credentials file by adding the flags <code>--profile</code>, <code>--apiendpoint</code> and <code>--cred</code> to the end of the command (see configuring a profile section above for specific examples).</p>"},{"location":"gen3-resources/user-guide/access-data/#expired-token","title":"Expired Token","text":"<p>Many commons have a limit to how long a token is good before it is expired.  Once expired you may receive an error such  </p> <p><code>RequestNewAccessToken with error code 401</code></p> <p>If this happens (and you are still authorized to access the data), you can download a new API token and re-create your profile using the previously used command.   </p>"},{"location":"gen3-resources/user-guide/analyze-data/","title":"Analyze Data","text":""},{"location":"gen3-resources/user-guide/analyze-data/#data-analysis-in-a-gen3-data-commons","title":"Data Analysis in a Gen3 Data Commons","text":"<p>The Gen3 platform for creating data commons co-locates data management with analysis workspaces, apps and tools.</p> <p>Workspaces are highly customizable by the operators of a Gen3 data commons and offer a variety of VM images (virtual machines) pre-configured with tools for specific analysis tasks. Custom applications for executing bioinformatics workflows or exploratory analyses may be integrated in the navigation bar as well.</p> <p>The following documentation primarily covers exploratory data analysis in the standard Gen3 Workspace, which can be accessed by clicking the \u201cWorkspace\u201d icon in the top navigation bar or navigating to the /workspace endpoint.</p>"},{"location":"gen3-resources/user-guide/analyze-data/#launch-workspace","title":"Launch Workspace","text":"<p>Click on the \u201cWorkspace\u201d icon or navigate to the /workspace endpoint to see a list of pre-configured virtual machine (VM) images offered by the data commons and click the \u201cLaunch\u201d button to spin-up a VM.  For more detailed instructions on the use of workspaces you can visit the Biomedical Research Hub (BRH) documentation.</p> <p></p> <p>Once connected to a workspace VM, the persistent drive named \u201c/pd\u201d can be seen in the Files tab of either JupyterHub or RStudio. All data, notebooks, or scripts that need to persist in the cloud after workspace termination should be saved in this \u201cpd\u201d drive. When logging out of a workspace, this personal drive is unmounted but saved, so that when launching a new workspace VM, the drive can be mounted again to make the saved work accessible.</p> <p>Jupyter notebooks and other analysis-related files like scripts can be uploaded to JupyterHub by clicking the \u201cupload\u201d button. Make sure to save files to the \u201c/pd\u201d directory if they need to remain accessible after logging out.</p> <p></p> <p>Alternatively, a new notebook can be created by clicking \u201cNew\u201d and then choosing the type of notebook, for example, \u201cPython 3\u201d or \u201cR\u201d.</p> <p></p> <p>JupyterHub supports interactive programming sessions in the Python and R languages. Code blocks are entered in cells, which can be executed individually or all at once. Code documentation and comments can also be entered in cells, and the cell type can be set to support Markdown. Results, including plots, tables, and graphics, can be generated in the workspace and downloaded as files.</p> <p>After editing a Jupyter Notebook, it can be saved in the workspace by clicking the \u201cSave\u201d icon or by clicking \u201cFile\u201d and then clicking \u201cSave and Checkpoint\u201d. Notebooks and files can also be downloaded from the server to a local computer by clicking \u201cFile\u201d then \u201cDownload as\u201d.</p> <p></p> <p>The following clip demonstrates creating a new Jupyter Notebook in the R language.</p> <p></p> <p>Terminal sessions can also be started in the Workspace and used to download other tools.</p> <p></p> <p>You can manage active notebooks and terminal processes by clicking on the tab \u201cRunning\u201d. Click \u201cShutdown\u201d to close the terminal session or Jupyter Notebook. Be sure to save your notebooks before terminating them, and again, ensure any notebooks to be accessed later are saved in the \u201cpd\u201d directory.</p> <p>Important: Always click the \"Terminate Workspace\" or \"Shutdown\" button when finished to release cloud resources</p> <p></p>"},{"location":"gen3-resources/user-guide/analyze-data/#getting-files-into-the-gen3-workspace","title":"Getting Files into the Gen3 Workspace","text":"<p>Bringing in files into the Gen3 Workspace can be achieved via the UI (directly from the Exploration page) or programmatically. Find below a description of both methods.</p> <p>Note: Not every PlanX Data Commons has the function in the UI enabled; users are advised to follow available commons-specific documentation.</p>"},{"location":"gen3-resources/user-guide/analyze-data/#exporting-files-from-the-exploration-tab-to-the-workspace","title":"Exporting Files from the Exploration Tab to the Workspace","text":"<p>The Exploration page allows to search through data and create cohorts, which can be exported to the Workspace.</p> <ul> <li>After a cohort has been selected, the data can be exported to a Workspace by clicking \u201cExport to Workspace\u201d.</li> <li>Do not navigate away from the browser after clicking the button.</li> <li>Allow up to 5 minutes to export your files.</li> <li>A popup window will appear confirming that exporting a \u201cmanifest\u201d to the workspace has been successful.</li> <li>Find the data or data files in the folder \u201cdata\u201d on your persistent drive \u201c/pd\u201d.</li> </ul> <p>Please note: the workspace mounts up to 5 different manifests while the workspace is running, but shows only the latest exported manifest in a newly launched workspace.</p> <p></p>"},{"location":"gen3-resources/user-guide/analyze-data/#getting-files-into-the-workspace-programmatically","title":"Getting Files into the Workspace programmatically","text":"<p>In order to download data files directly and programmatically from a Gen3 data commons into your workspace, install and use the gen3-client in a terminal window from your Workspace. Launch a terminal window by clicking on the \u201cNew\u201d dropdown menu, then click on \u201cTerminal\u201d.  General instructions for use of the Gen3 client can be found here.</p> <p>From the command line, download the latest Linux version of the gen3-client using the <code>wget</code> command. Next, unzip the archive and add it to your path:</p> <p>Example: </p>Bash<pre><code>wget https://github.com/uc-cdis/cdis-data-client/releases/download/2020.11/dataclient_linux.zip\nunzip dataclient_linux.zip\nPATH=$PATH:~/\n</code></pre> Now the gen3-client should be ready to use in the JupyterHub terminal. <p>Other required files, like the <code>credentials.json</code> file which contains API keys needed to configure a profile or a download <code>manifest.json</code> file can be uploaded to the workspace by clicking on the \u201cUpload\u201d button or by dragging and dropping into the \u2018Files\u2019 tab. Text can also be pasted into a file by clicking \u201cNew\u201d, then choosing \u201cText File\u201d. Filenames can be changed by clicking the checkbox next to the file and then clicking the \u201cRename\u201d button that appears.</p> <p></p> <p>Example: </p>Text Only<pre><code>jovyan@jupyter-user:~$ wget https://github.com/uc-cdis/cdis-data-client/releases/download/2020.11/dataclient_linux.zip\n--2020-11-12 19:33:45-- Resolving github-production-release-asset-2e65be.s3.amazonaws.com\nConnecting to github-production-release-asset-2e65be.s3.amazonaws.com\nHTTP request sent, awaiting response... 200 OK\nLength: 7445539 (7.1M) [application/octet-stream]\nSaving to: \u2018dataclient_linux.zip\u2019\ndataclient_linux.zip            100%[=====================================================&gt;]   7.10M  --.-KB/s    in 0.05s\n2020-11-12 19:33:45 (143 MB/s) - \u2018dataclient_linux.zip\u2019 saved [7445539/7445539]\n\n\njovyan@jupyter-user:~$ unzip dataclient_linux.zip\nArchive:  dataclient_linux.zip\n  inflating: gen3-client\n\njovyan@jupyter-user:~$ PATH=$PATH:~/\n\njovyan@jupyter-user:~$ gen3-client configure --apiendpoint=https://gen3.datacommons.io --profile=demo --cred=credentials.json\n2020/11/12 19:43:51 Profile 'demo' has been configured successfully.\n\njovyan@jupyter-user:~$ gen3-client download-single --profile=demo --guid=6e312ac3-874d-4cad-b84b-474aa0209d49 --no-prompt --skip-completed\n2020/11/12 20:06:43 Preparing file info for each file, please wait...\n 1 / 1 [===========================================================================================================] 100.00% 0s\n2020/11/12 20:06:43 File info prepared successfully\npheno_63878_2.txt  78.53 KiB / 78.53 KiB [============================================================================] 100.00%\n2020/11/12 20:06:44 1 files downloaded.\n\njovyan@jupyter-user:~$ mkdir files\n\njovyan@jupyter-user:~$ gen3-client download-multiple --profile=demo --manifest=file-manifest-demo.json --skip-completed\n2020/11/12 22:18:13 Reading manifest...\n 18.17 KiB / 18.17 KiB [====================================================================] 100.00% 0s\n2020/11/12 22:18:18 Total number of GUIDs: 98\n2020/11/12 22:18:18 Preparing file info for each file, please wait...\n 98 / 98 [==================================================================================] 100.00% 3s\n2020/11/12 22:18:21 File info prepared successfully\nGSM1558792_Sample9_3.CEL.gz  4.04 MiB / 4.04 MiB [=============================================] 100.00%\nGSM1558835_Sample31_1.CEL.gz  4.18 MiB / 4.18 MiB [============================================] 100.00%\nGSM1558866_Sample46_3.CEL.gz  4.02 MiB / 4.02 MiB [============================================] 100.00%\nGSM1558804_Sample15_3.CEL.gz  4.19 MiB / 4.19 MiB [============================================] 100.00%\nGSM1558810_Sample18_3.CEL.gz  4.16 MiB / 4.16 MiB [============================================] 100.00%\nGSM1558854_Sample40_3.CEL.gz  4.20 MiB / 4.20 MiB [====================....\n2020/11/12 22:18:55 98 files downloaded.\n\njovyan@jupyter-user:~$  mv *.gz files\n</code></pre>"},{"location":"gen3-resources/user-guide/analyze-data/#working-with-the-proxy-and-allow-lists","title":"Working with the proxy and allow lists","text":""},{"location":"gen3-resources/user-guide/analyze-data/#working-with-the-proxy","title":"Working with the Proxy","text":"<p>To prevent unauthorized traffic, the Gen3 VPC utilizes a proxy server. The proxy server acts as an intermediary between the internal network resources within the VPC and the external internet or other networks. If you are using one of the custom VMs setup, there is already a line in your .bashrc file to handle traffic requests.</p> <p></p>Bash<pre><code>export http_proxy=http://cloud-proxy.internal.io:3128\nexport https_proxy=$http_proxy\n</code></pre> Alternatively, if you have a different service or a tool that needs to call out, you can set the proxy with each command. Bash<pre><code>https_proxy=https://cloud-proxy.internal.io:3128 aws s3 ls s3://gen3-data/ --profile &lt;profilename&gt;\n</code></pre>"},{"location":"gen3-resources/user-guide/analyze-data/#allow-lists","title":"Allow lists","text":"<p>Additionally, to aid Gen3 Commons security, the installation of tools from outside resources is managed through an allow list. If you have problems installing a tool you need for your work, contact support@gen3.org (or the appropriate Gen3 operator) with a list of any sites from which you might wish to install tools. After passing a security review, these can be added to the allow list to facilitate access.</p>"},{"location":"gen3-resources/user-guide/analyze-data/#using-the-gen3-python-sdk","title":"Using the Gen3 Python SDK","text":"<p>To make programmatic interaction with Gen3 data commons easier, the bioinformatics team at the Center for Translational Data Science (CTDS) at University of Chicago has developed the Gen3 Python SDK, which is a Python library containing functions for sending standard requests to the Gen3 APIs. The code is open-source and available on GitHub along with documentation for using it.</p> <p>The SDK includes the following classes of functions:</p> <ol> <li>Gen3Auth, which contains an authorization wrapper to support JWT-based authentication,</li> <li>Gen3Submission, which interacts with the Gen3\u2019s submission service including GraphQL queries,</li> <li>Gen3Index, which interacts with the Gen3\u2019s Indexd service for GUID brokering and resolution.</li> </ol> <p>Below is a selection of commonly used functions along with notebooks demonstrating their use.</p>"},{"location":"gen3-resources/user-guide/analyze-data/#getting-started","title":"Getting Started","text":"<p>The Gen3 SDK can be installed using \u201cpip\u201d, the package installer for Python. For installation details, see this documentation.</p> Text Only<pre><code># Install Gen3 SDK:\npip install gen3\n\n# To clone and develop the source:\ngit clone https://github.com/uc-cdis/gen3sdk-python.git\n\n# Use the `!` magic command to clone and develop the python SDK in the workspace:\n!git clone https://github.com/uc-cdis/gen3sdk-python.git\n\n# As the Gen3 community updates repositories, keep them up to date using:\ngit pull origin master\n</code></pre>"},{"location":"gen3-resources/user-guide/analyze-data/#examples","title":"Examples","text":"<p>1) Most requests sent to a Gen3 data commons API will require an authorization token to be sent in the request\u2019s header. The SDK class Gen3Auth is used for authentication purposes, and has functions for generating these access tokens. Users do need to authenticate when using the SDK from the terminal, but do not need to authenticate once being logged in and working in the workspace of a Data Commons.</p> <p>From the python shell run the following:</p> Text Only<pre><code>import gen3\nfrom gen3.auth import Gen3Auth\nendpoint = \"https://gen3.datacommons.io/\"\ncreds = \"/user/directory/credentials.json\"\nauth = Gen3Auth(endpoint, creds)\n</code></pre> <p>2) The structured data in a Gen3 data commons can be created, deleted, queried and exported using functions in the Gen3Submission class.</p> <p>2.1) All available programs in the data commons will be shown with the function <code>get_programs</code>. The following commands:</p> <p></p>Text Only<pre><code>import gen3\nfrom gen3.submission import Gen3Submission\nsub = Gen3Submission(endpoint, auth)\nsub.get_programs()\n</code></pre> will return: <code>{'links': ['/v0/submission/OpenNeuro', '/v0/submission/GEO', '/v0/submission/OpenAccess', '/v0/submission/DEV']}</code> <p>2.2) All projects under a particular program (\u201cOpenAccess\u201d) will be shown with the function <code>get_projects</code>. The following commands:</p> Text Only<pre><code>from gen3.submission import Gen3Submission\nsub = Gen3Submission(endpoint, auth)\nsub.get_projects(\"OpenAccess\")\n</code></pre> <p>will return \u201cCCLE\u201d as the project under the program \u201cOpenAccess\u201d: <code>{'links': ['/v0/submission/OpenAccess/CCLE']}</code></p> <p>2.3) All structured metadata stored under one node of a project can be exported as a tsv file with the function <code>export_node</code>. The following commands: </p>Text Only<pre><code>from gen3.submission import Gen3Submission\nsub = Gen3Submission(endpoint, auth)\nprogram = \"OpenAccess\"\nproject = \"CCLE\"\nnode_type = \"aligned_reads_file\"\nfileformat = \"tsv\"\nfilename = \"OpenAccess_CCLE_aligned_reads_file.tsv\"\nsub.export_node(program, project, node_type, fileformat, filename)\n</code></pre> will return: <code>Output written to file: OpenAccess_CCLE_aligned_reads_file.tsv</code> <p>3) The function <code>get_record</code> in the class Gen3Index is used to show all metadata associated with a given id by interacting with Gen3\u2019s Indexd service. GUIDs can be found on the Exploration page under the <code>Files</code> tab. The following commands:</p> Text Only<pre><code>from gen3.index import Gen3Index\nind = Gen3Index(endpoint, auth)\nrecord1 = ind.get_record(\"92183610-735e-4e43-afd6-7b15c91f6d10\")\nprint(record1)\n</code></pre> <p>will return: <code>{'acl': ['*'], 'authz': ['/programs/OpenAccess/projects/CCLE'], 'baseid': 'e9bd6198-300c-40c8-97a1-82dfea8494e4', 'created_date': '2020-03-13T16:08:53.743421', 'did': '92183610-735e-4e43-afd6-7b15c91f6d10', 'file_name': None, 'form': 'object', 'hashes': {'md5': 'cbccc3cd451e09cf7f7a89a7387b716b'}, 'metadata': {}, 'rev': '13077495', 'size': 15411918474, 'updated_date': '2020-03-13T16:08:53.743427', 'uploader': None, 'urls': ['https://api.gdc.cancer.gov/data/30dc47eb-aa58-4ff7-bc96-42a57512ba97'], 'urls_metadata': {'https://api.gdc.cancer.gov/data/30dc47eb-aa58-4ff7-bc96-42a57512ba97': {}}, 'version': None}</code></p>"},{"location":"gen3-resources/user-guide/analyze-data/#jupyter-notebook-demos","title":"Jupyter Notebook Demos","text":"<p>Below are three tutorial Jupyter Notebooks that demonstrate various SDK functions that may be helpful for the analysis of data in a Gen3 workspace. You can also navigate to the notebook browser of Gen3 Data Hub or the Biomedical Research Hubto explore the notebooks.</p> <ol> <li> <p>Find this notebook \u201cGen3_authentication notebook\u201d to help guide you how to authenticate from the terminal or from the workspace (download also as .ipynb file). Note that users do need to authenticate when using the SDK from the terminal, but do not need to authenticate once being logged in and working in the workspace of a Data Commons.</p> </li> <li> <p>Download node files, show/select data, and plot with this notebook using data hosted on the Canine Data Commons. Users can upload this notebook as an .ipynb file to the workspace of the Canine Data Commons to start their analysis. Note, that bringing in files into the workspace as explained in this notebook can be also achieved on selected Data Commons by clicking the \u201cExport to Workspace\u201d button on the Exploration Page; please also note, that once files are exported from the Exploration page, users do not need to authenticate anymore in the workspace.</p> </li> <li> <p>Download data files and metadata using the gen3-client and the Gen3 SDK, respectively, and bring them into the workspace. Run gene expression analysis and statistical analysis on the data files and metadata, respectively, and plot the outcome in different scenarios. This Jupyter Data Analysis Notebook uses data hosted on the Gen3 Data Hub. Upload this notebook as an .ipynb file to the workspace of the Gen3 Data Hub and start your analysis. Note, that bringing in files into the workspace as explained in this notebook can be also achieved on selected Data Commons by clicking the \u201cExport to Workspace\u201d button on the Exploration Page; please also note, that once files are exported from the Exploration page, users do not need to authenticate anymore in the workspace.</p> </li> </ol> <p>When finished, please, shut down the workspace server by clicking the \u201cTerminate Workspace\u201d button.</p>"},{"location":"gen3-resources/user-guide/portal/","title":"Data Portal","text":""},{"location":"gen3-resources/user-guide/portal/#accessing-and-exploring-metadata-from-the-gen3-data-portal","title":"Accessing and Exploring Metadata from the Gen3 Data Portal","text":"<p>The data in a Gen3 data commons can be browsed and downloaded using several different methods. The following general documentation will cover some standard methods of data access in a Gen3 data commons. Ultimately, however, the methods of data access offered in a Gen3 data commons is determined by agreements made between the data commons\u2019 sponsors and data contributors.</p> <p>Various levels of data access can be configured in a Gen3 data commons using the Gen3 Framework Services. If open access data is hosted, a data commons can be configured to allow anonymous access to data, which means users can explore data without logging in. This is the case for the Gen3 Data Hub.</p> <p>In cases where data is controlled access, typically external users will receive instructions on how to access data and may be required to sign a DUA (Data Use Agreement) legal document.</p> <p>The following sections provide details on how to explore and access data from within the data commons website and from the command-line by sending requests to the Gen3 open APIs.</p>"},{"location":"gen3-resources/user-guide/portal/#access-data-from-the-data-portal","title":"Access Data from the Data Portal","text":"<p>The Gen3 software stack offers a data portal service that creates a website with graphical tools for performing the basic functionality of a data commons, like browsing data in projects, building patient cohorts across projects, downloading metadata or data files for cohorts, and building database queries.</p>"},{"location":"gen3-resources/user-guide/portal/#profile-page","title":"Profile Page","text":"<p>On the profile page users will find information regarding their access to projects, access to Gen3-specific tools (e.g. access to the Workspace), and the function to create API keys for credential downloads. API keys are necessary for the download of files using the Gen3 Client.</p> <p>Users can view their study access and API keys can be viewed/created/downloaded on the Profile Page.</p> <p></p>"},{"location":"gen3-resources/user-guide/portal/#exploration-page","title":"Exploration Page","text":"<p>The primary tool for exploring data within a Gen3 data commons is the Exploration Page, which offers faceted search of data across projects, for example, Gen3 Data Hub Exploration Page. This page can be accessed from the /explorer endpoint or the top navigation bar, by clicking on the \u201cExploration\u201d icon.</p> <p></p> <p>The exploration page has one or several tabs at the top, which each represent a flattened ElasticSearch document of structured metadata records across all the projects in the data commons, which is displayed as a table towards the bottom center of the page. For example, there may be a \u201cSubjects\u201d tab for building patient cohorts, which displays a table of all the records and associated metadata in the subject node, like medical history and demographics. Most commons also have one or more \u201cFile\u201d tabs for filtering all the files in a data commons based on things like the file format, data type, or other linked contextual variables, like linked patient demographics or medical history.</p> <p>Each of these main tabs will have filters on the left-hand side, which can be used to filter the data displayed in the table. There may be an optional button on each tab to download the flattened metadata table as a JSON file. This button should download the table in its filtered state. To remove a filter, click \u201cclear\u201d on individual facets, and you can remove all filters by reloading the page.</p> <p>Note: The main tabs in the Exploration Page, the available filters, and the properties listed in the data table are entirely customizable and will be different for various Gen3 data commons.</p> <p>If the table is a list of files, there should be a button for downloading a JSON file that serves as a manifest to use with the gen3-client for downloading multiple files. Otherwise, to download single a file listed in the table, simply click on the GUID (globally unique identifier, or object_id), which should open a page with a download button.</p> <p></p> <p>Note: Some data commons have security measures in place that limit what environments users can access data files. For example, users may be required to download and analyze data files in a protected environment, such as a virtual machines (VM) in a virtual private cloud (VPC) or in the built-in Gen3 Workspace, which is accessed by clicking on \u201cWorkspace\u201d in the top navigation bar of the data commons website. For more information on the Workspace, see the documentation on how to access and use the Gen3 Workspace.</p>"},{"location":"gen3-resources/user-guide/portal/#export-to-external-analysis-workspaces","title":"Export to external analysis workspaces","text":"<p>The explorer page can also be configured to allow export of a PFB (Portable Format for Biomedical data) file to external systems such as Terra or Seven Bridges.  A PFB file contains structured clinical data, the data dictionary, and pointers to associated files.</p>"},{"location":"gen3-resources/user-guide/portal/#query-page","title":"Query Page","text":"<p>The structured data in a Gen3 data commons can be queried by using the graphQL query language within the GraphiQL interface for building queries, which can be accessed by clicking the \u201cQuery\u201d button in the top navigation bar or by navigating to the /query endpoint, for example, the Gen3 Data Hub Query Page.</p> <p></p> <p>The button on the Query page to switch between the \u201cGraph Model\u201d or \u201cFlat Model\u201d, will direct the queries to different databases, namely Postgres and ElasticSearch, respectively. Pressing the \u201cDocs\u201d button will reveal documentation of the data commons graphQL schema, which will list the queryable nodes and properties.</p> <p></p> <p>For example, typing the name of a node, \u201csample\u201d, into the \u201cSearch Schema\u201d search-box in the Graph Model, and clicking the \u201croot.sample\u201d option will display all the properties that can be queried for that node.</p> <p> </p> <p>The following example query returns the subject and submitter ids in the subject node in the data commons:</p> Text Only<pre><code>    {\n        subject {\n            subject_id\n            submitter_id\n        }\n    }\n</code></pre> <p>More detailed information on how to query specific data can be found here.</p>"},{"location":"gen3-resources/user-guide/portal/#data-dictionary-viewer","title":"Data Dictionary Viewer","text":"<ul> <li> <p>The Data Dictionary Viewer is designed to make it easier to understand the data model, the field types associated with each node, and the potential values associated with each field. It displays available fields in a node and the dependencies a given node has to the existence of a prior node. This is an invaluable tool for both the submission of data and later analysis of the entire commons.</p> </li> <li> <p>The Data Dictionary Viewer allows toggling views and browsing the nodes as a graph and as tables.</p> </li> <li> <p>Gen3 members can use it through the \u2018Dictionary\u2019 icon in the Gen3 Data Hub.</p> </li> </ul> <p>NOTE: For these user guides, https://gen3.datacommons.io is an example URL and can be replaced with the URL of other data commons powered by Gen3.</p>"},{"location":"gen3-resources/user-guide/portal/#viewing-data-dictionary-as-a-graph","title":"Viewing Data Dictionary as a Graph","text":"<ul> <li>View the Data Dictionary as a graph to see each node, its properties, and its relationships.</li> <li>Relationships between nodes are represented by arrows from one node to another.</li> </ul>"},{"location":"gen3-resources/user-guide/portal/#viewing-data-dictionary-as-tables","title":"Viewing Data Dictionary as Tables","text":"<ul> <li>View the Data Dictionary as a table to see the name and a brief description of each node, organized by node category.</li> <li>Click on the name of a node to display more information about its properties.</li> <li>Click on a button in the Download Template column to a download a template for uploading a file of that particular node type. The file can be downloaded as a JSON or TSV file.</li> </ul>"},{"location":"gen3-resources/user-guide/portal/#toggling-between-different-views","title":"Toggling Between Different Views","text":"<ul> <li>Click on \u201cGraph View\u201d or \u201cTable View\u201d in the top left of the Dictionary Viewer to switch between these two views.</li> </ul>"},{"location":"gen3-resources/user-guide/portal/#viewing-properties-of-a-single-node","title":"Viewing Properties of a Single Node","text":"<ul> <li>The Properties View of a node lists each property, the property\u2019s type, whether or not the property is required, and a brief description of the property.</li> </ul>"},{"location":"gen3-resources/user-guide/portal/#opening-properties-view","title":"Opening Properties View","text":"<ul> <li>Click on a node to see its parent nodes and information about its properties.</li> </ul>"},{"location":"gen3-resources/user-guide/portal/#search-for-node-property-or-description","title":"Search for Node, Property, or Description","text":"<ul> <li>Enter a search a term in the search box.</li> <li>Search terms can be node names, node descriptions, property names, and property descriptions.</li> <li>While typing, a list of suggestions appears below the search bar. Click on a suggestion to search for it.</li> <li>A search will display all nodes that contain the search term in either its name or description, or its properties\u2019 names or descriptions.</li> <li>A history of your searches appears below the search bar. Click on an item here to display the results again.</li> </ul>"},{"location":"gen3-resources/user-guide/portal/#discovery-page","title":"Discovery Page","text":"<p>The Gen3 Discovery Page allows the visualization of metadata from within the metadata service (MDS).  This typically includes public metadata about projects to make it discoverable. The Discovery Page can also be used to store publication information, DOI metadata, or FHIR metadata.  It can be used by both data commons and meshes, although it can play a more central role in a data mesh.  Users should be able to search based on free text or filter based on tags.</p> <p></p> <p>The Discovery Page provides users a venue to search and find studies and datasets displayed on the Biomedical Research Hub. Users can browse through the publicly accessible study-level metadata without requiring authorization.</p> <p>Use text-based search, faceted search, and tags to rapidly and efficiently find relevant studies, discover new datasets across multiple resources, and easily export selected data files to the analysis workspace. Browse through datasets and study-level metadata and find studies using tags, advanced search, or the free text search field.</p> <p></p>"},{"location":"gen3-resources/user-guide/portal/#search-features","title":"Search Features","text":"<p>On the Discovery page, several features help you navigate and refine your search.</p> <p></p> <ol> <li>Total number of studies: shows the number of studies the BRH is currently displaying.</li> <li>Total number of subjects: shows the number of subjects the BRH is currently displaying.</li> <li>Free Text Search: Use keywords or tags in the free-text-based search bar to find studies. The free-text search bar can be used to search for study name, ID number, Data Commons, or any keyword that is mentioned in the metadata of the study.</li> <li>Data Resources/Data Commons Tags: view these by selecting \"Study Characteristics\". Click on a tag to filter by a Data Resource/Data Commons. Selecting multiple tags works in an \"OR\" logic (e.g., \"find AnVIL OR BioData Catalyst studies\").</li> <li>Export Options: Login first to leverage the export options. Select one or multiple studies and download a file manifest or export the data files to a secure cloud environment \"Workspace\" to start your custom data analysis in Python or R.</li> <li>Data Availability: Filter on available, pending, and not-yet-available datasets.</li> <li>Studies: This table feature presents all current studies on BRH. Click on any study to show useful information about the study (metadata).</li> </ol>"},{"location":"gen3-resources/user-guide/portal/#find-available-study-level-metadata","title":"Find available Study-level Metadata","text":"<p>Clicking on any study will display the available study-level and dataset metadata.</p> <p></p>"},{"location":"gen3-resources/user-guide/portal/#find-accessible-datasets","title":"Find accessible Datasets","text":"<p>Users can select and filter studies from multiple resources and conduct analyses on the selected datasets in a workspace. Users can search but not interact with data they do not have access to. By selecting the data access button in the top right corner of the study page user access can be displayed. The Discovery Page will automatically update the list of studies that are accessible.</p>"},{"location":"gen3-resources/user-guide/portal/#access-data-using-the-api","title":"Access Data using the API","text":"<p>All the functionality of the data commons data portal is available by sending requests to the open APIs of the data commons. Detailed API specifications of the Gen3 services can be browsed in the API documentation.</p>"},{"location":"gen3-resources/user-guide/search/","title":"Search for Data","text":""},{"location":"gen3-resources/user-guide/search/#searching-and-exploring-structured-data","title":"Searching and Exploring Structured Data","text":"<p>The data in a Gen3 data commons can be browsed and downloaded using several different methods. The following general documentation will cover some standard methods of data access in a Gen3 data commons. Ultimately, however, the methods of data access offered in a Gen3 data commons is determined by agreements made between the data commons\u2019 sponsors and data contributors.</p> <p>Various levels of data access can be configured in a Gen3 data commons using the Gen3 Framework Services. If open-access data is hosted, a data commons can be configured to allow anonymous access to metadata or structured data, which means users can explore data without logging in. This is the case for the Gen3 Data Hub. However, even in the Gen3 Data Hub where there is no required Data Use Agreement (DUA), users must still login to generate a token in order to download the open-access files. In cases where data is controlled access or otherwise protected, users will typically receive instructions on how to access data and may be required to sign a DUA legal document or perhaps signify approval of some terms of use within the portal.</p> <p>The following sections provide details on how to explore and access data from within the data commons website and from the command-line by sending requests to the Gen3 open APIs.</p>"},{"location":"gen3-resources/user-guide/search/#searching-for-data-from-the-data-portal","title":"Searching for Data from the Data Portal","text":"<p>Gen3 offers a data portal service that creates a website with graphical tools for performing the basic functionality of a data commons, like browsing data in projects, building patient cohorts across projects, downloading metadata or data files for cohorts, and building database queries.</p> <p>While this may vary from system to system, to find data in a data commons you can follow a general workflow of:</p> <ol> <li>Discover data in a mesh or commons using the Discovery Page</li> <li>Request Access to data using system specific solution</li> <li>Explore files in the Exploration Page</li> <li>Export data to workspace or download locally depending on the requirements of your system.</li> </ol> <p>If you prefer programmatic access, instead of using the Discovery and Exploration pages you could instead use the API for locating data of interest.</p>"},{"location":"gen3-resources/user-guide/search/#discovery-page","title":"Discovery Page","text":"<p>In many data commons or meshes the first place to explore your data will be on a Discovery Page.  This typically includes public metadata about projects to make it discoverable. The Discovery Page can also be used to store publication information, DOI metadata, and/or FHIR metadata.</p> <p>Users should be able to search based on free text or filter based on tags and can determine whether they have sufficient authorization to access file for a given project.</p> <p></p> <p>The Discovery Page provides users a venue to search and find studies and datasets displayed on the Biomedical Research Hub. Users can browse through the publicly accessible study-level metadata without requiring authorization.</p> <p>Use text-based search, faceted search, and tags to rapidly and efficiently find relevant studies, discover new datasets across multiple resources, and easily export selected data files to the analysis workspace. Browse through datasets and study-level metadata and find studies using tags, advanced search, or the free text search field.</p> <p></p>"},{"location":"gen3-resources/user-guide/search/#exploration-page","title":"Exploration Page","text":"<p>The primary tool for exploring data within a Gen3 data commons is the Exploration Page, which offers faceted search of data across projects, for example, https://gen3.datacommons.io/explorer. This page can be accessed from the /explorer endpoint or the top navigation bar, by clicking on the \u201cExploration\u201d icon.</p> <p></p> <p>The exploration page has one or several tabs at the top, which each represent a flattened ElasticSearch document of structured metadata records across all the projects in the data commons, which is displayed as a table towards the bottom center of the page. For example, there may be a \u201cSubjects\u201d tab for building patient cohorts, which displays a table of all the records and associated metadata in the subject node, like medical history and demographics. Most commons also have one or more \u201cFile\u201d tabs for filtering all the files in a data commons based on things like the file format, data type, or other linked contextual variables, like linked patient demographics or medical history.</p> <p>Each of these main tabs will have filters on the left-hand side, which can be used to filter the data displayed in the table. There may be an optional button on each tab to download the flattened metadata table as a JSON file. This button should download the table in its filtered state. To remove a filter, click \u201cclear\u201d on individual facets, and you can remove all filters by reloading the page.</p> <p>Note: The main tabs in the Exploration Page, the available filters, and the properties listed in the data table are entirely customizable and will be different for various Gen3 data commons.</p> <p>If the table is a list of files, there should be a button for downloading a JSON file that serves as a manifest to use with the gen3-client for downloading multiple files. Otherwise, to download single a file listed in the table, simply click on the GUID (globally unique identifier, or object_id), which should open a page with a download button.</p> <p></p> <p>Note: Some data commons have security measures in place that limit what environments users can access data files. For example, users may be required to download and analyze data files in a protected environment, such as a virtual machines (VM) in a virtual private cloud (VPC) or in the built-in Gen3 Workspace, which is accessed by clicking on \u201cWorkspace\u201d in the top navigation bar of the data commons website. For more information on the Workspace, see the documentation on how to access and use the Gen3 Workspace.</p>"},{"location":"gen3-resources/user-guide/search/#use-the-api","title":"Use the API","text":"<p>All the functionality of the data portal is available by sending requests to the open APIs of the Gen3 system. Detailed API specifications of the Gen3 services can be browsed in the developer documentation.</p> <p>Users can submit queries to the Gen3 APIs to access structured data across the projects in the data commons. Queries can be sent using, for example, the Python \u201crequests\u201d package or similar functions in other programming languages. Further details on how to send queries to the API are documented here.</p> <p>Users with \u201cread\u201d access to a project can export entire structured metadata records by sending requests to the API. Single records can be exported or all records in a specific node of a project can be retrieved. For more information, see the documentation on using the API.</p>"},{"location":"gen3-resources/user-guide/search/#the-gen3-sdk","title":"The Gen3 SDK","text":"<p>To make sending requests to the Gen3 APIs easier, the bioinformatics team at the Center for Translational Data Science (CTDS) at University of Chicago has put together a basic Python SDK (software development kit) to help users interact with the Gen3 APIs. It also exposes a Command Line Interface (CLI), which covers a lot of data ingestion support and doesn't require the user to write python.</p> <p>The SDK is essentially a collection of Python wrapper functions for sending requests to the API. It is open source and can be found on Github. Thorough documentation for the SDK can be found in the GitHub repository documentation page.</p>"},{"location":"gen3-resources/user-guide/search/#sending-queries-using-the-sdk","title":"Sending Queries using the SDK","text":"<p>The Gen3Submission class of the Python SDK has functions for sending queries to the API and also for retrieving the graphQL schema of the data commons. Queries can be used to pinpoint specific data of interest by providing query arguments that act as filters on records in the database and providing lists of properties to retrieve for those records. If all the structured data in a record or node is desired, as opposed to only specific properties, then see the export functions below.</p>"},{"location":"gen3-resources/user-guide/search/#exporting-structured-data-using-the-sdk","title":"Exporting Structured Data using the SDK","text":"<p>Entire structured data records can be exported as a JSON or TSV file using the Gen3Submission Python SDK class. The <code>export_record</code> function will export a single structured metadata record in a specific node of a specific project, whereas the <code>export_node</code> function will export all the structured metadata records in a specified node of a specific project.</p> <p>More SDK examples and how to get started with the SDK can be also found in the analyze-data section.</p>"},{"location":"gen3-resources/user-guide/search/#query-page","title":"Query Page","text":"<p>While you may call the API directly as described above, Gen3 also includes an interactive interface for creating graphQL query language calls on the Query Page. This can be accessed by clicking \u201cQuery\u201d in the top navigation bar or by navigating to the URL: https://gen3.datacommons.io/query. The URL https://gen3.datacommons.io can be replaced with the URL of other Gen3 data commons.</p> <p>This query portal has been optimized to autocomplete fields based on content, increase speed and responsiveness, pass variables, and generally make it easier for users to find information. The \u201cDocs\u201d button will display documentation of the queryable nodes and properties.</p> <p>From the GraphiQL interface of the data portal, you can switch between Graph Model or Flat Model \u2013 each using endpoints that query different databases (Postgres and ElasticSearch, respectively). Notably, the same queries can be sent to both the flat and graph model API endpoints from the command-line.</p>"},{"location":"gen3-resources/user-guide/search/#graph-model","title":"Graph Model","text":"<p>In the Graph Model, our microservice Peregrine converts GraphiQL queries and hits the PostgreSQL database.</p> <p></p>"},{"location":"gen3-resources/user-guide/search/#list-all-nodes-of-a-particular-node-category","title":"List all nodes of a particular node category","text":"Text Only<pre><code>{\n  _node_type (category: \"medical_history\") {\n    category\n    id\n  }\n}\n</code></pre>"},{"location":"gen3-resources/user-guide/search/#find-specific-files-by-querying-a-datanode","title":"Find specific files by querying a datanode","text":"<p>Metadata for specific files can be obtained by including arguments in \u201cdatanode\u201d queries. The following are some commonly used arguments (not an exhaustive list):</p> <ul> <li><code>submitter_id: \"a_submitter_id\"</code>: get information for a specific submitter_id</li> <li><code>quick_search: \"a_substring\"</code>: get information for all files with partial matches in submitter_id</li> <li><code>file_name: \"a_filename.txt\"</code>: get information for files matching a specified filename.</li> </ul> <p>For example, the following arguments can be used to obtain similar results:</p> <ul> <li><code>quick_search: \"sub-70080\"</code> will return files with the substring \u201csub-70080\u201d in the submitter_id.</li> <li><code>file_name: \"sub-70080_T1w.nii.gz\"</code> will return only files with that exact filename.</li> <li><code>submitter_id: \"OpenNeuro-ds000030_sub-70080_T1w.nii_6ff0\"</code> will return only the file with that exact submitter_id, which must be unique within a node.</li> </ul> <p>The following example query can be pasted into the GraphiQL interface at https://gen3.datacommons.io/query (be sure to click \u201cSwitch to Graph Model\u201d). Note that in this example, there are three individual \u201cdatanode\u201d queries that are sent simultaneously and assigned labels (\u201cmatch_file_name\u201d, \u201cmatch_quick_search\u201d, and \u201cmatch_submitter_id\u201d): </p>Text Only<pre><code>{\n  match_file_name: datanode (file_name: \"sub-70080_T1w.nii.gz\") {\n        project_id object_id id md5sum file_size file_name\n        data_type data_format data_category\n    }\n  match_quick_search: datanode (quick_search: \"sub-70080\") {\n    project_id object_id id md5sum file_size file_name\n    data_type data_format data_category\n  }\n  match_submitter_id: datanode (submitter_id: \"OpenNeuro-ds000030_sub-70080_T1w.nii_6ff0\") {\n      project_id object_id id md5sum file_size file_name\n      data_type data_format data_category\n  }\n}\n</code></pre> Result: applying the <code>file_name</code> and <code>submitter_id</code> arguments returns only the files that match the provided string exactly, while the <code>quick_search</code> argument returns all files with a submitter_id that matches the sub-string. There are two in this case: Text Only<pre><code>{\n  \"data\": {\n    \"match_file_name\": [\n      {\n        \"data_category\": \"T1-weighted Image\",\n        \"data_format\": \"NII/NIfTI\",\n        \"data_type\": \"fMRI Image\",\n        \"file_name\": \"sub-70080_T1w.nii.gz\",\n        \"file_size\": 11427935,\n        \"id\": \"e95e513e-b76e-4de9-aef5-6b9d74e2e60f\",\n        \"md5sum\": \"c800fe80a333e8d3439c854dea3fdad2\",\n        \"object_id\": \"31525da9-7b09-48ea-966d-dd9e93786ff0\",\n        \"project_id\": \"OpenNeuro-ds000030\"\n      }\n    ],\n    \"match_quick_search\": [\n      {\n        \"data_category\": \"T1-weighted Image\",\n        \"data_format\": \"NII/NIfTI\",\n        \"data_type\": \"fMRI Image\",\n        \"file_name\": \"sub-70080_T1w.nii.gz\",\n        \"file_size\": 11427935,\n        \"id\": \"e95e513e-b76e-4de9-aef5-6b9d74e2e60f\",\n        \"md5sum\": \"c800fe80a333e8d3439c854dea3fdad2\",\n        \"object_id\": \"31525da9-7b09-48ea-966d-dd9e93786ff0\",\n        \"project_id\": \"OpenNeuro-ds000030\"\n      },\n      {\n        \"data_category\": \"Diffusion-weighted Image\",\n        \"data_format\": \"NII/NIfTI\",\n        \"data_type\": \"fMRI Image\",\n        \"file_name\": \"sub-70080_dwi.nii.gz\",\n        \"file_size\": 39484002,\n        \"id\": \"f50e6f27-8d02-49f1-b30b-9a3d32c6075d\",\n        \"md5sum\": \"c37bbeb51c85471a7da4f3675c836f71\",\n        \"object_id\": \"94b1d6ef-5e4b-4945-bf31-bb7f06881c97\",\n        \"project_id\": \"OpenNeuro-ds000030\"\n      }\n    ],\n    \"match_submitter_id\": [\n      {\n        \"data_category\": \"T1-weighted Image\",\n        \"data_format\": \"NII/NIfTI\",\n        \"data_type\": \"fMRI Image\",\n        \"file_name\": \"sub-70080_T1w.nii.gz\",\n        \"file_size\": 11427935,\n        \"id\": \"e95e513e-b76e-4de9-aef5-6b9d74e2e60f\",\n        \"md5sum\": \"c800fe80a333e8d3439c854dea3fdad2\",\n        \"object_id\": \"31525da9-7b09-48ea-966d-dd9e93786ff0\",\n        \"project_id\": \"OpenNeuro-ds000030\"\n      }\n    ]\n  }\n}\n</code></pre> In the case that too many results are returned, a timeout error might occur. In that case, use pagination to break up the query. <p>For example, if there are 2,550 records returned, and the GraphiQL query is timing out with <code>(first:3000)</code>, then break the query into multiple queries with offsets: </p>Text Only<pre><code>(first:1000, offset:0)      # this will return records 0-1000\n(first:1000, offset:1000)   # this will return records 1000-2000\n(first:1000, offset:2000)   # this will return records 2000-2,550\n</code></pre>"},{"location":"gen3-resources/user-guide/search/#flat-model","title":"Flat Model","text":"<p>In the Flat Model, our microservice Guppy converts GraphiQL queries and hits the Elasticsearch database. Here, queries support Aggregations for string (bin counts; number of records that each key has) and numeric (summary statistics such as minimum, maximum, sum, etc) fields. For more details see the full description in the Guppy repository.</p>"},{"location":"gen3-resources/user-guide/search/#querying","title":"Querying","text":"<p>Guppy allows you to query the raw data with offset, the maximum number of rows, sorting, and filters. Queries by default return the first 10 entries. To return more entries, the query call can specify a larger number such as <code>(first: 100)</code>.</p> <p>Example: </p>Text Only<pre><code>{\n  subject(offset: 5, first: 100, sort: [\n    {\n      gender: \"asc\"\n    },\n    {\n      _aligned_reads_files_count: \"desc\"\n    }\n  ]) {\n    subject_id\n    gender\n    ethnicity\n    _aligned_reads_files_count\n  }\n}\n</code></pre> The maximum number of results returned is 10,000, which can be requested with the <code>(first: 10000)</code> argument. If you need to access more than that number, we suggest using the Guppy download endpoint."},{"location":"gen3-resources/user-guide/search/#aggregations","title":"Aggregations","text":"<p>Aggregation query is wrapped within the <code>_aggregation</code> keyword. In total, five aggregations are feasible at the moment:</p> <ol> <li>Total Count Aggregation,</li> <li>Text Aggregation,</li> <li>Numeric Aggregation,</li> <li>Nested Aggregation,</li> <li>Sub-aggregation.</li> </ol> <p>For more examples see the full description in the Github repository.</p> <p>Example for 1) Total Count Aggregation that includes a filter: </p>Text Only<pre><code>query ($filter: JSON) {\n _aggregation  {\n   subject(filter: $filter) {\n     _totalCount\n   }\n }\n}\n</code></pre>"},{"location":"gen3-resources/user-guide/search/#filtering","title":"Filtering","text":"<p>Currently, Guppy uses <code>JSON</code>-based syntax for filters. Filters can be text/string/number-based, combined, or nested. For more examples, see the full description in the Github repository.</p> <p>Example for a basic filter unit: </p>Text Only<pre><code>{\n  \"filter\": {\n    \"=\": {\n      \"gender\": \"Female\"\n    }\n  }\n}\n</code></pre> Example query including a filter: Text Only<pre><code>query ($filter: JSON) {\n  subject (filter: $filter, first: 20) {\n    gender\n    race\n    ethnicity\n    _matched {\n      field\n      highlights\n    }\n  }\n}\n</code></pre>"},{"location":"gen3-resources/user-guide/search/#submission-page","title":"Submission Page","text":"<p>A graphical model of the structured data of a data commons can be browsed by navigating to the /submission endpoint of a data commons website or by clicking on the \u201cBrowse Data\u201d or \u201cSubmit Data\u201d buttons in the top navigation bar (if available). This section is often available even if a user does not have access to submit data.  You can see more details in the Submit Structured Data section.</p>"},{"location":"gen3-resources/user-guide/using-api/","title":"Use the API","text":""},{"location":"gen3-resources/user-guide/using-api/#using-the-api","title":"Using the API","text":""},{"location":"gen3-resources/user-guide/using-api/#what-does-the-api-do","title":"What does the API do?","text":"<p>The application programming interface (API) can be a set of code, rules, functions, and URLs that allow apps, software, servers or more generally speaking, systems, to communicate with each other. The communication between APIs consists of requests and (data) responses, usually in .JSON format.</p> <p>The beauty of a Gen3 data commons is that all the functionality of the data commons website is available by sending requests to the open APIs of the data commons. Typical requests at Gen3 include querying, uploading or downloading data, which leads to communication between Gen3 microservices such as the data portal Windmill or the metadata submission service Sheepdog via open APIs.</p> <p>Note: The Gen3 commons uses GraphQL as the language for querying metadata across Gen3 Data Commons. To learn the basics of writing queries in GraphQL, please visit: http://graphql.org/learn. You can also try out creating and executing GraphQL queries in the Data Portal Query Page.</p> <p>Gen3 features a variety of API endpoints such as <code>/submission</code>, <code>/index</code>, or <code>/graphql</code>, which differ in how they access the resource and contain each a subset of REST (Representational State Transfer) APIs for networked applications. REST APIs are restricted in their interactions via HTTP request methods such as GET, POST, PATCH, PUT, or DELETE. The GET request retrieves data in read-only mode, POST typically sends data and creates a new resource, PATCH typically updates/modifies a resource, PUT typically updates/replaces a resource, and DELETE deletes a resource. At Gen3, the GET endpoint </p>Text Only<pre><code>/api/v0/submission/&lt;program&gt;/&lt;project&gt;/_dictionary\n</code></pre> will for example get the dictionary schema for entities of a project, or Text Only<pre><code>/api/v0/submission/graphql/getschema\n</code></pre> will get the data dictionary schema in .JSON format. <p>Further API specifications of the Gen3 microservices can be browsed in the developer documentation and the (REST) API Swagger documentation for each Gen3 microservice can be found in the microservice\u2019s documentation on GitHub (e.g. Sheepdog, Peregrine).</p> <p>Sending API requests can be done in any programming language (e.g. Python, Java, R). The Center for Translational Data Science (CTDS) at University of Chicago has put together a basic SDK in Python and in R to help users interact with the Gen3 APIs. Below, we list examples in how to send API requests at Gen3 using Python for demonstration purposes.</p>"},{"location":"gen3-resources/user-guide/using-api/#credentials-to-send-api-requests","title":"Credentials to send API requests","text":"<p>The credentials that allow access to \u201craw\u201d data in the object store and ssh keys to access VMs, also allow users to programmatically query or submit data to the API. This credential is used every time an API call is made.</p> <p>Each API key is valid for a month and is used to receive a temporary access token that is valid for only 30 minutes. The access token is what must be sent to the Gen3 API to access data in the commons.</p> <p>For users granted data access, the API key is provided on the Profile page after clicking the \u201cCreate API key\u201d button.</p> <p></p> <p>While displayed, click \u201ccopy\u201d to copy the API key to the clipboard or \u201cdownload\u201d to download a \u201ccredentials.json\u201d file containing the id/key pair in json format.</p> <p></p> <p>In Python, the following command is sent, using the module \u201crequests\u201d, to receive the access token: </p>Python<pre><code># Save the credentials.json file from the website. Copy the file path to the credentials file with the key.\n# Then, paste the file path as the value for the `key_file` variable\n\nimport json, requests\n\napi = \"https://gen3.datacommons.io\"\nkey_file = \"/put_path_to/credentials.json\"\n\n# Read the key from the file\nwith open(key_file) as json_file:\n    key = json.load(json_file)\n\n# Pass the API key to the Gen3 API using \"requests.post\" to receive the access token:\ntoken = requests.post('{}/user/credentials/cdis/access_token'.format(api), json=key).json()\n\n# Now the access_token should be displayed when the following line is entered:\ntoken\n</code></pre> When submitting a graphQL query to the Gen3 API, or requesting data download/upload, include the access token in the request header: Python<pre><code>headers = {'Authorization': 'bearer '+ token['access_token']}\n\n# A GraphQL Endpoint Query Using the \"key\" JSON:\nquery = {'query':\"\"\"{project(first:0){project_id id}}\"\"\"};\nql = requests.post('https://gen3.datacommons.io/api/v0/submission/graphql/', json=query, headers=headers)\nprint(ql.text) # display the response\n\n# Data Download via API Endpoint Request:\ndurl = 'https://gen3.datacommons.io/api/v0/submission/&lt;program name&gt;/&lt;project code&gt;/export?format=tsv&amp;ids=' + ids[0:-1] # define the download url with the GUIDs of the records to download in \"ids\" list\ndl = requests.get(durl, headers=headers)\nprint(dl.text) # display response\n\n# Data Upload via API Endpoint Request:\ntsv_file = '/cmd-test.tsv' # loading the tsv file that contains text string and tab-separated values\ntsv = ''\ncount = 0\nwith open(tsv_file, 'r') as file:\n    for line in file:\n        if count == 0:\n            header = line\n        tsv = tsv + line + \"\\r\"\n        count += 1\nprogram_name,project_code = &lt;program_name&gt;, &lt;project_code&gt;\nu = requests.put('https://gen3.datacommons.io/api/v0/submission/{}/{}'.format(program_name,project_code), data=tsv, headers=headers)\nu.text # should display the API response\n</code></pre> <p>If an error such as \"You don\u2019t have access\u2026 \" occurs, then either you do not have access, or the API key is out of date and a new access token will need to be made. Further errors could occur if the uploaded file is not correctly formatted for the Gen3 data model.</p>"},{"location":"gen3-resources/user-guide/using-api/#querying-and-downloading-metadata-using-the-api","title":"Querying and Downloading Metadata using the API","text":"<p>Users with read access to a project can download individual metadata records in the project or all records in a specified node of the project using the API.</p> <p>Example 1 The API endpoint for downloading all the records in a single node of a project is:</p> <p><code>{commons-url}/api/v0/submission/{program name}/{project code}/export/?node_label={node}&amp;format={json/tsv}</code></p> <p>Where: <code>{commons-url}</code> is the gen3 data commons url, <code>{program name}</code> is the program node's property 'name', <code>{project code}</code> is the project node's property 'code', <code>{node}</code> is the name (or 'node_id') of the node, <code>{json/tsv}</code> is the format in which data will be downloaded, either 'json' or 'tsv'.</p> <p>For example, submitting the following API request will download all the records in the \u2018sample\u2019 node of the project \u2018GEO-GSE63878\u2019 in the Gen3 Data Hub as a tab-separated values file (TSV): </p>Text Only<pre><code>https://gen3.datacommons.io/api/v0/submission/GEO/GSE63878/export/?node_label=sample&amp;format=tsv\n</code></pre> <p>Example 2 The API endpoint for downloading a single record in a project is as follows:</p> <p><code>{commons-url}/api/v0/submission/{program name}/{project code}/export?ids={ids}&amp;format={json/tsv}</code></p> <p>Where: <code>commons-url</code>is the gen3 data commons url, <code>program name</code> is the program node's property 'name', <code>project code</code> is the project node's property 'code', <code>ids</code> is a comma separated list of the GUIDs for the records to be downloaded, <code>json/tsv</code> is the format in which data will be downloaded, either 'json' or 'tsv'.</p> <p>For example, submitting the following API request will download the two records corresponding to the GUIDs (the \u2018id\u2019 property) \u2018180554c0-d0e1-41f2-b5b0-47655d7975ed\u2019 and \u20183e54268c-b4a6-4cf8-bedc-1e9e49f9d6e9\u2019. This downloads a \u2018subjects.tsv\u2019 file containing those two records.</p> Text Only<pre><code>https://gen3.datacommons.io/api/v0/submission/GEO/GSE63878/export?ids=180554c0-d0e1-41f2-b5b0-47655d7975ed,3e54268c-b4a6-4cf8-bedc-1e9e49f9d6e9&amp;format=tsv\n</code></pre> <p>Example 3 The API endpoint for querying the metadata associated with the given GUID is as follows:</p> <p><code>{commons-url}/index/{GUID}</code></p> <p>Where: <code>commons-url</code> is the gen3 data commons url, <code>GUID</code> is the globally unique identifier.</p> <p>The following request will show the metadata of the indexed record in .JSON format. Thus, a browser that includes a .JSON viewer (e.g. Firefox) or a manually installed plug-in can show the .JSON in pretty-print. </p>Text Only<pre><code>https://gen3.datacommons.io/index/47c46ead-f6f5-4cc9-86b9-2354cafe8c64\n</code></pre>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/operator/","title":"Operator","text":""},{"location":"blog/category/ctds/","title":"CTDS","text":""}]}